---
title: 微服务设计笔记11
tags: [微服务]
---
# 微服务设计笔记11
本篇笔记为微服务设计系列读书笔记的第十一篇，主要记录规模化微服务。  
讲述了功能降级、服务发现、文档服务、缓存、CAP定理。
## 故障无处不在
从统计学上看，规模化后故障将成为必然事件。所以需要假定故障肯定会发生，没必要阻止故障发生。
设计系统时需要考虑的一些指标：
- 响应时间/延迟（每秒处理200个并发连接时，90%响应时间在2秒内）
- 可用性 （可接受停机时间）
- 数据持久性 （多大比例的数据丢失可以接受？数据应该保存多久）

## 功能降级
做功能设计时，需要问自己“如果这个微服务宕机会发生什么”。实例：如果购物车微服务宕掉之后，需要考虑功能降级的具体方式：（1）购物车控件变成一个可下订单的电话号码（2）或者只展示马上回来。 系统需要具有弹性。   

## 架构性安全措施
有一些模式，组合起来称为架构性安全措施，可以确保事情出错之后，不会引起严重的级联影响。处理系统缓慢要比处理系统快速失败困难的多。在分布式系统中，延迟是致命的。问题解决方法：（1）正确设置超时（2）实现舱壁隔离不同的连接池并实现一个断路器。实际开发中的一个案例：短线发送业务，短线服务宕机。   

## 反脆弱的组织
《反脆弱》塔勒布认为事务实际上是受益于失败和混乱。在生产环境中通过编写程序引发故障。 
混乱猴子：在一天的特定时段随机停掉服务器。   
混乱大猩猩:随机关闭整个AWS可用区。  
延迟猴子：在系统之间注入网络延迟。  
系统分布在多台机器上，通过网络通信，这都会使系统更脆弱，而不是更见状。  

### 超时
1. 等待太久再决定调用失败，整个系统会被拖慢。
2. 超时太短，会把正常调用当做失败处理。
3. 完全没有超时，一个宕掉的下游系统可能会让整个系统挂起。
解决方案：给所有的跨进程调用设置超时，并选择一个默认的超时时间。当超时发生时，记录到日志中，并相应地调整他们。

### 断路器
使用断路器时，对下游的资源请求发生一定数量的失败后，断路器会打开。所有的请求在断路器打开状态下会快速失败。一段时间后，客户端发送一些请求查看下游服务是否已经恢复，如果得到正常响应，将重置断路器。  
疑问：断路器如何判断下游服务已经恢复？

![断路器视图](/images/wfw11_dlq.png)<br/>


### 舱壁
把自己从故障中隔离开的一种方式。断路器可以看做密封一个舱壁的自动机制。对所有同步的下游调用都使用断路器。


## 幂等
所谓幂等，多次执行所产生的影响，均与一次执行的影响相同。如果操作是幂等的，可以对其重复多次调用，而不必担心有不利影响。


## 扩展
扩展的目的：（1）帮助处理失败，担心有东西会失败，多一些这样的东西会有帮助。（2）性能扩展。   
### 更强大的主机
换更好的CPU和内存，改善延迟和吞吐量，垂直扩展。虚拟化机器比较方便使用此方法。   
### 拆分负载
比如报表服务负载较高，可以拆分出来，单独部署。
### 分散风险
微服务部署在多个主机上，甚至部署在多个不同的数据中心。  
### 负债均衡
### 基于worker的系统
### 重新设计
考虑10倍容量的增长，但是超过100倍容量时就要重写了。比如，从同步请求转换成基于事件的系统，采用新的部署平台，改变整个技术栈。   
花了6个月开发了一个项目，但是却没人用。倒不如在网页上挂个链接，看到底有没有人点击，再决定是否需要做。  

## 扩展数据库

### 服务的可用性和数据的持久性
区分服务的可用性和持久性，写入主数据库的所有数据，都复制到备用副本数据库。如果主数据库出现故障，应该需要有一个机制让主数据库恢复或者提升副本为主数据库。   
### 扩展读取
考虑模式只读副本。最终一致性：服务可以在单个主节点上进行所有的写操作，但是读取被分发到一个或多个只读副本。从主数据库复制到副本，是在写入后的某个时刻完成的，这种技术有时候看到的可能是失效的数据，但是最终能读取到一致的数据。   
![断路器视图](/images/wfw11_zdms.png)<br/>  

### 扩展写操作
使用分片的方法，将数据存储到多个数据库节点。   
写操作的复杂性来自于查询处理。查找单个记录很容易，可以应用哈希函数找到数据应该在哪个实例上，然后从正确的分片获取它。如果查询跨越了多个节点，往外采用异步机制，将查询的结果放进缓存。比如mongo使用,ap/reduce机制。   
使用分片系统会出现一个问题，增加一个额外的数据库节点该怎么办？需要大量的宕机时间，现在Cassandra可以不停机增加分片，但是仍然需要充分测试。  
写入分片会扩展容量，但是不会提高弹性。  
综上所述，扩展数据库写分片可能非常棘手，长远开可能需要Cassandra,mongo 这样的数据库系统。

## 缓存
### 客户端、代理和服务器端缓存
客户端缓存：如果想改变缓存方式，让大批的消费者全都变化是很困难的。  


### HTTP缓存
HTTP header中的`cache-control Expires`，
### 为写使用缓存
使用后写式，可以先写入本地缓存中，并在之后的某个时刻将缓存中的数据写入下游、可能更规范化的数据源中。 当有爆发式的写操作，或者同样的数据可能会被写入多次，可以尝试使用本方法。
### 为弹性使用缓存
缓存可以在故障时实现弹性，使用客户端缓存，如果下游服务不可用，客户端可以先使用缓存中可能失效了的数据。
### 隐藏源服务
保护源服务，在后台异步重建缓存。有些源服务只能处理一部分流程，因为大部分的请求已经被前面的缓存处理了，如果整个缓存区小时，源服务就容易被搞崩溃。   

![断路器视图](/images/wfw11_hcyfw.png)<br/> 


### 保持简单
缓存越多，越难评估任何数据的新鲜程度，保持简单，先在一处使用缓存，在添加更多的缓存前慎重考虑。   

## CAP定理
CAP定理：分布式系统中有三方面需要作出权衡：一致性、可用性、分区容忍性。我们最多只能保证三个中的两个。
- 一致性：当访问多个节点时能得到同样的值。
- 可用性：每个请求都能获得响应。
- 分区容忍性：集群中的某些节点在无法联系后，集群整体还能继续进行服务的能力。

![cap说明图](/images/wfwsj11_cap.png)<br/> 
考虑上图中双向数据同步中断失败的情况。 
### 牺牲一致性
不停用库存服务，导致DC1和DC2的数据不一致。但是系统仍然可以使用，两个节点在系统分区后仍然能够服务，但是失去一致性，AP系统。   
系统放弃一致性以保证分区容忍性和可用性的这种方法，称为最终一致性。   
### 牺牲可用性
我选采取拒绝响应服务，牺牲可用性。系统是一致且分区容忍的即CP。在这种模式下，我们的服务必须考虑如何做到功能降级，直到分区恢复以及数据库节点之间可以重新同步。建议最好还是放弃一致性，去努力构建一个最终一致性的AP系统。 

### 牺牲分区容忍性
CA系统是不存在的。如果系统没有区分容忍性，就不能跨网络运行，即需要在本地一个进程内运行。  

### AP还是CP
AP更简单，CP系统因为要支持分布式一致性会遇到更多的挑战。对于库存系统，一个记录过时了5分钟可以接受，那么这个系统是一个AP系统。但是对于银行查询余额聚德考虑CP系统了。   

### 不一定是全部
系统作为一个整体，不需要全部是AP或者CP的。目录服务可能是AP的，但是库存服务需要是CP的，因为我们不想给客户一个不存在的东西。 我们要做的是把CAP定理的权衡，推到单独服务的每个功能中去。

## 动态服务注册
### zookeepr
配置管理、服务间数据同步、leader选举、消息队列和命名服务。  
zookeeper核心提供了一个用于村粗信息的分层命名空间。我们可以在这个结构中存储服务位置的信息，并且可以作为一个客户端接收更改信息。经验：不应该实现自己的分布式协调系统，使用已有的可工作的选择是非常明智的。  

### Consul
也是提供配置管理和服务发现。

### Eureka
具有基本负载均衡的功能，可以支持服务实例的基本轮询调度查找。


## 文档服务
正确进行了服务发现，能够知道东西在哪。但是如何知道这些东西的用处，如何使用它们？一个明显的选择就是API稳定。文档总会过时，理想的情况下确保文档总是和最新的微服务API同步，并能够方便提供大家查阅。两种不同的技术Swagger和HAL都可以达到目的。   

### Swagger
产生一个友好的用户界面，可以查看文档并通过web浏览器与API交互。

### wiki
使用wiki记录API。




## 参考
《微服务设计中文版》








































































































