{"meta":{"title":"学习之路","subtitle":"做只笨鸟","description":"记录技术学习以及读书心得","author":"DuMing","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2018-04-14T13:26:37.000Z","updated":"2019-06-26T03:40:39.272Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"redis集群部署与数据迁移","slug":"2019-06-03-redis集群部署与数据迁移","date":"2019-06-26T05:24:56.738Z","updated":"2019-06-26T05:24:56.738Z","comments":true,"path":"2019/06/26/2019-06-03-redis集群部署与数据迁移/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-06-03-redis集群部署与数据迁移/","excerpt":"","text":"redis集群部署与数据迁移项目最初仅仅使用一台reidis单机，计划扩展成redis cluster，并且需要将原单机上的数据迁移到新的集群中。 集群部署在阿里云两台主机172.17.188.91、172.17.24.253。91主机计划部署master A、B、C三个redis节点，253主机计划部署对应的三个从节点。 redis从5.0开始部署集群不再需要ruby，如果您安装的是redis3.0、4.0版本仍然需要更新高版本的ruby。 安装ruby（redis3.0、4.0版本需要）12345678910cd /usr/local/srcwget https://cache.ruby-lang.org/pub/ruby/2.4/ruby-2.4.6.tar.gztar -xvf ruby-2.4.6.tar.gz./configure -prefix=/usr/local/ruby2.4make &amp;&amp; make installvim ~/.bash_profileRUBY_PATH=/usr/local/ruby2.4/binPATH=$PATH:$HOME/bin:/usr/local/mongdb/bin:$RUBY_PATH 安装redis5.0版本12345wget http://download.redis.io/releases/redis-5.0.5.tar.gztar -xvf redis-5.0.5.tar.gzcd redis-5.0.5make make install PREFIX=/usr/local/redis-cluster 创建redis节点实例123456cd /usr/local/redis-cluster172.17.188.91 机器mkdir -p /usr/local/redis-cluster/&#123;7001,7002,7003&#125;172.17.24.253 机器mkdir -p /usr/local/redis-cluster/&#123;7004,7005,7006&#125; 修改配置文件进入每个节点创建redis.conf文件，注意需要修改正确的端口 12345678910daemonize yes pidfile /var/run/redis_7001.pid port 7001 cluster-enabled yes cluster-config-file nodes_7001.conf cluster-node-timeout 5000 appendonly yeslogfile /usr/local/redis-cluster/logs/7001.logbind 0.0.0.0protected-mode no 创建启动redis各个节点脚本cd /usr/local/redis-cluster/vim start-redis-cluster.sh91机器脚本内容如下，253机器同样也部署一个类似脚本。 12345#!/bin/shREDIS_HOME=/usr/local/redis-cluster$REDIS_HOME/bin/redis-server $REDIS_HOME/7001/redis.conf$REDIS_HOME/bin/redis-server $REDIS_HOME/7002/redis.conf$REDIS_HOME/bin/redis-server $REDIS_HOME/7003/redis.conf chmod +x /usr/local/redis-cluster/start-redis-cluster.sh 启动redis分别进入91和253机器执行下述脚本sh start-redis-cluster.sh验证是否正常启动 1234567891091机器ps -ef|grep redisroot 9871 1 0 14:22 ? 00:00:00 /usr/local/redis-cluster/bin/redis-server *:7001 [cluster]root 9873 1 0 14:22 ? 00:00:00 /usr/local/redis-cluster/bin/redis-server *:7002 [cluster]root 9875 1 0 14:22 ? 00:00:00 /usr/local/redis-cluster/bin/redis-server *:7003 [cluster]253机器root 17820 1 0 14:22 ? 00:00:00 /usr/local/redis-cluster/bin/redis-server *:7004 [cluster]root 17822 1 0 14:22 ? 00:00:00 /usr/local/redis-cluster/bin/redis-server *:7005 [cluster]root 17824 1 0 14:22 ? 00:00:00 /usr/local/redis-cluster/bin/redis-server *:7006 [cluster] 创建redis集群因为使用的是redis5.0版本所以直接使用redis-cli创建即可。 1./redis-cli --cluster create 172.17.188.91:7001 172.17.188.91:7002 172.17.188.91:7003 172.17.24.253:7004 172.17.24.253:7005 172.17.24.253:7006 --cluster-replicas 1 其中--cluster-replicas 1表示每个master都一个slave。上述命令输入完毕后，出现下文，可以看到redis提示个股slot以及master和slave节点的分配。直接输入YES即可。 12345678910&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.17.24.253:7006 to 172.17.188.91:7001Adding replica 172.17.188.91:7003 to 172.17.24.253:7004Adding replica 172.17.24.253:7005 to 172.17.188.91:7002&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. redis-cli查看集群信息123456ln -s /usr/local/redis-cluster/bin/redis-cli /bin/redis-cliredis-cli -c -p 7001查看集群节点127.0.0.1:7001&gt; cluster nodes查看集群信息127.0.0.1:7001&gt; cluster info 检查节点状态1redis-cli --cluster check 172.17.24.253:7005 redis cluster集群加上认证登录到redis6个节点执行下面的操作redis-cli -p 7001 -c 1234&gt; config set masterauth password&gt; config set requirepass password&gt; auth password&gt; config rewrite 各个节点都完成上面的3条config操作，重启redis各节点，看下各节点的redis.conf，可以发现最后多了3行内容。停掉redis的命令为killall redis-server。加了认证的redis登陆redis-cli -h 127.0.0.1 -p 7001 -c -a &quot;password&quot; redis数据迁移数据需要从之前的单机迁移到现在的cluster，借助开源迁移工具Redis-Migrate-Tool。 软件安装12345678请先安装automake, libtool, autoconf and bzip2 很简单一路yum install$ cd redis-migrate-tool$ autoreconf -fvi$ ./configure$ make$ src/redis-migrate-tool -h 执行迁移从rdb向cluster迁移脚本如下所示： 123456789101112[source]type: rdb fileservers: - /usr/local/redis/bin/dump.rdbredis_auth:password[target]type: redis clusterservers: - 127.0.0.1:7001redis_auth:password[common]listen: 0.0.0.0:8888 迁移命令./src/redis-migrate-tool -c ./rmt.conf -o log -d 迁移结果我是从redis4.0单机迁移数据到redis5.0的cluster，看来不支持这种迁移。 12[2019-06-01 15:24:39.199] rmt_redis.c:6446 ERROR: Can&apos;t handle RDB format version -1084063656[2019-06-01 15:24:39.199] rmt_redis.c:6715 ERROR: Rdb file for node[/usr/local/redis/bin/dump.rdb] parsed failed 使用aof文件迁移进入 172.17.188.91:6379 执行命令 123456[root@TEST01 bin]# ./redis-cli 127.0.0.1:6379&gt;127.0.0.1:6379&gt; dbsize(integer) 11 127.0.0.1:6379&gt; BGREWRITEAOFBackground append only file rewriting started 修改配置文件123456789101112[source]type: aof fileservers: - /usr/local/redis/bin/appendonly.aofredis_auth:password[target]type: redis clusterservers: - 127.0.0.1:7001redis_auth:password[common]listen: 0.0.0.0:8888 迁移命令./src/redis-migrate-tool -c ./rmt.conf -o log -d 结果验证脚本运行完毕后发现数据已经分散到6台机器上了。 1234567127.0.0.1:7005&gt; cluster nodes12cbb09b6dac87a8e4042f9fcad1874dbe1f7fe8 172.17.24.253:7006@17006 slave 487fb1e15eec85add6c5cfbf2c51cbed8f03f0b0 0 1559377514869 6 connected0be214f0080eb249a748ccd05083acf7af3c96cb 172.17.188.91:7003@17003 slave 2cba4783d3193150ea5df466d19b6f2b11961453 0 1559377515873 4 connected937bb7d1f81cf8d57c2836a8f2d2afafac9fff07 172.17.188.91:7002@17002 master - 0 1559377516574 2 connected 10923-163837875e9a84e2c23cceb853db8bc029f4004feff74 172.17.24.253:7005@17005 myself,slave 937bb7d1f81cf8d57c2836a8f2d2afafac9fff07 0 1559377515000 5 connected2cba4783d3193150ea5df466d19b6f2b11961453 172.17.24.253:7004@17004 master - 0 1559377516072 4 connected 5461-10922487fb1e15eec85add6c5cfbf2c51cbed8f03f0b0 172.17.188.91:7001@17001 master - 0 1559377515571 1 connected 0-5460 根据上述结果 123456172.17.188.91:7001@17001 master172.17.24.253:7004@17004 master172.17.188.91:7002@17002 master登陆上述三台机器 ，分别执行dbsize 命令发现对应数量为3、3、5 整体跟单机11个是匹配的。 因为我整个集群key只有11个，且大多数为hash，登陆机器分别执行hlen 键值 判断数据数量是否成功。 经过上述判断，迁移数据应该是正常的。 参考Redis 5.0 Cluster集群带认证及客户端连接Redis5.0.0集群搭建-centos7Redis ​集群迁移工具 Redis-Migrate-Tool官方cluster-tutorialredis-migrate-tool初学乍练redis：Redis 5.0.3单实例数据迁移到ClusterRedis单实例数据迁移Cluster方案实战","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"Nodejs最佳实践","slug":"2019-06-26-nodejs最佳实践","date":"2019-06-26T03:40:39.269Z","updated":"2019-06-26T03:40:39.269Z","comments":true,"path":"2019/06/26/2019-06-26-nodejs最佳实践/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-06-26-nodejs最佳实践/","excerpt":"","text":"Nodejs最佳实践阅读了github上开源的nodejs最佳实践，具体每个实践规则已整理出思维导图。现将决定以后坚决在开发中使用的规则整理如下： 项目结构 按照业务模块去划分目录，而不是按照技术角色，但是可以在业务目录中按照技术角色去再次划分。切忌直接在项目目录中划分contoller、service目录。 封装通用工具类作为npm包 使用环境自动识别、安全结构化的配置文件，可以借助process.NODE.ENV环境执行变量。 异常处理 只使用nodejs内置的Error对象。 API文档推荐使用swagger，以便调用时知道错误返回信息。 参数校验推荐使用JOI，若是校验错误则快速失败。 代码风格 针对变量优先选择const而不是let或者var。 引入模块目录，而不是直接引入模块目录下的文件。 使用===运算符而不是==，双等号会把不同类型的变量先变成同种类型。 使用ESlint扩展插件 测试 测试要包括三个方面：测试的内容、测试的条件和场景、期望的输出结果。 使用linter探查代码问题，比如sonar。 定期检查需要更新的npm包，借助工具ncu。 生产发布 使用smart logging增加日志透明性，比如使用winston或者Bunyan库。 Set NODE_ENV=production 可以提高一些包的运行速度 不要在app中硬编码日志路径，dev应该将日志打印到stdout，借助运行环境将stdout定向到适当地方。 安全性 cookie安全性，secured参数设置为true，httponly设置为true。 校验http入参，若是校验失败fast fail，可以使用JOI控件。 对客户隐藏错误详情。 来源nodebestpractices","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"E大网格策略","slug":"2019-05-28-E大网格策略","date":"2019-06-26T03:40:39.264Z","updated":"2019-06-26T03:40:39.264Z","comments":true,"path":"2019/06/26/2019-05-28-E大网格策略/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-05-28-E大网格策略/","excerpt":"","text":"E大网格策略首先感谢E大的精彩文章，他把7年以来对网格策略的运用写成公众号无偿奉献给大家。这几篇文章我每篇都读了5遍，并用思维导图记录一下，方便以后回顾。 油气指数实际网格操作图： 来源E大原文链接","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"机构投资的创新之路","slug":"2019-05-24-机构投资的创新之路","date":"2019-06-26T03:40:39.262Z","updated":"2019-06-26T03:40:39.262Z","comments":true,"path":"2019/06/26/2019-05-24-机构投资的创新之路/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-05-24-机构投资的创新之路/","excerpt":"","text":"机构投资的创新之路本书是久负盛名的大卫斯文森编著，《不落俗套的成功》姊妹篇，文章有些篇幅是站在机构投资者角度上写的，但是有些方面仍然对个人投资者有帮助。 关于股票 股票偏好原则与适度分散化原则是制定资产配置的基础。 成功投资中最可靠的策略是价值型投资策略（5毛买1块） 盲目逆向投资会增加投资组合风险。 不加选择的买入低PB低PE股票是幼稚的。 非流动性资产为长期投资者带来机会。 关于债券 投资级以下的债券抵御不了严重的经济萧条。 债券投资的目的是安然度过危机并付出收益率低的代价。 外国债券收益率低并且汇率风险抵消了它的优点。 关于资产配置 资产组合中某项资产比例不到5%或10%意义不大。 配置比例超过25%或30%产生过度集中风险。 传统资产类别的收益是靠市场产生的（投资者无需依靠天赋或者运气）","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"阿里云内网机器通过代理访问外网","slug":"2019-05-16-阿里云内网机通过代理访问外网","date":"2019-06-26T03:40:39.259Z","updated":"2019-06-26T03:40:39.259Z","comments":true,"path":"2019/06/26/2019-05-16-阿里云内网机通过代理访问外网/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-05-16-阿里云内网机通过代理访问外网/","excerpt":"","text":"阿里云内网机器通过代理访问外网笔者购买了两台阿里云虚机，其中一台有外网服务，另一台没有购买外网服务。内网机因为不能联网，所以安装node 第三方包时，非常费劲。 无奈上网查询联网解决方案，找到了在外网机安装squid代理，内网通过连接代理实现外网访问。 安装squid 登陆外网机（能联通外网的机器），执行命令yun install squid。 修改squid配置，cd /etc/squid cp squid.conf squid.conf.bak配置文件中定义了如下网段12345acl localnet src 10.0.0.0/8 # RFC1918 possible internal networkacl localnet src 172.16.0.0/12 # RFC1918 possible internal networkacl localnet src 192.168.0.0/16 # RFC1918 possible internal networkacl localnet src fc00::/7 # RFC 4193 local private network rangeacl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines 默认是允许上述本地网段连接代理。配置规则如下：12http_access allow localnet //允许本地网段使用http_access deny all //拒绝所有 我因为通过阿里云的网络安全规则加一道阀门，所以这个配置文件没有改，维持默认。 重启squid服务service squid start。 在外网机添加端口转发在外网机执行下面的命令，将3128端口转发至80。 iptables -t nat -I PREROUTING -i eth0 -s 10.0.0.0/8 -p tcp --dport 80 -j REDIRECT --to-port 3128 查看配置iptables -t nat -vnL 内网机配置代理 登陆内网机 输入命令vim ~/.bashrc 在最后添加代理export http_proxy=http://外网机IP:3128 export https_proxy=http://外网机IP:3128 export ftp_proxy=http://外网机IP:3128 3 执行source ~/.bashrc 4 设置npm代理npm config set proxy=http://squid服务地址:squid监听端口 5 在内网机上测试是否能访问外网wget www.baidu.com 参考来源 在阿里云服务器内网下的NodeJS的代理设置 给Linux设置HTTP、FTP代理 使用squid在阿里云服务器（centos7）上搭建自己的代理服务器","categories":[],"tags":[{"name":"CICD","slug":"CICD","permalink":"http://yoursite.com/tags/CICD/"}]},{"title":"源码编译安装nodejs","slug":"2019-05-16-源码编译安装nodejs","date":"2019-06-26T03:40:39.257Z","updated":"2019-06-26T03:40:39.257Z","comments":true,"path":"2019/06/26/2019-05-16-源码编译安装nodejs/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-05-16-源码编译安装nodejs/","excerpt":"","text":"源码编译安装nodejsnodejs10源码编译安装需要gcc4.9以上的版本，本机是4.8的版本所以先升级gcc。 gcc更新版本1234567wget http://ftp.gnu.org/gnu/gcc/gcc-6.1.0/gcc-6.1.0.tar.gztar -zvxf gcc-6.1.0.tar.gz --directory=/usr/local/cd /usr/local/gcc-6.1.0./contrib/download_prerequisites mkdir build &amp;&amp; cd build ../configure -enable-checking=release -enable-languages=c,c++ -disable-multilib make &amp;&amp; make install 安装nodejs123456wget https://nodejs.org/dist/v10.15.3/node-v10.15.3.tar.gzmkdir /usr/local/nodecd node-v10.15.3./configure --prefix=/usr/local/nodemakemake install nodejs安装报错123node: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21&apos; not found (required by node)node: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.15&apos; not found (required by node)node: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.20&apos; not found (required by node) 检查动态库1234567891011121314151617[root@azure_a1/]# strings /usr/lib64/libstdc++.so.6 | grep GLIBCGLIBCXX_3.4GLIBCXX_3.4.1GLIBCXX_3.4.2GLIBCXX_3.4.3GLIBCXX_3.4.4GLIBCXX_3.4.5GLIBCXX_3.4.6GLIBCXX_3.4.7GLIBCXX_3.4.8GLIBCXX_3.4.9GLIBCXX_3.4.10GLIBCXX_3.4.11GLIBCXX_3.4.12GLIBCXX_3.4.13GLIBCXX_FORCE_NEWGLIBCXX_DEBUG_MESSAGE_LENGTH 查找编译gcc时生成的最新动态库：(一般会在Nodejs源码安装目录里面) find / -name &quot;libstdc++.so*&quot;输出如下：123456789/usr/local/gcc-6.1.0/build/prev-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6/usr/local/gcc-6.1.0/build/prev-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.22/usr/local/gcc-6.1.0/build/prev-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so/usr/local/gcc-6.1.0/build/stage1-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6/usr/local/gcc-6.1.0/build/stage1-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.22/usr/local/gcc-6.1.0/build/stage1-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so/usr/local/gcc-6.1.0/build/x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6/usr/local/gcc-6.1.0/build/x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.22/usr/local/gcc-6.1.0/build/x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so 将上面的最新动态库libstdc++.so.6.0.22复制到/usr/lib64目录下：cp /usr/local/gcc-6.1.0/build/stage1-x86_64-pc-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.22 /usr/lib64复制后，重建默认库的软连接:12cd /usr/lib64mv libstdc++.so.6 /tmp 将默认库的软连接指向最新动态库：ln -s libstdc++.so.6.0.22 libstdc++.so.6再次检查动态库strings /usr/lib64/libstdc++.so.6 | grep GLIBC 添加环境变量vim /etc/profile 在文件末尾增加：12export NODE_HOME=/usr/local/nodeexport PATH=$NODE_HOME/bin:$PATH 保存退出，并且执行source /etc/profile node npm验证12node -vnpm -v 参考来源 Linux(CentOS 6.5) 手动升级gcc到gcc-6.1.0 CentOS6.9编译安装Node.js8.4","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"E大春节特辑整理","slug":"2019-05-08-E大春节特辑整理","date":"2019-06-26T03:40:39.255Z","updated":"2019-06-26T03:40:39.255Z","comments":true,"path":"2019/06/26/2019-05-08-E大春节特辑整理/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-05-08-E大春节特辑整理/","excerpt":"","text":"E大春节特辑整理首先感谢E大的精彩文章，他把上一轮10年以来的投资经历总结了一下，我读第二遍的时候，用思维导图记录一下，方便以后回顾。读完这个春节特辑系列，对我的启发有以下几点： 收益不要和别人比较，衡量自己投资的标准：账户有没有创新高。 耐心，抓住大机会（比如08年左右的封闭基金、15年的分级A），这种风险收益不对等的机会，才是真正的机会。 来源E大原文链接","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"变量-看见中国社会小趋势","slug":"2019-04-17-中国社会小趋势","date":"2019-06-26T03:40:39.253Z","updated":"2019-06-26T03:40:39.253Z","comments":true,"path":"2019/06/26/2019-04-17-中国社会小趋势/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-04-17-中国社会小趋势/","excerpt":"","text":"变量-看见中国社会小趋势这本书讲述了2018年中国社会的5个小趋势： 大国博弈 技术赋能 新旧融合（互联网和传统行业的融合） 自下而上（维护城市系统多样性，多核城市） 重建社群","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"不落俗套的成功","slug":"2019-04-17-不落俗套的成功","date":"2019-06-26T03:40:39.251Z","updated":"2019-06-26T03:40:39.251Z","comments":true,"path":"2019/06/26/2019-04-17-不落俗套的成功/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-04-17-不落俗套的成功/","excerpt":"","text":"不落俗套的成功本书是久负盛名的大卫斯文森编著。书中讲述了核心资产类别、非核心资产类别、投资组合如何构建、再平衡原理，后面几章主要对主动管理型基金进行了抨击。作者认为高收益债、垃圾债是典型的收益风险不对等，不是好的投资机会（你看上了别人的利息，别人看上了你的本金），这一点对我比较有用。后续固定收益投资债券， 主要就投利率债，没必要自我增加难度投资企业债 、信用债。整本书的大纲如下所示，下一本书攻读机构投资创新之路。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"债券投资","slug":"2019-03-27-债券基金投资","date":"2019-06-26T03:40:39.248Z","updated":"2019-06-26T03:40:39.249Z","comments":true,"path":"2019/06/26/2019-03-27-债券基金投资/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-27-债券基金投资/","excerpt":"","text":"债券投资根据沈潜老师的公众号文章，整理了一份债券基金投资文章。主要涉及，债券的种类、债券估值方法、债券基金选择参考因素、债券优秀的基金。 整理相关思维导图如下： 参考沈潜公众号","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"dev十八般武器05-高并发设计","slug":"2019-03-26-dev十八般武器05-高并发设计","date":"2019-06-26T03:40:39.246Z","updated":"2019-06-26T03:40:39.246Z","comments":true,"path":"2019/06/26/2019-03-26-dev十八般武器05-高并发设计/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-26-dev十八般武器05-高并发设计/","excerpt":"","text":"高并发设计 系统拆分：大系统拆分成小系统，每个系统用dubbo互联。 缓存：读多写少的场景适用缓存，单机redis几万QPS，cluster10W QPS。 MQ：大量写请求先写入MQ ，进行降峰，单机能扛几万QPS。 分库分表：多库扛并发，多表sql性能会提高。 读写分离：主库写，从库读。 ES 查询：天生分布式、高并发。简单查询 统计、全文检索。 高并发设计相关知识点思维导图如下所示： 参考 github advanced-java","categories":[],"tags":[{"name":"架构","slug":"架构","permalink":"http://yoursite.com/tags/架构/"}]},{"title":"dev十八般武器04-分库分表","slug":"2019-03-25-dev十八般武器04-分库分表","date":"2019-06-26T03:40:39.244Z","updated":"2019-06-26T03:40:39.244Z","comments":true,"path":"2019/06/26/2019-03-25-dev十八般武器04-分库分表/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-25-dev十八般武器04-分库分表/","excerpt":"","text":"分库分表分库分表分为水平拆分和垂直拆分。mysql单库可以支持并发1000，构建系统时，可以直接一上来就构建32个库，每个库32张表，一般互联网企业都够用了。 分库分表常用开源库 sharding-jdbc jar包形式不需要单独部署 mycat 需要中间件单独部署 分库分表主键ID 生成 数据库自增id 适合并发不大，数据量大的场景。 设置数据库sequence 并设定固定的步长。实现简单 能满足性能要求，但是扩容性差。 snowflake算法 分库分表相关知识点思维导图如下所示： mysql主从分离一主多从，主库写入，从库负责读。主库将binlog传给从库，从库再执行一遍主库的binlog。高并发下，主库刚写入的数据，从库可能读不到。show status Seconds_Behind_Master 命令可以看到从库落后的时间。分库后，每个主库并发降低，同步时间会变小。 mysql主从分离相关知识点思维导图如下所示： 参考 github advanced-java","categories":[],"tags":[{"name":"架构","slug":"架构","permalink":"http://yoursite.com/tags/架构/"}]},{"title":"dev十八般武器03-redis总结","slug":"2019-03-25-dev十八般武器03-redis总结","date":"2019-06-26T03:40:39.242Z","updated":"2019-06-26T03:40:39.242Z","comments":true,"path":"2019/06/26/2019-03-25-dev十八般武器03-redis总结/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-25-dev十八般武器03-redis总结/","excerpt":"","text":"redis目前推荐搭建直接使用redis cluster，redis持久化方式（RDB和AOF同时使用）。redis 单机能支持上万QPS，cluster能支持10WQPS。 redis相关知识点思维导图如下所示： 参考 github advanced-java","categories":[],"tags":[{"name":"架构","slug":"架构","permalink":"http://yoursite.com/tags/架构/"}]},{"title":"dev十八般武器02-缓存","slug":"2019-03-20-dev十八般武器02缓存技术","date":"2019-06-26T03:40:39.240Z","updated":"2019-06-26T03:40:39.240Z","comments":true,"path":"2019/06/26/2019-03-20-dev十八般武器02缓存技术/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-20-dev十八般武器02缓存技术/","excerpt":"","text":"缓存技术针对数据修改，切记应该先删除缓存，然后再修改db中的数据。 缓存相关知识点思维导图如下所示： 参考 github advanced-java","categories":[],"tags":[{"name":"架构","slug":"架构","permalink":"http://yoursite.com/tags/架构/"}]},{"title":"dev十八般武器01-ES搜索引擎","slug":"2019-03-15-dev十八般武器01ES搜索引擎","date":"2019-06-26T03:40:39.238Z","updated":"2019-06-26T03:40:39.238Z","comments":true,"path":"2019/06/26/2019-03-15-dev十八般武器01ES搜索引擎/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-15-dev十八般武器01ES搜索引擎/","excerpt":"","text":"ES搜索引擎ES是分布式搜索引擎，本质上就是在多台机器上使用Lucence搭建一个集群，共同完成检索功能。ES本身是做搜索使用，不是一种数据存储方案。因为受制于OS filesystme cache大小，所以只适合存储重要的内容。 ES与mysql区别 两者检索keyword效率相近 mysql没有分词ES使用分词 ES查询内容效率就远远优于mysql（mysql针对大文本建索引也很难提高效率） ES应用场景 ES 可以对文章内容分词，然后根据分词建立倒排索引，即关键词-&gt;docID ，然后再根据docID 直接检索比如mysql或者hbase、mongodb。 再次强调ES中的内容越少越好本身不是数据存储解决方案。 ES搜索引擎相关知识点思维导图如下所示： 参考 github advanced-java","categories":[],"tags":[{"name":"架构","slug":"架构","permalink":"http://yoursite.com/tags/架构/"}]},{"title":"有效资产管理","slug":"2019-03-04-有效资产管理","date":"2019-06-26T03:40:39.236Z","updated":"2019-06-26T03:40:39.236Z","comments":true,"path":"2019/06/26/2019-03-04-有效资产管理/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-03-04-有效资产管理/","excerpt":"","text":"有效资产管理本书推荐星级：五星。 资产配置顺序 评估自己对风险的承受能力，承受能力越高组合中股票的仓位应该越重。个人能承受30%的回撤，则股票仓位可以做到70%。 评估自己对各类资产的熟悉程度，越熟悉可配置的资产种类就越多。 评估自己承受跑输大盘的心理承受能力，看自己是否能够克服攀比心理。如果克服能力较弱则多配置大盘股。 组合配置起来后可以根据估值动态调整组合，但是一定需要跟估值相反的方向去操作。 具体执行计划时可以采用价值平均法，资产价格跌的越多，则购入的资产可以越多。 定期（每年一次）再平衡实现被动买低卖高。 各类资产特点 小市值价值股长期收益更高、但是波动更大。 优先选择低成本指数，主动型基金经理长期无法取得优异成绩。 投资组合-方差均值收益分析，真正目的是找到各种情况下都不会与目标相差太远的组合，而不是找到最优的数值。 长期价值股一定会战胜成长股，小市值价值股收益会超过大市值价值股。 fama三因素模型：价值溢价、市值规模溢价、市场溢价。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"钱-7步创造终身收入","slug":"2019-02-25-钱-7步创造终身收入","date":"2019-06-26T03:40:39.233Z","updated":"2019-06-26T03:40:39.233Z","comments":true,"path":"2019/06/26/2019-02-25-钱-7步创造终身收入/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-02-25-钱-7步创造终身收入/","excerpt":"","text":"钱-7步创造终身收入近来读真本书，作者是一位成功学作者，写的理财投资书籍也是鸡汤味很足。此书应该是主要面向一些初次有理财意识的读者。作者的建议是从收入中拿出固定部分比例作为储蓄，参照着大师们的方法，建立起自己的投资组合，按照财务安全、财务独立、财务活力、财务自由、完全财务自由的路径去努力。我本想依照此书去建立自己的投资组合，但是读完觉着此书虽然涉及投资组合，但是深度不够，还需要再找资料书籍去学习。我用书中介绍的全天候投资策略，在聚宽上进行了回测，链接文章如下。全天候策略回测","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"指数季度再平衡VaR计算阈值","slug":"2019-02-20-指数季度再平衡VaR计算阈值","date":"2019-06-26T03:40:39.231Z","updated":"2019-06-26T03:40:39.231Z","comments":true,"path":"2019/06/26/2019-02-20-指数季度再平衡VaR计算阈值/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-02-20-指数季度再平衡VaR计算阈值/","excerpt":"","text":"指数季度再平衡VaR计算阈值统计指数或者公募基金季度涨跌幅绝对值，利用历史模拟法来计算VaR。（数据量其实很小，历史模拟法计算VaR其实不太准确。） 债券指数季度波动百分位中证转债中证转债60% 2.03719501792中证转债70% 3.29159821641中证转债80% 4.3773365758中证转债90% 8.29333619855中证转债95% 20.5748970974 嘉实债券嘉实债券60% 1.85960018596嘉实债券70% 2.09946747284嘉实债券80% 2.78846153846嘉实债券90% 4.39033606107嘉实债券95% 8.31798657718 股权指数季度波动百分位上证指数上证指数60% 14.8274104591上证指数70% 15.5577932804上证指数80% 25.7916497614上证指数90% 33.0653696239上证指数95% 42.8471025626 中证红利中证红利60% 8.56426112829中证红利70% 13.1707757789中证红利80% 16.7441791078中证红利90% 21.333241314中证红利95% 28.1007519348 沪深300沪深30060% 15.196126804沪深30070% 17.6015920852沪深30080% 24.8591867553沪深30090% 32.0435354157沪深30095% 54.5253233311 中证500中证50060% 11.6444303957中证50070% 17.0972691951中证50080% 19.6970809055中证50090% 28.4480590537中证50095% 51.4002148199 黄金季度波动VaR易基黄金60% 3.56200527704易基黄金70% 3.77094972067易基黄金80% 6.31136044881易基黄金90% 7.17092337917易基黄金95% 15.5591572123 季度波动再平衡阈值选择组合采用年初固定时间再平衡策略，但是年内如果季度内相关资产涨幅过大，超过阈值且资产估值较高，则采用年内再平衡策略。 根据上述数据，股权基金年内再平衡阈值为波动正负30%，黄金为10%，普通债券类为10%，可转债为15%。","categories":[],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/tags/数据分析/"}]},{"title":"全天候投资策略简化版","slug":"2019-02-19-简化版全天候投资策略","date":"2019-06-26T03:40:39.229Z","updated":"2019-06-26T03:40:39.229Z","comments":true,"path":"2019/06/26/2019-02-19-简化版全天候投资策略/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-02-19-简化版全天候投资策略/","excerpt":"","text":"全天候投资策略简化版《钱七步创造终身收入》这本书里，dalio 给作者提供了一份简化版的全天候配置策略。策略配置如下： 30%的股票 40%的长期国债 15%的中期国债 7.5%的黄金 7.5%的商品每年至少再平衡一次，本篇文章采用的是年初再平衡。 投资标的选择股票选择的是大成中证红利090010、长期国债选用南方十年国债160123、中期国债用的嘉实短债070009（中期债成立时间太短了）、商品用的华宝油气162411、黄金用的160719。 回测效果回测时间是2012010-20190218，策略收益是36.68%，基准收益沪深300是42.33%，最大回撤是19.75%，悲剧的是没跑过基准。 熊市回测相同策略改变回测时间，2016年1月1日至2019年2月18日，期间没有一轮大行情，策略年化收益18.16%，基准收益-11.33%，最大回撤4.69%。可见在熊市里，全天候策略确实效果很好，但是一旦回测区间中有一轮大牛市，哪怕是一轮快牛，急涨急跌，全天候策略就不如沪深300了。 回测结果如图：","categories":[],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/tags/数据分析/"}]},{"title":"资产配置的艺术读书笔记","slug":"2019-02-12-资产配置的艺术","date":"2019-06-26T03:40:39.227Z","updated":"2019-06-26T03:40:39.227Z","comments":true,"path":"2019/06/26/2019-02-12-资产配置的艺术/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-02-12-资产配置的艺术/","excerpt":"","text":"资产配置的艺术读书笔记春节期间打算研究下大类资产配置，因为我觉着靠选股去追求alpha对业余投资者来说不现实，不如踏踏实实配置大类资产，追求暴富是徒劳的。京东上搜了搜发现这本书名字比较match，于是kindle下载电子版读之。 读下来感觉这本书还是写的比较浅，偏概念偏理论。没有形成一个可以实践的投资体系，以下面的思维导图记录下重要的知识点。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"低相关性指数构建资产组合","slug":"2019-02-11-低相关性指数构建资产组合","date":"2019-06-26T03:40:39.225Z","updated":"2019-06-26T03:40:39.225Z","comments":true,"path":"2019/06/26/2019-02-11-低相关性指数构建资产组合/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-02-11-低相关性指数构建资产组合/","excerpt":"","text":"低相关性指数构建资产组合本文在聚宽平台上计算16年初到2019年3年时间里各个指数的每日涨幅相关性矩阵，从中人为选择出相关性较低的矩阵，按照人为设定的比例构建出一个投资组合，并回测了16年至今的收益。 指数相关性矩阵图 如上图所示组合中相关性最高的两只指数：中证红利和富国可转债的相关系数为0.62，0.62数值上属于中低相关，各相关指数整体非常低。 资产组合 黄金 5% 000216 大宗商品 5% 162411 国内股权 35% 090010-35% 国内债券 25% 100051 国际股权 30% 510900-10%、000614-10%、164906-10% 回测详情因为本组合都是采用的公募基金，申赎需要T+n确认，所以本回测在每年年初第一周先赎回超出比例的公募基金，然后7个交易日后，再申购对应金额（金额跟7日前赎回的金额相同）的公募基金。回测结果如下：策略收益 18.51%年化收益5.79%基准沪深300 -14.97%最大回撤20.42% 回测图示：","categories":[],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/tags/数据分析/"}]},{"title":"深入浅出数据分析01","slug":"2019-01-24-深入浅出数据分析01","date":"2019-06-26T03:40:39.223Z","updated":"2019-06-26T03:40:39.223Z","comments":true,"path":"2019/06/26/2019-01-24-深入浅出数据分析01/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-01-24-深入浅出数据分析01/","excerpt":"","text":"深入浅出数据分析01数据分析过程 确定问题 分解,分解问题和数据，使其成为更小的组成部分。 评估，在此步骤对前两部了解到的情况作出结论。 决策，把结论重新组合在一起，作出一个决策。 统计和分析最基本的原理就是比较，数据只有通过相互比较才会有意义。 控制组控制组是一组体现现状的处理对象，未经过任何处理。控制组应该选用同期的数据。以控制组为基准，没有控制组就意味着没有比较，没有比较就意味着无法对所发生的情况进行判断。 星巴克销量实验三月份，营销部把所有星巴克分店分成了控制组和对照组。 实验组，太平洋地区，咖啡$4-&gt;3.75$ 控制组，东安区，咖啡$4展示的结果如图1。 混杂因素混杂因素是所研究的各个组之间的差异，而不是试图进行比较的因素。可能的混杂因素： 文化，所有分店的文化应该相同 店址，肯定是混杂因素 咖啡温度，每家分店的温度应该一样 天气，有可能。上述实验将星巴克按区进行分组，会引入店址混杂因素，需要重新划分。 重新划分实验组控制组 轮流按不同价格给客户结账，不可行。客户体验太差。 历史控制法，这个月的所有店作为实验组，下个月的店作为控制组。历史控制法会带来问题。 将不同的店随机分配给实验组或者控制组。并不十分恰当，人们只会去便宜的店，而不会去控制组，店址仍然是混杂因素。 将大的地理区域分成小的地理区域，随机将这些微区域分解实验组和控制组。 只要分割区域足够大，使人们不至于为了喝上便宜的咖啡而四处奔波。同时又足够小，使得各个分割区域相似，就避开了店址混杂因素。 随机选择相似组从对象中随机选择对象是避免混杂因素的好办法，可能成为混杂因素的那些因素最终在实验组和控制组同票同权。 实验步骤 将数据表划分成微区域 将微区域随机分配给控制组和实验组 控制组：维持现状一个月 实验组1：降价一个月 实验组2：游说顾客星巴克很有价值一个月 收集结果 组与组之间进行相互比较分析结果","categories":[],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/tags/数据分析/"}]},{"title":"微服务架构学习28-微服务总结","slug":"2019-01-24-微服务架构学习28-微服务总结","date":"2019-06-26T03:40:39.220Z","updated":"2019-06-26T03:40:39.221Z","comments":true,"path":"2019/06/26/2019-01-24-微服务架构学习28-微服务总结/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-01-24-微服务架构学习28-微服务总结/","excerpt":"","text":"微服务架构学习28-微服务总结","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"巴比伦富翁的理财课","slug":"2019-01-21-巴比伦富翁理财课","date":"2019-06-26T03:40:39.218Z","updated":"2019-06-26T03:40:39.218Z","comments":true,"path":"2019/06/26/2019-01-21-巴比伦富翁理财课/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-01-21-巴比伦富翁理财课/","excerpt":"","text":"巴比伦富翁的理财课这本小书整体上，号召控制消费，坚持量入为出。对必需消费做出合理规划，对不切实际的欲望予以摒弃。在量入为出的基础上，尽量多留一些结余资金用于理财增值。但是理财增值需要向一些经验丰富的人请教，避免造成损失。作者建议人们拥有自己的住房，以提高生活质量。 知识结构图如下所示：","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"小韭资产配置","slug":"2019-01-21-小韭资产配置","date":"2019-06-26T03:40:39.216Z","updated":"2019-06-26T03:40:39.216Z","comments":true,"path":"2019/06/26/2019-01-21-小韭资产配置/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-01-21-小韭资产配置/","excerpt":"","text":"小韭资产配置读书笔记周末读了下小韭的资产配置课程，虽说比较基础，但是还是有很多收获的。 一年内需要使用的钱，优先配置银行理财。 建议普通投资者等权重配置各类资产，对于我自己来讲。股权还是需要占大比重的，具体到股权细分类重点配置A股，适当配置QDII（DAX30、华宝油气、H股、标普）。 债券是应当纳入到资产配置体系中，债券基金除了能获得利息外，另一种收益来源是资本利得就是买卖差价。 下一步需要研究的地方： 黄金白银等贵金属如何衡量贵贱（黄金最多配置5%）。 债券基金如何衡量贵贱。 利用10年国债收益率平均指标。参考链接 知识结构图如下所示：","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"设计模式25-总结","slug":"2019-01-17-设计模式总结","date":"2019-06-26T03:40:39.214Z","updated":"2019-06-26T03:40:39.214Z","comments":true,"path":"2019/06/26/2019-01-17-设计模式总结/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-01-17-设计模式总结/","excerpt":"","text":"设计模式25-总结通读完《大话设计模式》全书，针对每一个设计模式做了博客记录。设计模式分为创建型模式、结构型模式、行为型模式，相比较而言行为型模式比较灵活，在实操中比较有实践价值。需要重点掌握的设计模式：工厂方法、桥接模式、观察者模式、模板方法模式、状态模式、职责链模式。设计模式还是需要经常温习的，上面几种设计模式需要重点掌握。 总共23种设计模式架构图如下所示：","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"2019目标规划","slug":"2019-01-02-2019年学习目标","date":"2019-06-26T03:40:39.211Z","updated":"2019-06-26T03:40:39.211Z","comments":true,"path":"2019/06/26/2019-01-02-2019年学习目标/","link":"","permalink":"http://yoursite.com/2019/06/26/2019-01-02-2019年学习目标/","excerpt":"","text":"2019小目标首先过一下2018年的目标完成情况。 2018目标完成情况技术方面 掌握vue框架，能使用vue实现功能。（实现） 学习使用ELK日志分析系统。（实现） CICD-学习使用jenkins、Travis。（实现） nginx open resty开发。（部分实现） mongodb （实现） java8新特性比如lambda （实现） docker 阿里云部署实践docker （实现） java设计模式 （实现） nodejs学习并实践 （实现） 按照前端开发栈学习es6 webpack等基础技术（实现） 2019年目标技术 headfirst数据分析 docker技术 go语言学习 深度学习 tensorflow 金融 FRM看完handbook 书籍 至少写12篇读书笔记附带思维导图。 个人实践继续完善运维滚雪球网站，结合孙老湿公众号的量化分析技术，逐步完善工具。 个股相对沪深300的alpha、beta。 工作&amp;家庭 工作上还是坚持每天工作11小时以上（什么工作干久了都会枯燥，要学会克服）。 家庭上没说的，有一点时间就陪孩子。","categories":[],"tags":[{"name":"规划","slug":"规划","permalink":"http://yoursite.com/tags/规划/"}]},{"title":"设计模式24-访问者模式","slug":"2018-12-27-设计模式24-访问者模式","date":"2019-06-26T03:40:39.209Z","updated":"2019-06-26T03:40:39.209Z","comments":true,"path":"2019/06/26/2018-12-27-设计模式24-访问者模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-27-设计模式24-访问者模式/","excerpt":"","text":"设计模式24-访问者模式访问者模式，表示一个作用于某对象结构中的各个元素的操作。它使你可以在不改变各元素的前提下定义作用于这些元素的新操作。访问者模式适用于数据结构相对稳定的系统。它把数据结构和作用于结构上的操作之间的耦合解脱开，使得操作集合可以相对自由地演化。 访问者目的是要把处理从数据结构分离出来，很多系统可以按照算法和数据结构分开，如果这样的系统有比较稳定的数据结构（比如下面示例代码中的Man 和Woman类），又有易于变化的算法，使用访问者模式比较合适。 示例代码Action类1234public interface Action &#123; public abstract void getManConclusion(Man man); public abstract void getwoManConclusion(Woman woman);&#125; Action 具体类 Failing1234567891011public class Failing implements Action&#123; @Override public void getManConclusion(Man man) &#123; System.out.println(&quot;man failing&quot;); &#125; @Override public void getwoManConclusion(Woman woman) &#123; System.out.println(&quot;woman failing&quot;); &#125;&#125; Action 具体类 Success1234567891011public class Success implements Action &#123; @Override public void getManConclusion(Man man) &#123; System.out.println(&quot;man success&quot;); &#125; @Override public void getwoManConclusion(Woman woman) &#123; System.out.println(&quot;woman success&quot;); &#125;&#125; Person类123public abstract class Person &#123; public abstract void accept(Action vistor);&#125; Man类123456public class Man extends Person &#123; @Override public void accept(Action vistor) &#123; vistor.getManConclusion(this); &#125;&#125; Woman类123456public class Woman extends Person &#123; @Override public void accept(Action vistor) &#123; vistor.getwoManConclusion(this); &#125;&#125; ActionMan测试类1234567891011121314151617181920public class ActionMain &#123; public static void main(String [] args)&#123; Action failing = new Failing(); Action success = new Success(); Man man = new Man(); man.accept(failing); man.accept(success); Woman woman = new Woman(); woman.accept(success); woman.accept(failing); failing.getManConclusion(man); failing.getwoManConclusion(woman); success.getManConclusion(man); success.getwoManConclusion(woman); &#125;&#125;","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式23-解释器模式","slug":"2018-12-27-设计模式23-解释器模式","date":"2019-06-26T03:40:39.207Z","updated":"2019-06-26T03:40:39.207Z","comments":true,"path":"2019/06/26/2018-12-27-设计模式23-解释器模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-27-设计模式23-解释器模式/","excerpt":"","text":"设计模式23-解释器模式解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。如果特定类型的问题发生的频率足够高，就值得将该问题的各个实例表述为一个简单语言中的句子。 比如识别校验字符串的正则表达式。 代码实例Context 样例1234567891011121314151617181920public class Context &#123; private String input; private String output; public String getInput() &#123; return input; &#125; public void setInput(String input) &#123; this.input = input; &#125; public String getOutput() &#123; return output; &#125; public void setOutput(String output) &#123; this.output = output; &#125;&#125; TerminalExpression 终结符类123456public class TerminalExpression implements AbstractExpression&#123; @Override public void interpreter(Context context) &#123; System.out.println(&quot;终端解释器&quot;); &#125;&#125; NonterminalExpression 非终结符类123456public class NonterminalExpression implements AbstractExpression&#123; @Override public void interpreter(Context context) &#123; System.out.println(&quot;非终端解释器&quot;); &#125;&#125; 测试类12345678910111213public class TerminalMain &#123; public static void main(String[] args) &#123; Context context = new Context(); List&lt;AbstractExpression&gt; list = new ArrayList&lt;AbstractExpression&gt;(); list.add(new TerminalExpression()); list.add(new NonterminalExpression()); list.add(new TerminalExpression()); list.add(new TerminalExpression()); list.stream().forEach((element) -&gt; &#123; element.interpreter(context); &#125;); &#125;&#125;","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式22-享元模式","slug":"2018-12-27-设计模式22-享元模式","date":"2019-06-26T03:40:39.204Z","updated":"2019-06-26T03:40:39.205Z","comments":true,"path":"2019/06/26/2018-12-27-设计模式22-享元模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-27-设计模式22-享元模式/","excerpt":"","text":"设计模式22-享元模式享元模式：运用共享技术有效地支持大量细粒度的对象。 代码实例Flyweight 享元接口123public interface Flyweight &#123; public void operation(int extrinsicstate);&#125; ConcreteFlyweight 享元接口实现类123456public class ConcreteFlyweight implements Flyweight &#123; @Override public void operation(int extrinsicstate) &#123; System.out.println(&quot;具体的flyweight&quot; + extrinsicstate); &#125;&#125; UnsharedConcreteFlyweight 非共享类123456public class UnsharedConcreteFlyweight implements Flyweight &#123; @Override public void operation(int extrinsicstate) &#123; System.out.println(&quot;不共享的具体FlyWeight&quot; + extrinsicstate); &#125;&#125; FlyweightFactory享元工厂类12345678910111213public class FlyweightFactory &#123; private Map&lt;String, Flyweight&gt; flyweightMap = new HashMap&lt;String, Flyweight&gt;(); public FlyweightFactory() &#123; this.flyweightMap.put(&quot;X&quot;,new ConcreteFlyweight()); this.flyweightMap.put(&quot;Y&quot;,new ConcreteFlyweight()); this.flyweightMap.put(&quot;Z&quot;,new ConcreteFlyweight()); &#125; public Flyweight getFlyweightInstance(String key)&#123; return flyweightMap.get(key); &#125;&#125; 测试类1234567891011121314151617public class FlyweightMain &#123; public static void main(String[] args) &#123; int ext = 22; FlyweightFactory flyweightFactory = new FlyweightFactory(); Flyweight flyweightX = flyweightFactory.getFlyweightInstance(&quot;X&quot;); flyweightX.operation(--ext); Flyweight flyweightY = flyweightFactory.getFlyweightInstance(&quot;Y&quot;); flyweightY.operation(--ext); Flyweight flyweightZ = flyweightFactory.getFlyweightInstance(&quot;Z&quot;); flyweightZ.operation(--ext); UnsharedConcreteFlyweight unsharedConcreteFlyweight = new UnsharedConcreteFlyweight(); unsharedConcreteFlyweight.operation(--ext); &#125;&#125; 应用场景 一个应用程序使用了大量的对象，大量的这些对象造成了很大的存储开销时就应该考虑使用。 对象的大部分状态是外部状态，如果删除对象的外部状态，可以用相对较少的共享对象取代很多组对象，此时可以考虑使用享元模式。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式21-中介者模式","slug":"2018-12-27-设计模式21-中介者模式","date":"2019-06-26T03:40:39.202Z","updated":"2019-06-26T03:40:39.202Z","comments":true,"path":"2019/06/26/2018-12-27-设计模式21-中介者模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-27-设计模式21-中介者模式/","excerpt":"","text":"设计模式21-中介者模式中介者模式(Mediator)，用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 实例代码Mediator抽象中介者类123public abstract class Mediator &#123; public abstract void send(String message , Colleague colleague);&#125; 同事对象抽象类 Colleague1234567public abstract class Colleague &#123; protected Mediator mediator; public Colleague(Mediator mediator)&#123; this.mediator = mediator; &#125;&#125; 具体同事类 ConcreteColleagueOne1234567891011121314public class ConcreteColleagueOne extends Colleague &#123; public ConcreteColleagueOne(Mediator mediator) &#123; super(mediator); &#125; public void send(String message) &#123; mediator.send(message, this); &#125; public void notify(String message) &#123; System.out.println(&quot;同事1得到的消息&quot; + message); &#125;&#125; 具体同事类 ConcreteColleagueTwo1234567891011121314public class ConcreteColleagueTwo extends Colleague &#123; public ConcreteColleagueTwo(Mediator mediator) &#123; super(mediator); &#125; public void send(String message) &#123; mediator.send(message, this); &#125; public void notify(String message) &#123; System.out.println(&quot;同事2得到的消息&quot; + message); &#125;&#125; 具体中介者类123456789101112131415161718192021222324252627282930public class ConcreteMediator extends Mediator &#123; private ConcreteColleagueOne concreteColleagueOne; private ConcreteColleagueTwo concreteColleagueTwo; public ConcreteColleagueOne getConcreteColleagueOne() &#123; return concreteColleagueOne; &#125; public void setConcreteColleagueOne(ConcreteColleagueOne concreteColleagueOne) &#123; this.concreteColleagueOne = concreteColleagueOne; &#125; public ConcreteColleagueTwo getConcreteColleagueTwo() &#123; return concreteColleagueTwo; &#125; public void setConcreteColleagueTwo(ConcreteColleagueTwo concreteColleagueTwo) &#123; this.concreteColleagueTwo = concreteColleagueTwo; &#125; @Override public void send(String message, Colleague colleague) &#123; if (colleague == concreteColleagueOne) &#123; concreteColleagueOne.notify(message); &#125; else &#123; concreteColleagueTwo.notify(message); &#125; &#125;&#125; 测试类1234567891011public class MediatorMain &#123; public static void main(String [] args)&#123; ConcreteMediator concreteMediator = new ConcreteMediator(); ConcreteColleagueOne concreteColleagueOne = new ConcreteColleagueOne(concreteMediator); ConcreteColleagueTwo concreteColleagueTwo = new ConcreteColleagueTwo(concreteMediator); concreteMediator.setConcreteColleagueOne(concreteColleagueOne); concreteMediator.setConcreteColleagueTwo(concreteColleagueTwo); concreteColleagueOne.send(&quot;我是同事1&quot;); concreteColleagueTwo.send(&quot;我是同事2&quot;); &#125;&#125; 中介者模式的优缺点优点： Mediator的出现减少了各个Colleague的耦合，使得可以独立地改变和复用各个Colleague和Mediator。 由于把对象如何协作进行了抽象，将中介作为一个独立的概念并封装在一个对象中，这样关注的对象就从对象各自本身的行为转移到它们之间的交互上来了。缺点： 由于ConcreteMediator控制了集中化，于是就把交互复杂性变为了中介者的复杂性。这就使得中介者会变得比任何一个ConcreteColleague 都复杂。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式20-职责链模式","slug":"2018-12-18-设计模式20-职责链模式","date":"2019-06-26T03:40:39.200Z","updated":"2019-06-26T03:40:39.200Z","comments":true,"path":"2019/06/26/2018-12-18-设计模式20-职责链模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-18-设计模式20-职责链模式/","excerpt":"","text":"设计模式20-职责链模式职责链模式：使多个对象都有机会处理请求，从而避免请求的发送者与接收者之间的耦合关系。将这个对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。 示例代码Handler定义一个接收请求的接口123456789public abstract class Handler &#123; protected Handler successor; public void setSuccessor(Handler successor) &#123; this.successor = successor; &#125; // 处理请求的抽象方法 public abstract void handleRequest(int request);&#125; ConcreteHandlerOne 处理0-10的接口12345678910public class ConcreteHandlerOne extends Handler &#123; @Override public void handleRequest(int request) &#123; if (request &gt;= 0 &amp;&amp; request &lt; 10) &#123; System.out.println(&quot;处理请求One &quot; + request); &#125; else if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; ConcreteHandlerTwo 处理10-20的接口12345678910public class ConcreteHandlerTwo extends Handler &#123; @Override public void handleRequest(int request) &#123; if (request &gt;= 10 &amp;&amp; request &lt; 20) &#123; System.out.println(&quot;处理请求Two&quot; + request); &#125; else if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; ConcreteHandlerThree 处理20-30的接口12345678910public class ConcreteHandlerThree extends Handler &#123; @Override public void handleRequest(int request) &#123; if (request &gt;= 20 &amp;&amp; request &lt; 30) &#123; System.out.println(&quot;处理请求Three&quot; + request); &#125; else if (successor != null) &#123; successor.handleRequest(request); &#125; &#125;&#125; 测试类123456789101112public class HandlerMain &#123; public static void main(String[] args) &#123; Handler handlerOne = new ConcreteHandlerOne(); Handler handlerTwo = new ConcreteHandlerTwo(); Handler handlerThree = new ConcreteHandlerThree(); handlerOne.setSuccessor(handlerTwo); handlerTwo.setSuccessor(handlerThree); handlerOne.handleRequest(8); handlerOne.handleRequest(15); handlerOne.handleRequest(25); &#125;&#125; 运行结果123处理请求One 8处理请求Two15处理请求Three25 职责链的优点接收者和发送者都没有对方的明确信息，且链中的对象自己也并不知道链的结构。职责链可以简化对象的相互连接，它们仅需要保持一个指向其后继者的引用，而不需要保持所有可选接受者的引用。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式19-命令模式","slug":"2018-12-18-设计模式19-命令模式","date":"2019-06-26T03:40:39.198Z","updated":"2019-06-26T03:40:39.198Z","comments":true,"path":"2019/06/26/2018-12-18-设计模式19-命令模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-18-设计模式19-命令模式/","excerpt":"","text":"设计模式19-命令模式命令模式（Command），将一个请求封装成一个对象，从而使你可用不同的请求对客户进行参数化。对请求排队或记录请求日志，以及支持可撤销的操作。 实例代码Command类用来声明执行操作的接口123456789public abstract class Command &#123; protected Receiver receiver; public Command(Receiver receiver) &#123; this.receiver = receiver; &#125; abstract public void execute();&#125; ConcreteCommand 类，将一个接受者对象绑定于一个动作，调用接收者相应的操作，以实现Execute。12345678910public class ConcreteCommand extends Command&#123; public ConcreteCommand(Receiver receiver) &#123; super(receiver); &#125; @Override public void execute() &#123; receiver.action(); &#125;&#125; Invoker 要求该命令执行这个请求1234567891011public class Invoker &#123; private Command command; public void setCommand(Command command) &#123; this.command = command; &#125; public void executeCommand() &#123; command.execute(); &#125;&#125; Receiver类，知道如何实施与执行一个与请求有关的操作，任何类都可能作为一个接受者。12345public class Receiver &#123; public void action() &#123; System.out.println(&quot;执行请求!&quot;); &#125;&#125; CommandMain 类测试类12345678910public class CommandMain &#123; public static void main(String [] args)&#123; Receiver receiver = new Receiver(); Command command = new ConcreteCommand(receiver); // invoker相当于服务员类 控制命令执行 Invoker invoker = new Invoker(); invoker.setCommand(command); invoker.executeCommand(); &#125;&#125; 命令模式的作用 比较容易设计一个命令队列。 方便将命令计入日志。 允许接收请求的一方绝对是否否决请求。 可以容易实现对请求的撤销和重做。 容易增加新的具体命令类。 命令模式把请求一个操作的对象与知道怎么执行一个操作的对象分割开。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式18-单例模式","slug":"2018-12-18-设计模式18-单例模式","date":"2019-06-26T03:40:39.196Z","updated":"2019-06-26T03:40:39.196Z","comments":true,"path":"2019/06/26/2018-12-18-设计模式18-单例模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-18-设计模式18-单例模式/","excerpt":"","text":"设计模式18-单例模式单例模式，保证一个类仅有一个实例，并提供一个访问它的全局访问点。通常我们可以让一个全局变量使得一个对象被访问，但它不能防止你实例化多个对象。一个最好的办法是，让类自身负责保存它的唯一实例。这个类可以保证没有其他实例可以被创建，并且它可以提供一个访问该实例的方法。 样例代码Singleton单例类1234567891011121314public class Singleton &#123; private static Singleton instance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 测试类SingletonMain12345678910111213public class SingletonMain &#123; public static void main(String[] args) &#123; Singleton singleton1 = Singleton.getInstance(); Singleton singleton2 = Singleton.getInstance(); if (singleton1 == singleton2) &#123; System.out.println(&quot;same&quot;); &#125; else &#123; System.out.println(&quot;different&quot;); &#125; &#125;&#125; 线程安全方法同步构建线程安全123456789101112131415public class SingletonThreadOne &#123; private static SingletonThreadOne instance; private SingletonThreadOne() &#123; &#125; //同步方法 public static synchronized SingletonThreadOne getInstance() &#123; if (instance == null) &#123; instance = new SingletonThreadOne(); &#125; return instance; &#125;&#125; 使用双重同步锁12345678910111213141516171819public class SingleThreadTwo &#123; private static SingleThreadTwo instance; private SingleThreadTwo() &#123; &#125; //双重同步锁 public static synchronized SingleThreadTwo getInstance() &#123; if (instance == null) &#123; synchronized (SingleThreadTwo.class) &#123; if (instance == null) &#123; instance = new SingleThreadTwo(); &#125; &#125; &#125; return instance; &#125;&#125; 现在coding一般都是依靠spring框架实现单例模式。 参考设计模式之单例模式(线程安全)","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式17-迭代器模式","slug":"2018-12-10-设计模式17-迭代器模式","date":"2019-06-26T03:40:39.193Z","updated":"2019-06-26T03:40:39.193Z","comments":true,"path":"2019/06/26/2018-12-10-设计模式17-迭代器模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-10-设计模式17-迭代器模式/","excerpt":"","text":"设计模式17-迭代器模式迭代器模式：提供一种方法顺序访问一个聚合对象中各个元素，而又不暴露该对象的内部表示。为遍历不同的聚集结构提供如开始、下一个、是否结束、当前哪一项等统一的接口。 示例代码Aggregate聚集类接口123public interface Aggregate &#123; public Iterator createIterator();&#125; 聚集类实现123456789101112131415161718192021public class ConcreteAggregate&lt;T&gt; implements Aggregate &#123; private List&lt;T&gt; items = new ArrayList&lt;T&gt;(); @Override public Iterator createIterator() &#123; return new ConcreteIterator(this); &#125; public int count() &#123; return items.size(); &#125; public T getIndex(int index) &#123; return items.get(index); &#125; public void insert(T val, int index) &#123; items.add(index, val); &#125;&#125; 遍历接口123456789public interface Iterator &#123; public Object first(); public Object next(); public boolean isDone(); public Object currentItem();&#125; 遍历接口实现123456789101112131415161718192021222324252627282930313233public class ConcreteIterator implements Iterator &#123; private ConcreteAggregate aggregate; private int current = 0; public ConcreteIterator(ConcreteAggregate aggregate) &#123; this.aggregate = aggregate; &#125; @Override public Object first() &#123; return aggregate.getIndex(0); &#125; @Override public Object next() &#123; if (!isDone()) &#123; return aggregate.getIndex(current++); &#125; return null; &#125; @Override public boolean isDone() &#123; return current &gt;= aggregate.count() ? true : false; &#125; @Override public Object currentItem() &#123; return aggregate.getIndex(current); &#125;&#125; 测试类123456789101112131415public class IteratorMain &#123; public static void main(String[] args) &#123; ConcreteAggregate ca = new ConcreteAggregate(); ca.insert(&quot;hello0&quot;, 0); ca.insert(&quot;hello1&quot;, 1); ca.insert(&quot;hello2&quot;, 2); ca.insert(&quot;hello3&quot;, 3); ca.insert(&quot;hello4&quot;, 4); Iterator iter = new ConcreteIterator(ca); while (!iter.isDone()) &#123; System.out.println(iter.next()); &#125; &#125;&#125; 客户端代码通过访问Iterator接口实现遍历访问聚集类，而不需要关注ConcreteAggregate类的具体存储。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式16-备忘录模式","slug":"2018-12-10-设计模式16-备忘录模式","date":"2019-06-26T03:40:39.191Z","updated":"2019-06-26T03:40:39.191Z","comments":true,"path":"2019/06/26/2018-12-10-设计模式16-备忘录模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-10-设计模式16-备忘录模式/","excerpt":"","text":"设计模式16-备忘录模式备忘录：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可以将该对象恢复到原先保存的状态。 备忘录模式结构图Originator（发起人），负责创建一个备忘录Momento，用以记录当前时刻它的内部状态，并可使用备忘录恢复内部状态。Originator可以根据需要决定Memento存储Originator的哪些内部状态。 12345678910111213141516171819202122232425public class Originator &#123; private String state; // 创建备忘录，将当前需要保存的信息导入并实例化出一个Momento对象 public Momento saveMomento() &#123; return new Momento(state); &#125; // 恢复备忘录，将Momento导入并将相关数据恢复 public void restoreMomento(Momento momento) &#123; this.state = momento.getState(); &#125; public void show() &#123; System.out.println(&quot;State=&quot; + this.state); &#125; public String getState() &#123; return state; &#125; public void setState(String state) &#123; this.state = state; &#125;&#125; Momento备忘录：负责存储Originator对象的内部空间，可以防止Originator以外的其他对象访问备忘录Momento。备忘录有两个接口，Caretaker只能看到备忘录的窄接口，它只能将备忘录传递给其他对象。Originator可以看到一个宽接口，允许它访问返回到先前状态的所需的所有数据。 123456789101112131415public class Momento &#123; private String state; public Momento(String state) &#123; this.state = state; &#125; public String getState() &#123; return state; &#125; public void setState(String state) &#123; this.state = state; &#125;&#125; Caretaker 管理者：负责保存好备忘录Momento，不能对备忘录的内容进行操作或检查。1234567891011public class Caretaker &#123; private Momento momento; public Momento getMomento() &#123; return momento; &#125; public void setMomento(Momento momento) &#123; this.momento = momento; &#125;&#125; 测试类1234567891011121314151617181920public class MomentoMain &#123; public static void main(String[] args) &#123; Originator originator = new Originator(); System.out.println(&quot;初始值&quot;); originator.setState(&quot;ON&quot;); originator.show(); Caretaker ck = new Caretaker(); ck.setMomento(originator.saveMomento()); System.out.println(&quot;修改值&quot;); originator.setState(&quot;OFF&quot;); originator.show(); System.out.println(&quot;恢复&quot;); originator.restoreMomento(ck.getMomento()); originator.show(); &#125;&#125; 测试输出123456初始值State=ON修改值State=OFF恢复State=ON 适用场景适用于功能比较复杂的，但是需要维护或者记录属性历史的类，或者需要保存的属性只是众多属性的一小部分时，Originator可以根据保存的Momento信息还原到前一状态。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式15-适配器模式","slug":"2018-12-10-设计模式15-适配器模式","date":"2019-06-26T03:40:39.189Z","updated":"2019-06-26T03:40:39.189Z","comments":true,"path":"2019/06/26/2018-12-10-设计模式15-适配器模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-10-设计模式15-适配器模式/","excerpt":"","text":"设计模式15-适配器模式适配器，将一个类的接口转换成客户希望的另外一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。适配器主要用于复用一些现存的类，但是接口又与复用环境要求不一致的情况。 样例代码Target类，客户想要的类123public interface Target &#123; public void request();&#125; 需要适配的类Adaptee123456789/** * 需要适配的类 */public class Adaptee &#123; public void specificRequest() &#123; System.out.println(&quot;特殊需求&quot;); &#125;&#125; 适配器类 1234567public class Adapter implements Target&#123; @Override public void request() &#123; Adaptee adaptee = new Adaptee(); adaptee.specificRequest(); &#125;&#125; 测试类123456public class AdapterMain &#123; public static void main(String [] args)&#123; Target target = new Adapter(); target.request(); &#125;&#125; 使用场景两个类所做的事情相同或相似，但是具有不同的接口时要使用它。由于类都共享同一个接口，客户代码统一调用同一个接口即可。调用端和被调用端双方都不太容易修改的时候再使用适配器模式适配。在有小的接口不统一时，及时重构问题不至于扩大，只有碰到无法改变原有设计和代码的情况时，才考虑适配。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"指数roe分析","slug":"2018-12-10-指数roe分析","date":"2019-06-26T03:40:39.187Z","updated":"2019-06-26T03:40:39.187Z","comments":true,"path":"2019/06/26/2018-12-10-指数roe分析/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-10-指数roe分析/","excerpt":"","text":"指数roe分析数据源从聚宽网站获取，各个指数roe等相关指标使用的加权计算，根据每季度指数的权重，加权计算得出roe。之所以采用加权算法，因为指数本质上是模拟价格的，使用指数权重计算更合理。 上证50由图中看出，上证50 从12年到17年roe是稳定下降的，主要是资产周转率下降、杠杆下降，17年roe回升主要是资本周转率大幅上升。 中小板-中证50012-17年roe持续下降，roe17年上涨较多，主要由净利润率和周转率推升导致，猜测与供给侧改革、环保严格政策有关。一些化工股、与钢铁股等周期性行业盈利能力增强。 中证消费14年之后roe持续稳定上涨，净利润率同比逐渐走高，杠杆率和资产周转率保持稳定，是个不错的行业指数，保持稳定增长，可以给予相对高估值。 中证红利看着中证红利的指数roe，让我想起来房价的走势图，roe低谷对应房价降低的年份。2013年房价高点，2，中证红利当年roe也较高。2015年房价低点，当期中证红利roe也较低。猜测是房地产周期触发了 某些行业某些公司更高的分红，从而被纳入到红利指数了。 中证龙头 创业板roe在逐年走高，但是资产周转率和净利润率倒是变化不大，主要得益于杠杆提高。 医药100能看出来17年受医药政策影响，净利润率下滑，但是周转率却上升。药价降低，但是卖出的药多了，维持roe变化不大。17年年报roe提高较大，猜测与环保与供给侧改革政策相关，医药100指数杠杆系数一直是稳定走低的。 沪深30015年roe降低，猜测沪深300指数跟房地产短周期也有相关关系。 深证F60深证F60指数roe走势较健康，17年roe提高主要是净利润率以及周转率双提升。猜测也与供给侧改革以及环保严打相关。 总结 与房地产相关性较大的指数：中证红利、沪深300 与供给侧改革、环保政策相关的指数：中小板、中证500 医药100指数受医药改革政策影响较大，未来看能不能以价换量，保持roe平稳。 走势相对健康的指数：中证消费、深证F60。","categories":[],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/tags/数据分析/"}]},{"title":"设计模式14-状态模式","slug":"2018-12-03-设计模式14-状态模式","date":"2019-06-26T03:40:39.184Z","updated":"2019-06-26T03:40:39.184Z","comments":true,"path":"2019/06/26/2018-12-03-设计模式14-状态模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-12-03-设计模式14-状态模式/","excerpt":"","text":"设计模式14-状态模式状态模式：当一个对象的内在状态改变时允许改变其行为，这个对象看起来像是改变了其类。状态模式主要解决的是当控制一个对象状态转换的条件表达式过于复杂时的情况。把状态判断逻辑转移到表示不同状态的一系列类当中，可以把复杂的判断逻辑简化，避免过长的方法体。 将特定的状态相关的行为都放入到一个对象中，由于所有与状态相关的代码都存在于某个ConcreteState中，所以通过定义新的子类可以很容易地增加新的状态和转换。 状态模式通过把各种状态转移逻辑分布到State的子类之间，来减少相互间的依赖。 当一个对象的行为取决于它的状态，并且它必须在运行时刻根据状态改变它的行为时，可以考虑使用状态模式。 业务需求中某项业务有多个状态，状态的判断都是依靠大量的多分支判断语句来实现，可以考虑把每一种业务状态定义为一个State子类。避免以后频繁改需求的时候，改动代码较多。 实际代码State抽象类123public abstract class State &#123; public abstract void writePrograme(Work work);&#125; Work类，类似Context类12345678910111213141516171819202122232425262728293031323334353637383940public class Work &#123; private State current; public Work() &#123; this.current = new ForenoonState(); &#125; private double hour; private boolean finish = false; public void writeProgramme() &#123; current.writePrograme(this); &#125; public boolean isFinish() &#123; return finish; &#125; public void setFinish(boolean finish) &#123; this.finish = finish; &#125; public double getHour() &#123; return hour; &#125; public void setHour(double hour) &#123; this.hour = hour; &#125; public State getState() &#123; return current; &#125; public void setState(State current) &#123; this.current = current; &#125;&#125; ForenoonState上午类1234567891011public class ForenoonState extends State &#123; @Override public void writePrograme(Work work) &#123; if (work.getHour() &lt; 12) &#123; System.out.println(work.getHour() + &quot;上午工作精神百倍&quot;); &#125; else &#123; work.setState(new NoonState()); work.writeProgramme(); &#125; &#125;&#125; NoonState中午类1234567891011public class NoonState extends State &#123; @Override public void writePrograme(Work work) &#123; if (work.getHour() &lt; 13) &#123; System.out.println(work.getHour() + &quot;吃饭犯困午休&quot;); &#125; else &#123; work.setState(new AfternoonState()); work.writeProgramme(); &#125; &#125;&#125; EveningState 晚上类1234567891011121314151617public class EveningState extends State &#123; @Override public void writePrograme(Work work) &#123; if (work.isFinish()) &#123; //休息 work.setState(new RestState()); work.writeProgramme(); &#125; else &#123; if (work.getHour() &lt; 21) &#123; System.out.println(work.getHour() + &quot;加班&quot;); &#125; else &#123; work.setState(new SleepingState()); work.writeProgramme(); &#125; &#125; &#125;&#125; RestState 和 SleepingState1234567891011121314public class RestState extends State &#123; @Override public void writePrograme(Work work) &#123; System.out.println(work.getHour() + &quot;下班回家了&quot;); &#125;&#125;public class SleepingState extends State &#123; @Override public void writePrograme(Work work) &#123; System.out.println(work.getHour() + &quot;睡觉了&quot;); &#125;&#125; 测试类12345678910111213141516171819202122232425262728293031public class StateMain &#123; public static void main(String[] args) &#123; Work work = new Work(); work.setHour(9); work.writeProgramme(); work.setHour(11); work.writeProgramme(); work.setHour(12); work.writeProgramme(); work.setHour(13); work.writeProgramme(); work.setHour(14); work.writeProgramme(); work.setHour(17); work.writeProgramme(); work.setFinish(false); work.setHour(19); work.writeProgramme(); work.setHour(22); work.writeProgramme(); &#125;&#125; 程序输出123456789.0上午工作精神百倍11.0上午工作精神百倍12.0吃饭犯困午休13.0下午状态不错继续coding14.0下午状态不错继续coding17.0加班19.0加班22.0睡觉了","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式13-抽象工厂模式","slug":"2018-11-30-设计模式13-抽象工厂模式","date":"2019-06-26T03:40:39.181Z","updated":"2019-06-26T03:40:39.182Z","comments":true,"path":"2019/06/26/2018-11-30-设计模式13-抽象工厂模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-30-设计模式13-抽象工厂模式/","excerpt":"","text":"设计模式13-抽象工厂模式 代码实现IDepartment接口，用户客户端访问，解除与具体数据库访问的耦合12345public interface IDepartment &#123; void insert(String department); String getDepartment(int id);&#125; IUser接口，用户客户端访问，解除与具体数据库访问的耦合12345public interface IUser &#123; void insert(String user); String getUser(int id);&#125; IFactory接口，定义一个创建访问USer表对象和Department表对象。123456public interface IFactory &#123; IUser createUser(); IDepartment createDepart();&#125; AccessFactory 类实现IFactory接口，实例化AccessUser和AccessDept。1234567891011public class AccessFactory implements IFactory &#123; @Override public IUser createUser() &#123; return new AccessUser(); &#125; @Override public IDepartment createDepart() &#123; return new AccessDeparment(); &#125;&#125; AccessUser 用于access数据库访问user表123456789101112public class AccessUser implements IUser &#123; @Override public void insert(String user) &#123; System.out.println(&quot;access insert user&quot; + user); &#125; @Override public String getUser(int id) &#123; System.out.println(&quot;access getUser&quot; + id); return null; &#125;&#125; AccessDeparment类用户Access数据库访问depart表123456789101112public class AccessDeparment implements IDepartment &#123; @Override public void insert(String department) &#123; System.out.println(&quot;access department&quot; + department); &#125; @Override public String getDepartment(int id) &#123; System.out.println(&quot;access getDepartment&quot; + id); return null; &#125;&#125; SqlserverFactory 类实现IFactory接口，实例化SqlserverUser和SqlserverDepartment。 1234567891011public class SqlserverFactory implements IFactory &#123; @Override public IUser createUser() &#123; return new SqlserverUser(); &#125; @Override public IDepartment createDepart() &#123; return new SqlserverDepartment(); &#125;&#125; SqlserverDepartment、SqlserverUser 实现sql server数据库depart表和user表的访问。12345678910111213141516171819202122232425public class SqlserverDepartment implements IDepartment &#123; @Override public void insert(String department) &#123; System.out.println(&quot;sql server insert&quot; + department); &#125; @Override public String getDepartment(int id) &#123; System.out.println(&quot;sql server get department&quot; + id); return null; &#125;&#125;public class SqlserverUser implements IUser &#123; @Override public void insert(String user) &#123; System.out.println(&quot;sql server insert user&quot; + user); &#125; @Override public String getUser(int id) &#123; System.out.println(&quot;sqlserver getUser&quot; + id); return null; &#125;&#125; 测试类，可以看出测试类中完全是基于抽象实现，以后如果要切换数据库只需要修改工厂类new AccessFactory()或者new SqlserverFactory()一句就可以，具体方法调用不需要修改。 12345678910111213141516171819202122232425public class FactoryMain &#123; public static void main(String[] args) &#123; IFactory factory = new AccessFactory(); IUser iUser = factory.createUser(); iUser.insert(&quot;access user&quot;); iUser.getUser(1); IDepartment iDepart = factory.createDepart(); iDepart.insert(&quot;access depart&quot;); iDepart.getDepartment(1); // sqlserver factory = new SqlserverFactory(); iUser = factory.createUser(); iUser.insert(&quot;sqlserver user&quot;); iUser.getUser(1); iDepart = factory.createDepart(); iDepart.insert(&quot;sqlserver depart&quot;); iDepart.getDepartment(1); &#125;&#125; 抽象工厂模式概述抽象工厂模式，提供一个创建一系列相关或相互依赖对象的接口，而无需指定他们具体的类。AbstractProductA 和 AbstractProductB 是两个抽象产品，之所以是抽象因为它们可能有两种不同的实现，就像刚才说的User和Department ，而ProductA1、ProductA2 和 ProductB1、ProdctB2就是对两个抽象产品的具体分类实现。比如ProductA1可以理解为SqlserverUser，而ProductB1可以理解为AccessUser。AbstractFactory是一个抽象工厂接口，里面包含素有产品创建的抽象方法。通常是在运行时刻再创建一个ConcreteFactory类的实例，这个具体的工厂再创建具有特定实现的产品对象。为创建不同的产品对象，客户端使用不同的具体工厂。 抽象工厂的有点和缺点优点： 易于交换产品系列，具体工厂类在一个应用中只需要在初始化的时候出现过一次，这就使得改变一个应用的具体工厂变得非常容易，它只需要改变具体工厂即可使用不同的产品配置。 让具体的创建过程与客户端分离，客户端通过它们的抽象接口操纵实例，产品的具体类名也被具体工厂的实现分离，不会出现在客户代码中。 缺点：抽象工厂可以很方便的切换两个数据库的访问代码，但是如果需求来自增加功能，比如增加项目表Project，至少要增加三个类，IProject，SqlserverProject、AccessProject 还需要更改IFactory和SqlserverFactory和AaccessFactory 才可以完全实现。 使用简单工厂来改进抽象工厂工厂类123456789101112131415161718192021222324252627public class DataAccess &#123; private static final String db = &quot;sqlserver&quot;; public static IUser createUser() &#123; switch (db) &#123; case &quot;sqlserver&quot;: return new SqlserverUser(); case &quot;access&quot;: return new AccessUser(); default: return null; &#125; &#125; public static IDepartment createDepart() &#123; switch (db) &#123; case &quot;sqlserver&quot;: return new SqlserverDepartment(); case &quot;access&quot;: return new AccessDeparment(); default: return null; &#125; &#125;&#125; 测试类1234567public class DataAccessMain &#123; public static void main(String[] args) &#123; IUser sqlServerUser = DataAccess.createUser(); sqlServerUser.insert(&quot;sqlServer User&quot;); sqlServerUser.getUser(1); &#125;&#125; 代码结构图：","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式12-观察者模式","slug":"2018-11-30-设计模式12-观察者模式","date":"2019-06-26T03:40:39.179Z","updated":"2019-06-26T03:40:39.179Z","comments":true,"path":"2019/06/26/2018-11-30-设计模式12-观察者模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-30-设计模式12-观察者模式/","excerpt":"","text":"设计模式12-观察者模式观察者模式定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个主题对象在状态发生变化时，会通知所有观察者对象，使他们能够自动更新自己。 观察者模式结构图 代码实现Subject类为抽象通知者，一般由一个抽象类或者接口实现。把所有的的引用保存在一个聚集中，每个主题都可以有任何数量的观察者。抽象主题提供一个接口，可以增加或者删除观察者对象。12345678910111213141516171819public abstract class Subject &#123; private List&lt;Observer&gt; observerList = new ArrayList&lt;Observer&gt;(); //面向抽象编程 public void attach(Observer observer) &#123; this.observerList.add(observer); &#125; public void detach(Observer observer) &#123; this.observerList.remove(observer); &#125; public void notifyObservers() &#123; this.observerList.stream().forEach((obj) -&gt; &#123; obj.update(); &#125;); &#125;&#125; Observer类，抽象观察者，为所有的具体观察者定义一个接口，在得到主题的通知时更新自己。这个接口叫作更新接口。抽象观察者一般用一个抽象类或者一个接口实现。更新接口一般都包含一个update()方法，这个方法叫作更新方法。1234567891011121314151617181920212223242526272829public interface Observer &#123; void update();&#125;public class ConcreteObserver implements Observer &#123; private String name; private String observerState; private ConcreteSubject subject; public ConcreteObserver(String name, ConcreteSubject subject) &#123; this.name = name; this.subject = subject; &#125; @Override public void update() &#123; this.observerState = subject.getSubjectState(); System.out.println(&quot;观察者的新状态是&quot; + name + &quot;_&quot; + observerState); &#125; public ConcreteSubject getSubject() &#123; return subject; &#125; public void setSubject(ConcreteSubject subject) &#123; this.subject = subject; &#125;&#125; ConcreteSubject类，通常用一个具体子类实现。 具体通知者，在具体主题的内部状态改变时，给所有登记过的观察者发出通知。1234567891011public class ConcreteSubject extends Subject&#123; private String subjectState; public String getSubjectState() &#123; return subjectState; &#125; public void setSubjectState(String subjectState) &#123; this.subjectState = subjectState; &#125;&#125; 测试类12345678910111213141516public class ObserverMain &#123; public static void main(String[] args) &#123; ConcreteSubject concreteSubject = new ConcreteSubject(); concreteSubject.attach(new ConcreteObserver(&quot;No1&quot;, concreteSubject)); concreteSubject.attach(new ConcreteObserver(&quot;No2&quot;, concreteSubject)); concreteSubject.attach(new ConcreteObserver(&quot;No3&quot;, concreteSubject)); concreteSubject.setSubjectState(&quot;上车!&quot;); concreteSubject.notifyObservers(); &#125;&#125;输出观察者的新状态是No1_上车!观察者的新状态是No2_上车!观察者的新状态是No3_上车! 模式特点及应用场景观察者和通职者是你中有我，我中有你的状态。观察者模式动机：将一个系统分割成一系列相互协作的类有一个很不好的副作用，那就行需要维护相互对象之间的一致性。不希望为了维持一致性而使得各个类紧密耦合。 应用场景 当一个对象的改变需要同时改变其他对象时，而且它不知道具体有多少对象有待改变时，应该考虑使用观察者模式。 观察者模式所做的工作其实就是在解除耦合，让耦合的双方都依赖于抽象，而不是依赖于具体。从而使得各自的变化都不会影响另一边的变化。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"邻家的百万富翁","slug":"2018-11-28-邻家的百万富翁","date":"2019-06-26T03:40:39.177Z","updated":"2019-06-26T03:40:39.177Z","comments":true,"path":"2019/06/26/2018-11-28-邻家的百万富翁/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-28-邻家的百万富翁/","excerpt":"","text":"邻家的百万富翁这本书跟踪了美国百万富翁的日常生活，研究这些富人的财富是如何积累的。研究发现收入毫无疑问是财富积累的一个前提，但是如果无节制的花费，即使收入很高也会是个低级财富积累者。 预期净资产公式预期净资产=年龄除以10乘以年收入。个人实际净资产超过预期净资产2倍认为是高级别财富积累者。 财富积累者特质作者总结了高级财富积累者的特质： 多入少出 高效率安排时间精力金钱 金钱上充分自立比现实地位更重要 父母不会提供经济门诊照顾 成年孩子经济自给自足 精明抓住市场机会 选择了合适的职业 职业 富翁鼓励孩子从事自营专业人员：牙科医生、医生、律师 专业人员的生活福利大大超过其成本 单调乏味的普通企业出富人 喜欢自己的工作，天不亮就急冲冲奔向办公室 子女教育 给孩子越多现金，孩子成为财富积累者的可能性就越低 天降的财富更多用于消费而不是储蓄和投资 将节俭价值观传给孩子 不要过早告诉孩子家庭的富裕程度 鼓励孩子取得成就，挣钱去消费不应该成为一个人的终极目标 求学时间会导致财富积累时间延后（但是富人一般会支持孩子的教育，从而让他们成为自营专业人员） 投资 投资计划与财富积累之间存在高度正相关 使可税收入最小化 愿意购买旧车的人中，财富积累者比重更高","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"设计模式11-建造者模式","slug":"2018-11-22-设计模式11-建造者模式","date":"2019-06-26T03:40:39.174Z","updated":"2019-06-26T03:40:39.174Z","comments":true,"path":"2019/06/26/2018-11-22-设计模式11-建造者模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-22-设计模式11-建造者模式/","excerpt":"","text":"设计模式11-建造者模式建造者模式，将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。建造者模式通用构建图如下： 实现代码PersonBuilder抽象类12345678910111213public abstract class PersonBuilder &#123; public abstract void buildHead(); public abstract void buildBody(); public abstract void buildArmLeft(); public abstract void buildArmRight(); public abstract void buildLegLeft(); public abstract void buildLegRight();&#125; 实现类-瘦子建造者12345678910111213141516171819202122232425262728293031public class PersonThinBuilder extends PersonBuilder &#123; @Override public void buildHead() &#123; System.out.println(&quot;head thin&quot;); &#125; @Override public void buildBody() &#123; System.out.println(&quot;body thin&quot;); &#125; @Override public void buildArmLeft() &#123; System.out.println(&quot;arm left thin&quot;); &#125; @Override public void buildArmRight() &#123; System.out.println(&quot;arm right thin&quot;); &#125; @Override public void buildLegLeft() &#123; System.out.println(&quot;leg left thin&quot;); &#125; @Override public void buildLegRight() &#123; System.out.println(&quot;leg right thin&quot;); &#125;&#125; 实现类-胖子建造者12345678910111213141516171819202122232425262728293031public class PersonFatBuilder extends PersonBuilder &#123; @Override public void buildHead() &#123; System.out.println(&quot;head fat&quot;); &#125; @Override public void buildBody() &#123; System.out.println(&quot;body fat&quot;); &#125; @Override public void buildArmLeft() &#123; System.out.println(&quot;arm left fat&quot;); &#125; @Override public void buildArmRight() &#123; System.out.println(&quot;arm right fat&quot;); &#125; @Override public void buildLegLeft() &#123; System.out.println(&quot;leg left fat&quot;); &#125; @Override public void buildLegRight() &#123; System.out.println(&quot;leg right fat&quot;); &#125;&#125; 构建者-利用模板方法 ，PersonDirector类目的就是根据用户的选择一步一步构建小人，建造的过程在构建者完成，利用09节所讲的模板模式完成。12345678910111213141516public class PersonDirector &#123; private PersonBuilder pb; public PersonDirector(PersonBuilder pb) &#123; this.pb = pb; &#125; public void drawPerson() &#123; this.pb.buildHead(); this.pb.buildBody(); this.pb.buildArmLeft(); this.pb.buildArmRight(); this.pb.buildLegLeft(); this.pb.buildLegRight(); &#125;&#125; 使用场景主要用于构建一些复杂的对象，这些对象内部构建间的建造顺序通常是稳定的，但是对象内部的构建通常面临复杂的变化。建造代码与表示代码分离，由于建造者隐藏了产品如何组装，所以若需要改变产品的内部表示，只需要再定义一个具体的建造者即可。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式10-外观模式","slug":"2018-11-16-设计模式10-外观模式","date":"2019-06-26T03:40:39.172Z","updated":"2019-06-26T03:40:39.172Z","comments":true,"path":"2019/06/26/2018-11-16-设计模式10-外观模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-16-设计模式10-外观模式/","excerpt":"","text":"设计模式10-外观模式迪米特法则，也叫最少知识原则。如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类要调用另一个类的某一个方法的话，可以通过第三者转发这个调用。在类的结构设计上，每一个类都应当尽量降低成员的访问权限。迪米特法则根本思想是强调了类之间的松耦合。 外观模式，为子系统中的一组接口提供一个一致的界面。此模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 代码实例三个子系统代码1234567891011121314151617public class SubSystemOne &#123; public void methodOne()&#123; System.out.println(&quot;子系统方法1&quot;); &#125;&#125;public class SubSystemTwo &#123; public void methodTwo()&#123; System.out.println(&quot;子系统方法2&quot;); &#125;&#125;public class SubSystemThree &#123; public void methodThree()&#123; System.out.println(&quot;子系统方法3&quot;); &#125;&#125; 门面模式123456789101112131415161718192021222324public class Facade &#123; private SubSystemOne subSystemOne; private SubSystemTwo subSystemTwo; private SubSystemThree subSystemThree; public Facade() &#123; this.subSystemOne = new SubSystemOne(); this.subSystemTwo = new SubSystemTwo(); this.subSystemThree = new SubSystemThree(); &#125; public void methodA() &#123; this.subSystemOne.methodOne(); &#125; public void methodB() &#123; this.subSystemTwo.methodTwo(); &#125; public void methodC() &#123; this.subSystemThree.methodThree(); &#125;&#125; 测试模式12345678public class FacadeMain &#123; public static void main(String [] args)&#123; Facade facade = new Facade(); facade.methodA(); facade.methodB(); facade.methodC(); &#125;&#125; 业务场景 在设计初期阶段，应该有意识将不同的两个层分离，比如经典的三层架构，需要考虑在DAO和MODEL层、MODEL层和VIEW层之间建立外观facade，这样可以为复杂的子系统提供一个简单的接口，使得耦合大大降低。增加外观Facade可以提供一个简单的接口，减少它们之间的依赖。 维护一个遗留的大型系统时，可能这个系统已经非常难以扩展了，这时候可以为新系统开发一个外观Facade类，来提供设计粗糙或高度复杂的遗留代码的比较清晰的简单接口，让新系统与Facade对象交互，Facade与遗留代码交互所有复杂的工作。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式09-模板模式","slug":"2018-11-16-设计模式09-模板模式","date":"2019-06-26T03:40:39.170Z","updated":"2019-06-26T03:40:39.170Z","comments":true,"path":"2019/06/26/2018-11-16-设计模式09-模板模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-16-设计模式09-模板模式/","excerpt":"","text":"设计模式09-模板模式模板方法模式，定义一个操作中的算法的框架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 AbstractClass 是抽象类，其实也是一个抽象模板，定义并实现了一个模板方法。这个模板方法一般是一个具体方法，给出了一个顶级逻辑的骨架，而逻辑的组成步骤在相应的抽象操作中，推迟到子类实现。顶级逻辑也有可能调用一些具体的方法。 123456789101112public abstract class AbstractClass &#123; public abstract void primitiveOperation1(); public abstract void primitiveOperation2(); public void templateMethod() &#123; primitiveOperation1(); primitiveOperation2(); System.out.println(&quot;tempate method&quot;); &#125;&#125; ConcreteClass 实现父类所定义的一个或多个抽象方法，每一个AbstractClass都可以有任意多个ConcreteClass与之对应，而每一个ConcreteClass 都可以给出这些抽象方法的不同实现，从而使得顶级逻辑的实现各不相同。123456789101112131415161718192021222324public class ConcreteClassA extends AbstractClass&#123; @Override public void primitiveOperation1() &#123; System.out.println(&quot;template A oper 1&quot;); &#125; @Override public void primitiveOperation2() &#123; System.out.println(&quot;template A oper 2&quot;); &#125;&#125;public class ConcreteClassB extends AbstractClass &#123; @Override public void primitiveOperation1() &#123; System.out.println(&quot;template B oper 1&quot;); &#125; @Override public void primitiveOperation2() &#123; System.out.println(&quot;template B oper 2&quot;); &#125;&#125; 测试类12345678910public class TemplateMain &#123; public static void main(String[] args) &#123; ConcreteClassA concreteClassA = new ConcreteClassA(); concreteClassA.templateMethod(); ConcreteClassB concreteClassB = new ConcreteClassB(); concreteClassB.templateMethod(); &#125;&#125; 模板方法模式特点 模板方法模式是通过把不变的方法版移到超类，去除子类中的重复代码来体现它的优势。 模板方法提供了一个很好的代码复用平台，这个过程从高层次上看是相同的，但是有些步骤的实现可能不同，这时候应该考虑模板方法模式了。 为了防止子类改变模板方法中的算法，可把模板方法声明为final。 模板模式和策略模式都封装了算法，一个用组合（策略模式），一个用继承（模板模式）。 模板方法和策略方法区别 模板模式：控制算法内部 策略模式：不同算法的管理","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式08-原型模式","slug":"2018-11-16-设计模式08-原型模式","date":"2019-06-26T03:40:39.168Z","updated":"2019-06-26T03:40:39.168Z","comments":true,"path":"2019/06/26/2018-11-16-设计模式08-原型模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-16-设计模式08-原型模式/","excerpt":"","text":"设计模式08-原型模式原型模式，用原型实例指定创建对象的种类，并且通过copy这些原型创建新的对象。原型模式就是从一个对象再创建另一个可定制的对象，而且不需要知道任何创建的细节。原型类12345678910111213public abstract class Prototype &#123; private String id; public Prototype(String id) &#123; this.id = id; &#125; public String getId() &#123; return this.id; &#125; public abstract Prototype clone();&#125; 具体原型类12345678910public class ConcretePrototype extends Prototype &#123; public ConcretePrototype(String id) &#123; super(id); &#125; @Override public Prototype clone() &#123; return this; &#125;&#125; 测试类12345678public class PrototypeMain &#123; public static void main(String[] args) &#123; ConcretePrototype p1 = new ConcretePrototype(&quot;prototype&quot;); ConcretePrototype c1 = (ConcretePrototype) p1.clone(); System.out.println(p1.getId()); System.out.println(c1.getId()); &#125;&#125; 一般在初始化的信息不发生变化的情况下，克隆是最好的办法，既隐藏了对象创建的细节，又是对性能大大的提高。 等于不用重新初始化对象， 而是动态地获得对象运行时的状态。 模式优缺点优点 如果创建新的对象比较复杂时，可以利用原型模式简化对象的创建过程，同时提高效率。 可以使用深克隆保持对象的状态。 原型模式提供了简化的创建结构。 缺点 实现深克隆有时候需要比较复杂的代码。 需要为每一个类配备一个克隆方法，而且这个克隆方法需要对类的功能进行通盘考虑，这对全新的类来说不是很难，但对已有的类进行改造时，不容易，必须修改其源代码，违背了开闭原则。 使用场景 创建新对象的成本较大，可以利用已有的对象进行复制来获得。 系统要保存对象的状态，而对象的状态变化很小，或者对象本身占内存不大的时候，可以使用原型模式配合备忘录模式来应用。相反，如果对象的状态变化很大，或者对象占用的内存很大，那么采用状态模式会比原型模式更好。 避免使用分层次的工厂类创建分层次对象，并且类的实例对象只有一个或很少的几个组合状态，通过赋值原型对象得到新实例可能比使用构造函数创建一个实例更方便。 模式总结 原型模式向客户隐藏了创建对象的复杂性，客户只需要知道要创建的对象的类型，然后通过请求就可以获得和该对象一模一样的新对象，无需知道具体的创建过程。 克隆分为浅克隆和深克隆。 原型模式的对象复制可能会相当复杂，比如深克隆。 参考设计模式读书笔记-原型模式","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式07-代理模式","slug":"2018-11-16-设计模式07-代理模式","date":"2019-06-26T03:40:39.166Z","updated":"2019-06-26T03:40:39.166Z","comments":true,"path":"2019/06/26/2018-11-16-设计模式07-代理模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-16-设计模式07-代理模式/","excerpt":"","text":"设计模式07-代理模式代理模式，为其他对象提供一种代理以控制对这个对象的访问。代理模式的元素：共同接口、代理对象、目标对象。代理模式的行为：由代理对象执行目标对象的方法、由代理对象扩展目标对象的方法。代理模式的宏观特性：对客户端只暴露出接口，不暴露它的架构。Subject类，定义了RealSubject和Proxy的共用接口，这样就可以在任何使用RealSubject的地方都可以使用Proxy。123public interface Subject &#123; public void request();&#125; RealSubjecy类，定义了Proxy所代表的真实实体。123456public class RealSubject implements Subject &#123; @Override public void request() &#123; System.out.println(&quot;真实的请求&quot;); &#125;&#125; Proxy类，保存一个引用使得代理可以访问实体，并提供一个与Subject的接口相同的接口，这样代理可以完全用来替代实体。12345678910public class Proxy implements Subject&#123; private Subject realSubject; @Override public void request() &#123; if(this.realSubject ==null)&#123; this.realSubject = new RealSubject(); &#125; this.realSubject.request(); &#125;&#125; 测试类123456public class ProxyMain &#123; public static void main(String [] args)&#123; Proxy proxy = new Proxy(); proxy.request(); &#125;&#125; 代理模式应用 远程代理，为一个对象在不同的地址空间提供局部代表，这样可以隐藏一个对象存在于不同地址空间的事实。 虚拟代理根据需要创建开销很大的对象，通过它来存放实例化需要很长时间的真实对象。打开html页面时，图片一张一张下载，未打开的图片框，就是通过虚拟代理来代替了真实的图片，此时代理存储了真实图片的路径和尺寸。 安全代理，用来控制真实对象访问时的权限。 智能指引，当调用真实对象时，代理处理另外的一些事情。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式06-依赖倒转模式","slug":"2018-11-16-设计模式06-依赖倒转模式","date":"2019-06-26T03:40:39.163Z","updated":"2019-06-26T03:40:39.163Z","comments":true,"path":"2019/06/26/2018-11-16-设计模式06-依赖倒转模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-16-设计模式06-依赖倒转模式/","excerpt":"","text":"设计模式06-依赖倒转模式依赖倒转原则依赖倒转原则，抽象不应该依赖细节，细节应用依赖于抽象。说白了，要针对接口编程，不要对实现编程。 高层模块不应该依赖低层模块，两个都应该依赖抽象。 高层模块依赖低层模块实例，访问数据库的代码。如果客户需要更换数据库，高层代码需要重写。而依赖抽象的接口或者抽象类，就不怕更改。不怕更改的原因，里氏代换原则。 里氏代换原则，子类型必须能够替换掉他们的父类型。只有当子类可以替换掉父类，软件单位的功能不受影响时，父类才能被真正复用，而子类也能在父类的基础上增加新的行为。由于子类型的可替换性才使得使用父类类型的模块在无需修改的情况下就可以扩展。 依赖倒转可以说是面向对象设计的标志，用哪种语言来编写程序不重要，如果编写时考虑的都是如何针对抽象编程而不是针对细节编程，即程序中所有的依赖关系都是终止于抽象类或者接口，那就是面向对象的设计，反之就是过程化的设计。 依赖倒转的例子汽车驾驶的例子，第一次设计驾驶员驾驶奔驰汽车的时候，因为场景单一，很有可能陷入到面向实现编程的陷阱。实现代码：12345678910111213141516171819202122232425262728public class Benz &#123; public String run() &#123; return &quot;奔驰汽车可以跑&quot;; &#125;&#125;public class Driver &#123; private String name; public Driver(String name) &#123; this.name = name; &#125; public void driver(Benz benz) &#123; System.out.println(this.name + benz.run()); &#125;&#125;public class DipMain &#123; public static void main(String[] args) &#123; Driver oldDriver = new Driver(&quot;dm&quot;); oldDriver.driver(new Benz()); &#125;&#125;输出：dm奔驰汽车可以跑 上述代码如果需要让司机驾驶宝马，却不能让司机Driver类开起来。 因为司机类和汽车类是紧耦合的关系，导致系统的可维护性大大降低。必须修改实际类的public void driver(Benz benz)方法，现在根据DIP原则进行修改。 实现代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface IDriver &#123; public void driverCar(ICar iCar);&#125;public interface ICar &#123; public String run();&#125;public class Benz implements ICar&#123; @Override public String run() &#123; return &quot;奔驰汽车running&quot;; &#125;&#125;public class BMW implements ICar &#123; @Override public String run() &#123; return &quot;BMW RUNNING&quot;; &#125;&#125;public class Driver implements IDriver&#123; private String name; public Driver(String name) &#123; this.name = name; &#125; @Override public void driverCar(ICar iCar) &#123; System.out.println(this.name + iCar.run()); &#125;&#125;public class DriverMain &#123; public static void main(String [] args)&#123; Driver driver = new Driver(&quot;dm good&quot;); driver.driverCar(new BMW()); driver.driverCar(new Benz()); &#125;&#125; 参考设计模式六大原则例子（四）– 依赖倒置原则（DIP）例子","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式05-策略模式","slug":"2018-11-12-设计模式05-策略模式","date":"2019-06-26T03:40:39.161Z","updated":"2019-06-26T03:40:39.161Z","comments":true,"path":"2019/06/26/2018-11-12-设计模式05-策略模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-12-设计模式05-策略模式/","excerpt":"","text":"设计模式05-策略模式策略模式：定义了算法家族，分别封装起来，让它们之间可以互相替换。此模式算法的变化，不会影响到使用算法的客户。 样例代码策略类1234567891011121314151617181920212223242526public abstract class Strategy &#123; public abstract void algorithmInterface();&#125;public class ConcreteStrategyA extends Strategy &#123; @Override public void algorithmInterface() &#123; System.out.println(&quot;ConcreteStrategyA&quot;); &#125;&#125;public class ConcreteStrategyB extends Strategy &#123; @Override public void algorithmInterface() &#123; System.out.println(&quot;ConcreteStrategyB&quot;); &#125;&#125;public class ConcreteStrategyC extends Strategy &#123; @Override public void algorithmInterface() &#123; System.out.println(&quot;ConcreteStrategyC&quot;); &#125;&#125; 上下文类1234567891011121314public class Context &#123; private Strategy strategy; public Context(Strategy strategy) &#123; this.strategy = strategy; &#125; /** * 根据具体的策略对象，调用其算法的方法 */ public void ContextInterface()&#123; strategy.algorithmInterface(); &#125;&#125; 测试类12345678910111213public class StrategyMain &#123; public static void main(String[] args) &#123; Context context = new Context(new ConcreteStrategyA()); context.ContextInterface(); context = new Context(new ConcreteStrategyB()); context.ContextInterface(); context = new Context(new ConcreteStrategyC()); context.ContextInterface(); &#125;&#125; 策略模式解析策略模式是一种定义一系列算法的方法，从概念上来看，所有这些算法完成的都是相同的工作，只是实现不同，它可以以相同的方式调用所有的算法，减少了各种算法类与使用算法类之间的耦合。 优点 策略模式的Strategy类层次为Context定义了一系列的可供重用的算法或者行为。集成有助于析取出这些算法中的公共功能。 简化了单元测试，每个算法都有自己的类，可以通过自己的接口单独测试。 实践中可以用它来封装几乎任何类型的规则，只要在分析过程中听到需要在不同时间应用不同的业务规则，就可以考虑使用策略模式处理这种变换的可能性。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式04-装饰模式","slug":"2018-11-08-设计模式04-装饰模式","date":"2019-06-26T03:40:39.158Z","updated":"2019-06-26T03:40:39.159Z","comments":true,"path":"2019/06/26/2018-11-08-设计模式04-装饰模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-08-设计模式04-装饰模式/","excerpt":"","text":"设计模式04-装饰模式装饰模式，动态地给一个对象添加一些额外的职责，就增加功能来说，装饰模式比生成子类更为灵活。 Component定义了一个对象接口，可以给这些对象动态地添加职责。 ConcreteComponent 定义了一个具体的对象，也可以给这个对象添加一些职责。 Decorate 装饰抽象类，继承了Component，从外类来扩展Component类的功能，但是对于Component来说，是无需知道Decorate的存在的。 ConcreteDecorate 具体的装饰对象，起到给Component添加职责的功能。 实例代码123456789101112131415package cn.dm.decorate;public abstract class Component &#123; public abstract void operation();&#125;package cn.dm.decorate;public class ConcreteComponent extends Component &#123; @Override public void operation() &#123; System.out.println(&quot;具体对象的操作&quot;); &#125;&#125; 装饰类：1234567891011121314151617181920package cn.dm.decorate;/** * 装饰类 */public class Decorate extends Component &#123; protected Component component; public void setComponent(Component component) &#123; this.component = component; &#125; @Override public void operation() &#123; if (this.component != null) &#123; this.component.operation(); &#125; &#125;&#125; 具体装饰类A123456789101112package cn.dm.decorate;public class ConcreteDecorateA extends Decorate&#123; private String addedState; @Override public void operation() &#123; super.operation(); this.addedState=&quot;New State&quot;; System.out.println(&quot;A的操作&quot;+addedState); &#125;&#125; 具体装饰类B1234567891011121314package cn.dm.decorate;public class ConcreteDecorateB extends Decorate&#123; @Override public void operation() &#123; super.operation(); this.addBehave(); &#125; private void addBehave()&#123; System.out.println(&quot;B add new behave&quot;); &#125;&#125; 调用程序123456789101112package cn.dm.decorate;public class DecorateMain &#123; public static void main(String [] args)&#123; ConcreteComponent c = new ConcreteComponent(); ConcreteDecorateA d1 = new ConcreteDecorateA(); ConcreteDecorateB b1 = new ConcreteDecorateB(); d1.setComponent(c); b1.setComponent(d1); b1.operation(); &#125;&#125; 先执行装饰器A还是先执行装饰器B都可以。 装饰模式利用Decorate的setComponent来对对象进行包装。这样每个装饰对象的实现就与如何使用这个对象分离开了。每个装饰对象只需要关心自己的功能，不需要关心如何被添加到对象链中。 服饰实例Component类和装饰类123456789101112131415161718public class Person &#123; protected void wear()&#123; System.out.println(&quot;我是装饰模式&quot;); &#125;;&#125;public class WearDecorate extends Person &#123; protected Person person; public void setPerson(Person person) &#123; this.person = person; &#125; @Override protected void wear() &#123; this.person.wear(); &#125;&#125; 具体装饰类1234567891011121314151617181920212223public class BigTrouser extends WearDecorate&#123; @Override protected void wear() &#123; this.person.wear(); System.out.println(&quot;大裤子&quot;); &#125;&#125;public class Tie extends WearDecorate&#123; @Override protected void wear() &#123; this.person.wear(); System.out.println(&quot;领带&quot;); &#125;&#125;public class TShirt extends WearDecorate&#123; @Override protected void wear() &#123; this.person.wear(); System.out.println(&quot;Tshirt&quot;); &#125;&#125; 客户端代码12345678910111213public class WearMain &#123; public static void main(String[] args) &#123; Person person = new Person(); Tie tie = new Tie(); TShirt tShirt = new TShirt(); BigTrouser big = new BigTrouser(); tie.setPerson(person); tShirt.setPerson(tie); big.setPerson(tShirt); big.wear(); &#125;&#125; 结果显示1234我是装饰模式领带Tshirt大裤子 总结装饰模式是为已有功能动态地添加更多功能的一种方式。当系统需要新功能的时候，是向旧的类中添加新的代码。而新加的这些代码通常装饰了原有类的核心职责或主要行为。在主类中增加了新的字段、新的方法和新的逻辑，从而增加了主类的复杂度，而新加的这些代码仅仅是为了满足一些只在特定情况下才会执行的特殊行为的需要。装饰模式提供了一个非常好的解决方案，把要装饰的功能放在单独的类中，并让这个类包装它所要装饰的对象。因此，当需要执行特殊行为的时候，客户代码就可以在运行时根据需要有选择地、按顺序使用装饰功能包装对象。 优点 有效地把类的核心职责与装饰功能区分开，而且可以去除相关类中重复的装饰逻辑。 把类的装饰功能从类中搬移出去，这样可以简化原有的类。 理想情况是保证装饰类之间彼此独立，这样就可以任意的顺序进行组合。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"微服务架构学习27-微服务混合云部署实践","slug":"2018-11-08-微服务架构学习27-微服务混合云部署","date":"2019-06-26T03:40:39.156Z","updated":"2019-06-26T03:40:39.156Z","comments":true,"path":"2019/06/26/2018-11-08-微服务架构学习27-微服务混合云部署/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-08-微服务架构学习27-微服务混合云部署/","excerpt":"","text":"微服务架构学习27-微服务混合云部署实践混合云部署，既在企业内部的私有云部署服务，又使用企业外部公有云部署服务的模式。面临的问题： 跨云服务实现负债均衡 跨云服务实现数据同步 跨云服务实现容器运维 跨云服务的负载均衡私有云机房部署了VIP和Nginx分别作为四层和七层的负债均衡，阿里云机房部署了SLB和Nginx分别用作四层和七层负载均衡。 跨云服务的数据同步公有云部署和内部私有云部署服务一些不同点： 私有云与公有云之间的网络隔离，为了实现互通需要假设VPN网络或者专线。 数据库能否上云，企业的核心业务数据，往往会出于安全隐私考虑，并不敢直接放到云上部署。阿里云的机房主要用于承担下行的读请求，部署的缓存也不是跟内网机房完全一致，而是只部署了最核心的服务所依赖的缓存，这样可以把大部分阿里云机房的请求都在内部消化，减少到内网数据库的穿透，从而节省跨云专线的带宽使用。 跨云服务的容器运维要求有一套统一的容器运维平台能同时对接内部私有云和外部公有云。 跨云的主机管理跨云主机管理的关键点在于，如何对内部私有云的机器和公有云的ECS进行管理，在DCP里按照“主机-服务池-集群”的模式进行管理。 主机：某一台具体的服务器，可能是私有云创建的虚拟机，也有可能是公有云创建的ECS。 服务池：针对某个具体的服务，由这个服务部署的主机组成，可能同时包含私有云和公有云，规模可能是几台或者上百台。 集群，针对某个具体的业务线而言，可能包含有多个服务池。 跨云服务发现DCP的服务发现主要有两种方式：一种是针对HTTP服务采用的nginx-upsync-module，一种是RPC服务的Config Service。除此之外，阿里云上部署的服务还可以直接使用SLB（负载均衡）来做服务发现。 跨云弹性扩容流量上涨，超出内部私有云机房部署所能承受的范围时，需要扩容阿里云机房的机器，然后把流量切换到阿里云机房。有两种方案，一是在DNS层切换，把原先解析到私有云机房VIP的流量，解析到阿里云机房的SLB，这时候阿里云机房部署的SLB、Nginx、JAVA WEB 都需要扩容。另一种是在Nginx 层切换，把原先转发到私有云机房的Nginx流量，转发到阿里云机房的JAVA WEB，这时候只需要扩容阿里云的JAVA web。 跨云服务编排在进行服务编排时，如果服务跨云部署，要考虑跨机房访问的问题。下图FEED服务依赖user 和 card服务，如果feed扩容的话，需要先扩容user和card。 总结微服务混合云部署必须解决三个问题：跨云服务的负载均衡、跨云服务的数据同步、跨云服务的容器运维。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习26-多机房部署实践","slug":"2018-11-08-微服务架构学习26-多机房部署实践","date":"2019-06-26T03:40:39.154Z","updated":"2019-06-26T03:40:39.154Z","comments":true,"path":"2019/06/26/2018-11-08-微服务架构学习26-多机房部署实践/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-08-微服务架构学习26-多机房部署实践/","excerpt":"","text":"微服务架构学习26-多机房部署实践多机房部署微服务的关键问题： 一切正常时用户请求访问哪个机房 多个机房之间数据如何同步 多个机房之间的数据如何确保一致性 多机房负载均衡根据用户访问的IP，通过DNS解析到不同的机房，北方用户访问联通机房，南方用户访问电信机房。为了实现负载均衡，会在每个机房分别部署四层负载均衡器VIP以及七层负载均衡器Nginx。比如来自北方用户的请求通过DNS解析到联通机房下任意一个VIP，然后通过VIP把请求转发给联通机房下任意一个Nginx，Nginx再把请求转发给联通机房下任意一个Tomcat容器。 但是在实际部署时，并不能完全遵循就近访问的原则，需要根据需要调配流量，示意图如下： 多机房数据同步实现服务部署到多机房供用户访问的前提是：每个机房的数据是一样的。这就要求多个机房之间的数据必须保持同步，一般对于高并发访问的服务来说，数据通常会有两层存储缓存层和数据库层。数据一致既要保证数据库层的数据一致又要保证缓存层的数据一致。 主从机房架构主从机房架构以一个机房为主机房，所有的写请求都只发给主机房的处理机，由主机房的处理机来更新本机房的缓存和数据库。其他机房的缓存也通过主机房的处理机来更新，而数据库则通过MySQL的binlog同步机制来实现数据同步。采用这样的架构，风险点在主机房。 独立机房架构同步方案见下图，联通电信机房都有写请求，并通过一个叫作WMB的消息同步组件把各自机房的写请求同步一份给对方机房。每个机房的处理机接收到写请求后更新各自机房的缓存，但是只会有一个机房更新数据库，其他机房的数据库通过MySql的binlog同步机制实现数据同步。 独立机房的核实在于WMB消息同步组件，实现原理图如下：WMB分成两个部分： reship，负责把本机房的写请求分发一份给别的机房。 collector，负责从别的机房读取写请求，然后再把请求转发给本机房的处理机。WMB 如何实现消息同步功能，方案有两种，一种是通过MCQ消息队列，一种是通过RPC调用。 MCQ 消息队列实现 RPC调用实现 多机房数据一致性多机房数据同步过程中，会因为各种原因导致各机房之间数据不一致。微博的服务主要通过消息对账机制来保证最终一致性。 消息对账机制见下面。在整个环节中，requestID会一直保持向下传递，无论处理成功或者失败都会记录一条包含requestId和机房标记的处理日志，并写到Elasticsearch集群上去。然后通过一个定时线程，每间隔1分钟扫描Elasticsearch集群上的日志，找到包含同一个requestID的不同机房的处理日志，然后验证是否在各个机房都请求处理成功，如果有失败，则可以根据日志信息重试该阶段直成功（重试前提是满足服务的幂等），从而保证数据的最终一致性。 数据库与缓存先后顺序：一般是先写数据库，再写缓存。 总结主要讲解了微服务多机房部署时要面临的三个问题，一是如何保证负债均衡，二是多机房之间数据如何保证同步，三是多机房之间数据如何保证一致性。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"设计模式03-组合模式","slug":"2018-11-05-设计模式03-组合模式","date":"2019-06-26T03:40:39.151Z","updated":"2019-06-26T03:40:39.151Z","comments":true,"path":"2019/06/26/2018-11-05-设计模式03-组合模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-05-设计模式03-组合模式/","excerpt":"","text":"设计模式03-组合模式（Composite Pattern）组合模式(Composite Pattern)，又叫做部分整体模式，是用于把一组相似的对象当做一个单一的对象。组合模式依据树形结构来组合对象，用来表示部分以及整体层次。这种类型的设计模式属于结构型模式，它创建了对象组的树形结构。大话设计模式中的定义是，组合模式：将对象组合成树形结构以表示“部分-整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。 实例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package cn.dm.composite;import java.util.ArrayList;import java.util.List;public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; public Employee(String name, String dept, int salary, List&lt;Employee&gt; subordinates) &#123; super(); this.name = name; this.dept = dept; this.salary = salary; this.subordinates = subordinates; &#125; public Employee(String name, String dept, int salary) &#123; super(); this.name = name; this.dept = dept; this.salary = salary; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates() &#123; return subordinates; &#125; public String toString() &#123; return &quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary + &quot; ]&quot;; &#125;&#125;package cn.dm.composite;public class Main &#123; public static void main(String[] args)&#123; Employee CEO = new Employee(&quot;John&quot;,&quot;CEO&quot;, 30000); Employee headSales = new Employee(&quot;Robert&quot;,&quot;Head Sales&quot;, 20000); Employee headMarketing = new Employee(&quot;Michel&quot;,&quot;Head Marketing&quot;, 20000); Employee clerk1 = new Employee(&quot;Laura&quot;,&quot;Marketing&quot;, 10000); Employee clerk2 = new Employee(&quot;Bob&quot;,&quot;Marketing&quot;, 10000); Employee salesExecutive1 = new Employee(&quot;Richard&quot;,&quot;Sales&quot;, 10000); Employee salesExecutive2 = new Employee(&quot;Rob&quot;,&quot;Sales&quot;, 10000); CEO.add(headSales); CEO.add(headMarketing); headSales.add(salesExecutive1); headSales.add(salesExecutive2); headMarketing.add(clerk1); headMarketing.add(clerk2); //打印该组织的所有员工 System.out.println(&quot;-------------------公司员工情况----------------------&quot;); System.out.println(CEO); for (Employee headEmployee : CEO.getSubordinates()) &#123; //打印CEO的直属一级部下 System.out.println(headEmployee); for (Employee employee : headEmployee.getSubordinates()) &#123; //打印CEO的二级部下 System.out.println(employee); &#125; &#125; &#125; &#125; 使用场景需求中体现部分与整体层次结构的时候，以及当我们希望用户可以忽略组合对象与单个对象的不同，统一的使用组合结构中的所有对象时。 在树型结构的问题中，模糊了简单元素和复杂元素的概念、客户程序可以向处理简单元素一样来处理复杂元素，从而使得程序与复杂元素的内部结构结构。 树枝和叶子实现统一接口，树枝内部组合该接口。 实践项目考虑做一套办公管理系统，并且总公司的人力资源部、财务部等的办公挂历功能在所有的分公司也都要有。 抽象公司类：123456789101112131415161718package cn.dm.composite.company;/** * 抽象公司类 */public abstract class Company &#123; protected String name; public Company(String name)&#123; this.name = name; &#125; public abstract void add(Company company); public abstract void remove(Company company); public abstract void display(int depth); public abstract void lineofDuty();&#125; 具体公司类（树枝节点）12345678910111213141516171819202122232425262728293031323334353637383940package cn.dm.composite.company;import java.util.ArrayList;import java.util.List;/** * 具体公司类树枝节点 */public class ConcreteCompany extends Company &#123; public ConcreteCompany(String name) &#123; super(name); &#125; private List&lt;Company&gt; childrenCompany = new ArrayList&lt;Company&gt;(); @Override public void add(Company company) &#123; childrenCompany.add(company); &#125; @Override public void remove(Company company) &#123; childrenCompany.remove(company); &#125; @Override public void display(int depth) &#123; System.out.println(&quot;第 &quot; + depth + &quot; 层的机构名为： &quot; + name); childrenCompany.stream().forEach((com) -&gt; &#123; com.display(depth + 1); &#125;); &#125; @Override public void lineofDuty() &#123; childrenCompany.stream().forEach((com) -&gt; &#123; com.lineofDuty(); &#125;); &#125;&#125; 叶子节点123456789101112131415161718192021222324252627282930package cn.dm.composite.company;/** * 叶子节点财务部 */public class FinanceDept extends Company &#123; @Override public void add(Company company) &#123; &#125; @Override public void remove(Company company) &#123; &#125; @Override public void display(int depth) &#123; System.out.println(&quot;第 &quot; + depth + &quot; 层的机构名为： &quot; + name); &#125; @Override public void lineofDuty() &#123; System.out.println(name + &quot; 负责公司财务收支管理&quot;); &#125; public FinanceDept(String name) &#123; super(name); &#125;&#125; 12345678910111213141516171819202122232425262728293031package cn.dm.composite.company;/** * 叶子节点-人力部门 */public class HrDept extends Company &#123; public HrDept(String name) &#123; super(name); &#125; @Override public void add(Company company) &#123; &#125; @Override public void remove(Company company) &#123; &#125; @Override public void display(int depth) &#123; System.out.println(&quot;第 &quot; + depth + &quot; 层的机构名为： &quot; + name); &#125; @Override public void lineofDuty() &#123; System.out.println(name + &quot; 负责员工招聘管理培训&quot;); &#125;&#125; 测试代码1234567891011121314151617181920212223242526272829303132package cn.dm.composite.company;public class Main &#123; public static void main(String[] args) &#123; //一个总公司 ConcreteCompany root = new ConcreteCompany(&quot;北京总公司&quot;); root.add(new HrDept(&quot;总公司人力资源部&quot;)); root.add(new FinanceDept(&quot;总公司财务部&quot;)); //三个子公司 ConcreteCompany com1 = new ConcreteCompany(&quot;广州分公司&quot;); com1.add(new HrDept(&quot;广州分公司人力资源部&quot;)); com1.add(new FinanceDept(&quot;广州分公司财务部&quot;)); root.add(com1); ConcreteCompany com2 = new ConcreteCompany(&quot;杭州分公司&quot;); com2.add(new HrDept(&quot;杭州分公司人力资源部&quot;)); com2.add(new FinanceDept(&quot;杭州分公司财务部&quot;)); root.add(com2); ConcreteCompany com3 = new ConcreteCompany(&quot;深圳分公司&quot;); com3.add(new HrDept(&quot;深圳分公司人力资源部&quot;)); com3.add(new FinanceDept(&quot;深圳分公司财务部&quot;)); root.add(com3); System.out.println(&quot;-------公司结构图--------&quot;); root.display(1); System.out.println(&quot;----------各部门职责----------&quot;); root.lineofDuty(); &#125;&#125; 运行结果12345678910111213141516171819202122-------公司结构图--------第 1 层的机构名为： 北京总公司第 2 层的机构名为： 总公司人力资源部第 2 层的机构名为： 总公司财务部第 2 层的机构名为： 广州分公司第 3 层的机构名为： 广州分公司人力资源部第 3 层的机构名为： 广州分公司财务部第 2 层的机构名为： 杭州分公司第 3 层的机构名为： 杭州分公司人力资源部第 3 层的机构名为： 杭州分公司财务部第 2 层的机构名为： 深圳分公司第 3 层的机构名为： 深圳分公司人力资源部第 3 层的机构名为： 深圳分公司财务部----------各部门职责----------总公司人力资源部 负责员工招聘管理培训总公司财务部 负责公司财务收支管理广州分公司人力资源部 负责员工招聘管理培训广州分公司财务部 负责公司财务收支管理杭州分公司人力资源部 负责员工招聘管理培训杭州分公司财务部 负责公司财务收支管理深圳分公司人力资源部 负责员工招聘管理培训深圳分公司财务部 负责公司财务收支管理 参考组合模式","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"微服务架构学习25-微服务容量规划","slug":"2018-11-05-微服务架构学习25-微服务容量规划","date":"2019-06-26T03:40:39.148Z","updated":"2019-06-26T03:40:39.148Z","comments":true,"path":"2019/06/26/2018-11-05-微服务架构学习25-微服务容量规划/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-11-05-微服务架构学习25-微服务容量规划/","excerpt":"","text":"微服务架构学习25-微服务容量规划在单体应用时，只需要针对这个单体应用的访问量和实际接口性能来决定要不要给单体应用扩容，拆分为众多的微服务之后，需要考虑每个服务的容量规划，复杂度主要来自下面几个方面： 服务数量众多，人肉运维难以管理，微博FEED业务仅仅RPC服务就有将近40个。 服务接口表现差异巨大，有轻接口（访问量大但响应时间快）和重接口之分。 服务部署的集群规模大小不同，需要扩容的机器数量差异很大。Feed服务上千台机器。 服务之间还存在依赖关系，在服务扩容的时候，还需要考虑依赖服务的容量是否足够。 容量规划系统容量规划系统的作用是根据各个微服务部署集群的最大容量和线上实际运行的负荷，来决定各个微服务是否需要弹性扩缩容，以及需要扩缩容多少台机器。 容量评估一般集群的容量评估都是通过线上实际压测来确定的，线上压测的关键点如下： 选择合适的压测指标 一类是系统类指标，比如机器的CPU使用率、内存占用量、磁盘IO使用率以及网卡带宽。 一类是服务类指标，比如接口响应的平均耗时，P999耗时、错误率。（平均耗时不是个好的观察量，平均耗时正常的情况下，可能出现慢请求） 接口的慢速比，也就是接口响应时间高于某个阈值的比例。比如微博Feed接口压测时，选择的压测指标是Feed接口响应时间大于1S的比例，压测的终止条件是Feed接口响应时间大于1S的比例超过1%。对于大多数在线服务，接口慢速比不超过1%是服务质量保证的底线，可以作为一个通用的压测指标。 压测获取单机的最大容量集群的最大容量就是单机的最大容量*集群的机器数量。获得集群的最大容量，就必须获得单机的最大容量。通常有两种方式获取单机的最大容量，一种是单机压测，一种是集群压测。 单机压测两种方式，一种是通过日志回放手段，模拟线上流量对单机进行压测。 一种是通过TCP-Copy的方式，把线上的流量拷贝过来对单机进行压测。 集群压测是对整个集群进行压测，以获取单机的最大容量。一般做法是通过不断把线上集群的节点摘除，以减少机器数的方式，来增加线上节点单机的流量。 采用集群压测方式更合理一些，因为它完全使用线上真实流量进行压测，获取单机的最大容量数值更精确。不过使用集群压测，压测时候可能会对线上用户的实际需求产生影响。 采用集群压测，不断缩减线上节点数量，并观察服务的慢速比指标，当慢速比达到1%时，就停止压测，这个时候就可以计算单机的最大容量了，一般做法是用压测停止时的单机平均QPS作为单机的最大容量。 由上图可知，在单机QPS都是100的情况下，左边单机还能继续加大QPS，右边的单机已经出现超过500ms以上的慢请求了。一个合理的计算单机容量的方式是采用区间加权来计算，响应时间越长权重越高。比如0-10ms区间权重1，10-50ms权重2，50-100ms权重4,500ms以上权重32。 实时获取集群的运行负荷统计每台单机在不同耗时区间内的请求数，推送到集中处理的地方进行聚合，将同一个集群内的单机位于不同耗时区间内的请求数进行汇总，就得到整个集群的请求在不同耗时区间的分布了，再利用区间加权的方式就可以计算出整个集群的运行负荷。 如上图水位线所示，当水位线位于致命线下时，就需要立即扩容。在扩容一定数量的机器后，水位线回到安全线以上并保持一段时间后，就可以进行缩容了。在进行扩缩容时，机器的数量决定： 扩容在决定扩多少机器时，一般有两种方式，一种按数量，一种按比例。因为不同集群内的机器数量差别很大，所以一般采取按比例的方式，比如一次性都增加30%的机器数量。 缩容在扩容完成后，集群的水位线保持在安全线以上一段时间以后，需要进行缩容。可以根据实际业务特点来决定多久可以缩容，比如微博的业务一般突发流量在1小时以内，因此集群的水位线在安全线上超过1个小时后，就可以缩容。缩容的时候，不是一次把所有扩容的机器都缩掉，而是采用逐步缩容的方式。比如，每5分钟判断一次集群的水位线是否还在致命线上，然后按照10%、30%、50%、100%的比例进行缩容。 在实际根据水位线进行扩缩容时，还需要防止网络抖动等原因造成的水位线瞬间抖动，这个时候集群运行负荷会突然变大，导致水位线异常。为了防止瞬间抖动，可以每分钟采集一次系统的水位线，一共采集5个点，只有5个点有3个点满足扩容条件，才真正扩容。 总结 容量评估，通过压测获取集群的最大容量，并实时采集服务调用的数据以获取集群的实时运行负荷，这样就可以获取集群的实时水位线。 调度决策，通过水位线与致命线和安全线对比来决定什么时候扩缩容。扩容的机器数一般按照集群的机器数量的比例来决定。缩容一般采取逐步缩容的方式以避免缩容太快。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"设计模式01-桥接模式(Bridge Pattern)","slug":"2018-10-31-设计模式02-桥接模式","date":"2019-06-26T03:40:39.145Z","updated":"2019-06-26T03:40:39.145Z","comments":true,"path":"2019/06/26/2018-10-31-设计模式02-桥接模式/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-31-设计模式02-桥接模式/","excerpt":"","text":"设计模式02-桥接模式(Bridge Pattern)定义桥接模式：将抽象部分与它的实现部分分离，使他们都可以相互独立的变化。它是一种对象结构型模式，又称为柄体模式或接口模式。 桥接模式将继承管理转换为关联关系，从而降低了类与类之间的耦合，减少了代码编写量。 将两个角色之间的继承关系改为聚合关系，就是将它们之间的强关联改换为弱关联。桥接模式中所谓的脱藕，就是指在一个软件系统的抽象化与实现化之间使用组合/聚合关系而不是继承关系，从而使两者可以相互独立地变化。 由上图可知，上述系统含有两个等级结构： 由抽象化和修正抽象化角色组成的抽象化等级结构 由实现化角色和两个具体实现化角色组成的实现化等级结构。 设计角色桥梁模式所涉及的角色： 抽象化角色（Abstraction）：抽象化给出的定义，并保存一个对实现化对象的引用。 修正抽象化角色（Refined Abstraction）扩展抽象化角色，改变和修正父类对抽象化的定义。 实现化角色（Implementor）：这个角色给出实现化角色的接口，但是不给出具体的实现。这个接口不一定和抽象化角色的接口定义相同。实际上，两个接口可以非常不一样。实现化角色应该只给出底层操作，而抽象化角色应该只给出基于底层操作的更高一层的操作。 具体实现化角色（Concrete Implementor）：这个角色给出实现化角色接口的具体实现。 代码实例：12345678910111213141516171819202122232425262728293031323334public interface Implementor&#123; public void operationImpl();&#125; public class ConcreteImplementor implements Implementor&#123; public void operationImpl() &#123; //具体实现 &#125;&#125; public abstract class Abstraction&#123; protected Implementor impl; public void setImpl(Implementor impl) &#123; this.impl=impl; &#125; public abstract void operation();&#125; public class RefinedAbstraction extends Abstraction&#123; public void operation() &#123; //代码 impl.operationImpl(); //代码 &#125;&#125; 实际代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public abstract class Pen &#123; protected Color color; public void setColor(Color color) &#123; this.color = color; &#125; public abstract void draw(String name);&#125;public class BigPen extends Pen&#123; @Override public void draw(String name) &#123; String penType = &quot;大号毛笔绘制&quot;; this.color.paint(penType ,name); &#125;&#125;public class SmallPen extends Pen&#123; @Override public void draw(String name)&#123; String penType = &quot;小号毛笔绘制&quot;; this.color.paint(penType , name); &#125;&#125;public class MiddlePen extends Pen&#123; @Override public void draw(String name) &#123; String penType = &quot;中号毛笔绘制&quot;; this.color.paint(penType,name); &#125;&#125;public interface Color &#123; void paint(String penType , String name);&#125;public class GreenColor implements Color&#123; @Override public void paint(String penType, String name) &#123; System.out.println(penType + &quot;绿色的&quot; + name + &quot;.&quot;); &#125;&#125;public class RedColor implements Color&#123; @Override public void paint(String penType, String name) &#123; System.out.println(penType + &quot;红色的&quot; + name + &quot;.&quot;); &#125;&#125;public class WhiteColor implements Color &#123; @Override public void paint(String penType, String name) &#123; System.out.println(penType + &quot;白色的&quot; + name + &quot;.&quot;); &#125;&#125;public class BlackColor implements Color&#123; @Override public void paint(String penType, String name) &#123; System.out.println(penType + &quot;黑色的&quot; + name + &quot;.&quot;); &#125;&#125; 调用方法：12345678public class Main &#123; public static void main(String[] args) &#123; Pen pen = new SmallPen(); Color color = new RedColor(); pen.setColor(color); pen.draw(&quot;我的名字是dm&quot;); &#125;&#125; 从以上代码可以看出，桥接模式的脱藕，是指一个软件系统的抽象化和实现化之间使用关联关系（或者聚合关系）而不是继承关系，从而使两者可以相互独立地变化，这就是桥接模式的用意。 桥接模式优缺点优点 分离抽象接口及其实现部分。 桥接模式类似于多继承，但是多继承模式违背类单一职责原则。 提高了系统的可扩充性，在两个变化维度中任意扩展一个维度，都不需要修改原有系统。 实现细节对客户透明，可以对用户隐藏实现细节。 缺点 引入会增加系统的理解与设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计和编程。 要求正确识别出系统中两个独立变化的维度，因此其使用范围具有一定的局限性。 桥接模式适用环境 如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多灵活性，避免在两个层次之间建立静态继承关系，通过桥接模式可以使抽象层建立一个关联关系。 抽象化角色和具体化角色可以以继承的方式独立扩展而不影响。在程序运行时，可以动态将一个抽象化子类和一个实现化子类的对象进行组合，即系统需要对抽象化角色和实现化角色进行动态耦合 一个类存在两个独立变化的维度，且这两个维度都需要进行扩展。 虽然在系统中使用继承是没有问题额，但是由于抽象化角色和具体化角色需要独立变化，设计要求需要独立管理这两者。 对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用。 应用实践JDBC 驱动程序是桥接模式应用之一。使用JDBC驱动程序的应用系统是抽象角色， 而所使用的数据库是实现角色。驱动程序可以动态地将一个特定类型数据库和一个JAVA APP绑定在一起，实现抽象角色和实现角色之间的动态耦合。 参考桥接模式","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"股债平衡策略","slug":"2018-10-29-股债平衡策略","date":"2019-06-26T03:40:39.143Z","updated":"2019-06-26T03:40:39.143Z","comments":true,"path":"2019/06/26/2018-10-29-股债平衡策略/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-29-股债平衡策略/","excerpt":"","text":"股债平衡策略策略概述初始资金均分成两部分，一部分投资股权类ETF，一部分投资国债类ETF，每个周期结束后进行再平衡。周期可以按照一年、半年、每个月。 策略回测采用三种再平衡周期： 年初第一个交易日 年初及年中的第一个交易日（1月和7月） 每月第一个交易日均进行一次平衡 回溯测试回测的标的股权采用ETF 510180 上证180ETF。债券使用000012国债指数。使用这两个ETF是因为这两个标的成立时间早，能支持从2007年开始回溯验证。 测试基准：ETF510180 ，这样更容易看出股债平衡策略的优劣。 初始现金：100万。 测试时间：20070101-20181028，跨越两轮牛熊且目前深处熊市。 测试结果 比较基准收益率：71.01% 一年平衡一次，策略收益率121.27%，最大回撤35.47%,beta0.49 年初年终各平衡一次，策略收益率101.89%，最大回撤39.50% 每月平衡一次，策略收益83%，最大回撤41.82% 感想 回测时间内，股债平衡按年平衡好于半年或者按月平衡策略。因为平衡周期越短，效果就越接近纯股权策略。 根据图形可以看出，股债平衡策略牛转熊时期回撤幅度要小于纯股权，熊市或者震荡市股债策略均好于纯股权。 牛市大幅上涨时期，股债平衡策略肯定是跑不赢纯股权投资的，但是目前A股牛短熊长，股债平衡策略能够保住胜利果实。","categories":[],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/tags/数据分析/"}]},{"title":"影响力","slug":"2018-10-29-影响力","date":"2019-06-26T03:40:39.141Z","updated":"2019-06-26T03:40:39.141Z","comments":true,"path":"2019/06/26/2018-10-29-影响力/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-29-影响力/","excerpt":"","text":"影响力影响力这本书主要讲了销售人员如何利用人类对捷径的依赖，从而扩大自己的销量。读完本书，我反思自己在购买商品的过程中，遭遇的种种销售伎俩。 跟中介看房子，先看环境最差的房子，然后再看好的（对比）。 买了价格很高的商品后，被推荐一堆附属商品，自己就觉着这些附属品不贵了。（认知对比） 汽车销售员告知优惠只限周末（稀缺性-时间限制）。 买戒指被告知，有人可能预定了，需要跟领导商量（利用稀缺性心理）。 跟汽车销售试驾前先签单子，这种利用了承诺心理。 一些水果小贩可能会先让你尝，然后你购买的可能性会大大提高。（互惠原理被利用）## 总结","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"微服务架构学习24-微服务实现DevOps","slug":"2018-10-26-微服务架构学习24-微服务实现DevOps","date":"2019-06-26T03:40:39.138Z","updated":"2019-06-26T03:40:39.138Z","comments":true,"path":"2019/06/26/2018-10-26-微服务架构学习24-微服务实现DevOps/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-26-微服务架构学习24-微服务实现DevOps/","excerpt":"","text":"微服务架构学习24-微服务实现DevOps单体应用拆分成多个微服务之后，每个服务都可以独立进行开发、测试和运维，工作量比之前提升很多。这时候就迫切需要一种新的开发、测试和运维模式来解决问题，这就涉及到了微服务与DevOps。 DevOps传统模式下，开发人员、测试人员和运维人员职责划分十分明确，分属于不同的职能部门，一次业务上线需要三者之间进行多次沟通，整个周期以天为单位。DevOps是一种新型的业务研发流程，业务的开发人员不仅需要负责业务代码的开发，还需要负责业务的测试以及上线发布等全生命周期，真正做到掌控服务全流程。 而要实现DevOps，就必须开发完成代码之后，能够进行自动测试、测试通过后，自动发布到线上。对应的两个过程就是CI和CD。 CI（Continous Integration），持续集成。开发完成代码之后，能够自动进行代码检查，单元测试、打包部署到测试环境，进行集成测试，跑自动化测试用例。 CD（Continous Deploy），持续部署。代码测试通过后，能自动部署到类生产环境进行集中测试，测试通过后再进行小流量的灰度验证，验证通过后可以把代码自动部署到线上。CD 还有另一个解释就是持续交付（Continous Delivery），与持续部署不同的是，持续交付只需要做到代码达到线上发布要求的阶段就可以，接下来代码部署到线上既可以选择手动部署也可以选择自动部署。因为实际情况下，需要人为判断整个发布过程是否正常。 微博的DevOps实践目前业界比较通用的实现DevOps主要有两种，一种是使用Jenkins另一种使用GitLab，微博主要使用GitLab。 由上图可知，一个服务的发布流程主要包含三个步骤： 持续集成。该步骤主要是确保每一次代码的Merge Request都测试通过，可随时合并到代码的DEV分支，主要包括：build阶段（开发分支代码的编译与单元测试）、package阶段（开发分支代码打包成Docker镜像）、deploy阶段（开发分支代码部署到测试环境）、test阶段（开发分支代码集成测试）。 持续交付，步骤的主要作用是确保所有的代码合并到dev分支后，dev的分支代码能够在生产环境测试通过，并通过灰度验证，随时交付线上。主要包含五个阶段：build（Dev分支的代码编译和单元测试）、package阶段（dev分支的代码打包成docker镜像）、deploy阶段（dev分支的代码部署到测试环境）、test阶段（dev分支的代码集成测试）、canary阶段（dev分支的代码小流量灰度验证） 持续部署，该步骤的主要作用是合并dev分支到master，并打包成docker镜像，可随时发布到线上。主要包括四个阶段：build（master主干的代码编译与单元测试）、package阶段（master主干的代码打包成docker镜像）、clear阶段（master主干的代码merge回dev分支）、prod阶段（master代码发布到线上）在gitlab中可以通过一个叫gitlab-ci.yml的文件来定义自动化流程包含哪些阶段，以及每个阶段执行的脚本。 实现DevOps的关键点持续集成阶段持续集成阶段主要目的是保证每一次开发的代码都没有问题，即使合并到主干也能正常工作，主要依靠以下下个部分： 代码检查：通过代码检查可以发现一些潜在的bug，可以集成类似sonarqube来实现代码检查。 单元测试。针对每个具体的代码模块机械能测试。 集成测试就是将各个代码的修改集成到一起，统一部署在测试环境进行测试。为了实现整个流程的自动化，集成自测阶段的主要任务就是跑每个服务的自动化测试用例，所以自动化测试用例覆盖越全，集成测试可靠性就越高。集成测试业务代码部署的测试机器可使用docker技术，以避免测试机器的不足。 持续交付阶段持续交付阶段的目的是保证最新的业务代码，能够在类生产环境中正常运行，一般做法是通过在线上生产环境摘掉两个节点，然后在这两个节点上部署最新的业务代码，再进行集成测试。集成测试通过后再引入线上流量，来观察服务是否正常。通常需要解决两个问题： 如何从生产环境摘除两个节点，需要接入线上的容器管理平台。 如何观察服务是否正常，一个是观察节点本身的状态，如CPU、内存等，一个是观察业务运行产生的warn、error的日志量的大小，尤其error量有异常时，说明最新的代码可能存在异常。 持续部署阶段持续部署并不要求那么完美，许多公司在这个阶段都采用手动发布的方式以控制风险，或者只做到持续交付阶段，对于持续部署并不要求自动化。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习23-容器运维平台DCP","slug":"2018-10-26-微服务架构学习23-容器运维平台DCP","date":"2019-06-26T03:40:39.135Z","updated":"2019-06-26T03:40:39.135Z","comments":true,"path":"2019/06/26/2018-10-26-微服务架构学习23-容器运维平台DCP/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-26-微服务架构学习23-容器运维平台DCP/","excerpt":"","text":"微服务架构学习23-容器运维平台DCPDCP 整体架构主要分成四个部分：基础设施层、主机层、调度层、编排层。 基础设施层：解决镜像仓库的问题。 主机层：解决如何进行资源调度的问题。 调度层：解决容器如何在资源上创建的问题。 编排层：解决容器如何运作以及对外提供服务的问题。 基础设置层DCP基础设施层主要用于提供各种基础设施。主要包括：（1）用于存放容器镜像的镜像仓库（2）提供监控服务的监控中心（3）实时监控系统容量以便自动扩缩容的容量评估系统（4）容器创建后如何加入线上服务的服务发现组件。镜像仓库是DCP核心的基础组件。 主机层主要为了完成资源的调度，针对不同的集群、完成主机的创建、成本的管理以及配置初始化工作，也叫做Pluto层。 主机创建阿里云自动创建的示意图如下： 阿里云除了有单账户调用的并发限制，还会有可用区的库存限制，安全组库存限制以及vSwitchk库存限制，所以在使用阿里云API创建ECS时，当机器规模较大，如果直接指定某个可用区、安全组和vSwitch时，可能因为库存原因导致创建失败。 解决方案是针对可用区、安全组以及vSwwitch都做了多可用区、多安全组以及多Switch配置，在出现库存不够时，就自动切换到别的地方来创建，极大提高了大规模ECS创建的成功率。 成本管理无论是从共享池内创建的机器，还是调用阿里云API创建的ECS，都是有成本的，必须对机器的数量以及使用时长进行记录，以便进行成本管理。阿里云有如下的计费策略： 按量付费 按月付费 按年付费 配置初始化主机创建完成后，需要进行一些基础软件的安装以及配置修改等工作，这就是配置初始化的过程。如果短时间内创建上千台ECS，这时候配置工作量就会非常大。DCP进行主机配置化时，会通过Ansible向所有主机下发配置文件和基础软件，并通过自定义callback queue，把每台主机的初始化状态异步写入到DB中，避免上百台机器同时并发写入DB造成死锁。 调度层DCP中调度层的主要功能是在可用的主机上创建容器，选择Swarm作为容器调度的工具，并在Swarm基础上进行二次封装，定制了自己的调度层Roam，使其具备跨IDC、高可用以及可扩展的特性。下面是Roam的架构图： 编排层DCP编排层的主要作用是对服务进行整合以对外提供服务，主要包括服务依赖、服务发现以及自动扩缩容。 服务依赖DCP通过模板来管理容器的创建，一个服务如果需要进行扩容、创建容器，必须按照模板里的定义的参数来执行。模板定义的参数主要包括：任务的名称、机器的配置、任务依赖、任务详细配置（包括调用阿里云 API创建ECS时的可用区、安全组参数等），其中任务依赖配置项：12&#123;&quot;Sid&quot;:1707061842070000,&quot;Ratio&quot;:0.2,&quot;ElasticCount&quot;:0&#125;&#123;&quot;Sid&quot;:1703271821000000,&quot;Ratio&quot;:0.3,&quot;ElasticCount&quot;:0&#125; 它的含义是执行这个扩容任务时，会自动执行ID为1707061842070000和1703271821000000的扩容任务，并按照没扩容10台容器分别扩容2台和3台依赖容器的比例来执行。 服务发现微博的业务场景包含两种服务，一种是HTTP服务，一种是Motan RPC服务，分别使用不同的服务发现方式。 HTTP服务。考虑到基于Nginx配置reload机制实现的服务发现方式，在高并发访问的情况下，导致吞吐量下降10%，为此DCP在实际业务中基于Nginx和Consul开发了解决方案nginx-upsync-module。 Motan RPC服务。Motan RPC服务启动时，会向注册中心Config Service注册服务，并且注册中心支持多IDC部署。 自动扩缩容DCP 实现自动扩缩容主要依赖容量决策支持系统，由容量决策支持系统来实时监控系统的容量。一旦决策支持系统检测到某个服务需要扩容，会自动创建扩容任务，Config Watcher会监控到扩容任务，并通知CronTrigger有调度策略变更。CronTrigger 接到扩容任务，会调用Schedule来具体执行扩容。 总结","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习22-容器调度和服务编排","slug":"2018-10-24-微服务架构学习22-容器调度和服务编排","date":"2019-06-26T03:40:39.133Z","updated":"2019-06-26T03:40:39.133Z","comments":true,"path":"2019/06/26/2018-10-24-微服务架构学习22-容器调度和服务编排/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-24-微服务架构学习22-容器调度和服务编排/","excerpt":"","text":"微服务架构学习22-容器调度和服务编排容器调度容器调度说的是现在集群里有一批可用的物理机或者虚拟机，当服务需要发布的时候，该选择哪些机器部署容器的问题。Docker容器调度系统，比如Swarm、Mesos、K8s。容器调度主要解决的问题如下： 主机过滤主机过滤是为了解决容器创建时什么样的机器可以使用的问题，主要包含两种过滤。 存活过滤，必须选择存活的节点，主机有可能下线或者故障。 硬件过滤，可选择的集群有Web集群、RPC集群、缓存集群和大数据集群等。不同集群硬件配置差异很大，需要选择合适的集群。 调度策略调度策略主要为了解决容器创建时选择哪些主机最合适的问题，一般通过给主机打分实现。会根据每台主机的可用CPU、内存以及正在运行的容器数量来给每台主机打分。spread策略会选择一个资源使用最少的节点，binpack策略会选择一个资源使用最多的节点。具体选择哪种调度策略，还要看业务场景： 各主机配置基本相同，并且使用比较简单，一台主机上只创建一个容器。每次创建容器的时候，直接从还没有创建过的容器主机中选择一台就可以了。 在某些在线、离线混布场景下，为了达到主机资源最高的目标，需要综合考量容器中跑的业务特点，比如在线业务主要使用CPU，离线业务主要使用磁盘和IO资源，这两种业务适合混跑在一起。 还有一种业务场景，主机上的资源都是充足的，每个容器只要划定了资源限制，理论上跑在一起没问题，但是某些时候会出现资源的抢占，比如都是CPU密集型或者IO密集型的业务就不适合容器混布在一起。 服务编排服务依赖大部分情况下，微服务之间是相互独立的，在容器调度时不需要考虑彼此。但有时候也会存在一些场景，比如服务A调度的前提是必须先有服务B，这样的话就要求在容器调度的时候，需要考虑服务之间的依赖关系。 为此，Docker官方提供了Docker Compose解决方案，允许用用户通过单独的docker-compose.yml来定义一组互相关联的容器组成一个项目，以项目的形式来管理应用。 服务发现容器调度完成后，容器就可以启动了。但是这时容器还不能对外提供服务，服务消费者还不知道这个新节点，所以必须具有服务发现机制，使新的容器节点能够加入到线上服务中去。 常用两种一个是基于Nginx的服务发现，一个是基于注册中心的服务发现。 基于Nginx的服务发现主要针对HTTP服务的，当有新的容器节点时，修改Nginx的节点列表配置，然后利用Nginx的reload机制，会重新读取配置并把新的节点加载进来。 基于注册中心的服务发现主要针对提供RPC服务的，当有新的容器节点时，需要调用注册中心提供的服务注册接口。 自动扩缩容容器完成调度后，仅仅做到有容器不可用时故障治愈还不够，还需要根据实际服务的运行状况做到自动扩容。常见自动扩容的做法是根据容器的CPU负载情况来设置一个扩缩容的容器数量或者比例，比如可以设定容器的CPU使用率不超过50%买一单超过这个使用率就扩容一倍的机器。 总结K8s 解决方案在容器调度、服务编排方面都有成熟的组件，并经过大业务量验证。但是考虑到K8s的复杂性和概念理解的门槛，中小业务团队使用K8s大材小用。 相比之下Swarm和Compose就简单较多，但是功能有限，如果不能满足业务需求的话，不容易进行二次开发。K8s基本上已经是业界标准。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习21-微服务容器化运维","slug":"2018-10-23-微服务架构学习21-微服务容器化运维","date":"2019-06-26T03:40:39.131Z","updated":"2019-06-26T03:40:39.131Z","comments":true,"path":"2019/06/26/2018-10-23-微服务架构学习21-微服务容器化运维/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-23-微服务架构学习21-微服务容器化运维/","excerpt":"","text":"微服务架构学习21-微服务容器化运维服务容器化之前，服务部署在物理机或者虚拟机上，运维通过既有的运维平台来发布服务。应用代码通过Puppet等工具分批发布到这些物理机或者虚拟机上，然后重启服务，完成一次发布流程。业务容器化之后，需要一个面向容器的新型运维平台，能够在现有的物理机或者虚拟机上创建容器，并能够对容器的生命周期进行管理，通常叫容器运维平台。 一个容器运维平台通常包括以下组成部分：镜像仓库、资源调度、容器调度和服务编排。 镜像仓库镜像仓库的概念类似git代码仓库，有一个集中存储的地方，把镜像存储在这里，在服务发布的时候，各个服务器都访问这个集中存储来拉取镜像，然后启动容器。 docker 官方会提供一个镜像仓库地址：https://hub.docker.com/，但是对于业务团队来说，出于安全或者访问速度的需要，会搭建一套私有的镜像仓库。搭建私有镜像仓库的步骤： 权限控制镜像仓库设有两层权限控制：一是必须登录才可以访问，最外层控制。二是对镜像按照项目的方式进行划分，，每个项目拥有自己的镜像仓库目录，并且给每个项目设置项目管理员、开发者以及客人这三个角色。 镜像同步实际生产环境，镜像需要同时发布到几十台或者上百台集群节点上，单个镜像仓库实例往往受带宽原因限制无法同时满足大量节点的下载需求，这时候需要配置多个镜像仓库实例来做负载均衡，这就产生了镜像在多个镜像仓库之间同步的问题。 一般有两种方案，一种是一主多从，主从复制的方案。比如开源镜像仓库Harbor，另一种是P2P方案，比如阿里的容器镜像分发系统蜻蜓。下面介绍Harbor方案。Harbor采取主从复制方案，把镜像传到一个主镜像仓库实例上去，然后其他从镜像仓库实例都从主镜像仓库实例同步。 高可用性一般是把服务部署在多个IDC。两个IDC的镜像同步采用Harbor的双主复制策略，互相复制镜像。这样即使一个IDC出现问题，另一个IDC仍然能够提供服务。 资源调度资源调度解决的是，Docker镜像要分发到哪些机器上去，这些机器从哪里来。服务部署的集群主要包括三种：（1）物理机集群（2）虚拟机集群（3）公有云集群。为了解决资源调度的问题，Docker官方提供了Docker Machine功能，通过它可以在企业内部物理机集群、虚拟机集群或者公有云集群上直接部署容器。 资源调度的最大难点在于如何连接各个不同的集群，统一管理来自不同集群的机器权限管理、成本核算以及环境初始化等操作，这时候需要有一个统一的层来完成这个操作。微博专门开发了 DCP 容器运维平台。 总结镜像仓库解决的是Docker镜像如何存储或者访问的问题，在业务规模较大时，个股业务团队都需要搭建自己的私有镜像仓库。类似Harbor这种开源解决方案能够很好地解决权限控制、镜像同步等操作。 资源调度帮我们解决的是如何整合来自不同的集群的资源的问题，如果业务不止在内部私有云上部署、在公有云上也有部署，那么资源调度会是复杂问题。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习20-微服务容器化","slug":"2018-10-22-微服务架构学习20-微服务容器化","date":"2019-06-26T03:40:39.129Z","updated":"2019-06-26T03:40:39.129Z","comments":true,"path":"2019/06/26/2018-10-22-微服务架构学习20-微服务容器化/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-22-微服务架构学习20-微服务容器化/","excerpt":"","text":"微服务架构学习20-微服务容器化之前的章节讲述了微服务架构的基础组成以及落地过程中的各种问题和解决方案。现在开始进入高阶阶段，讲述微服务与容器、DevOps之间的关系。没有容器的发展，就没有微服务架构的蓬勃发展，也就没有DevOps的盛行。首先看微服务为什么需要容器化。 微服务带来的问题单体应用分成多个微服务后，能够实现快速迭代，但是随之带来的问题是测试和运维部署成本的提高。因为业务需求可能会修改多个微服务的代码，这时候就有多个微服务都需要打包、测试和上线发布。一个业务需求就需要同事测试多个微服务接口、上线多个系统、给测试和运维的工作量增加了很多。减轻测试和运维的负担，解决方案就是DevOps。 DevOps 可以理解为开发和运维的结合，服务的开发者不再负责服务的代码开发，还要负责服务的测试，上线发布甚至故障处理等全生命周期，这样就把测试和运维人员从微服务拆分后所带来的复杂工作中解放出来。 DevOps要求开发、测试和发布的流程必须自动化，需要保证开发人员将本地部署测试通过的代码和运行环境、能够复制到测试环境中去，测试通过后再复制到线上环境进行发布。 复制代码看上去很简单，但是现实里，本地环境、测试环境以及线上环境往往是隔离的，软件配置环境的差异也很大，这也导致了开发、测试和发布流程的割裂。微服务还会面对频繁的线上扩容缩容，一般使用公有云，ECS上只有OS ，还需要按照比如JDK，但是可能不同微服务依赖的JDK版本不同，因此部署工作非常繁琐。解决此种问题的一个方案就是使用Docker。 DockerDocker镜像不光可以打包应用程序，还可以打包应用程序的所有依赖，甚至是OS。Docker解决了DevOps中微服务运行的环境难以在本地环境、测试环境以及线上环境保持一致的难题。 微服务容器化实践Docker镜像在使用的时候并不是把业务代码、依赖软件环境以及OS本身都打包成一个镜像，而是利用Docker镜像的分层机制，在每一层通过编写Dockerfile逐层打包。设计镜像的时候，分层设计、逐层调用，这样可以减少每层镜像的文件大小。 docker大致分成4层： 基础环境层。这一层定义OS运行的版本、时区、yum源、TERM等。 运行时的环境层。定义了业务代码的运行时环境。比如JDK。 Web容器层。定义业务代码运行的容器配置，比如Tomcat的JVM参数。 业务代码层。这一层定义了实际的业务代码的版本。 dockerfile的内容如下：1234567891011121314FROM registry.intra.weibo.com/weibo_rd_content/tomcat_feed:jdk8.0.40_tomcat7.0.81_g1_dnsADD confs /data1/confs/ADD node_pool /data1/node_pool/ADD authconfs /data1/authconfs/ADD authkey.properties /data1/ADD watchman.properties /data1/ADD 200.sh /data1/weibo/bin/200.shADD 503.sh /data1/weibo/bin/503.shADD catalina.sh /data1/weibo/bin/catalina.shADD server.xml /data1/weibo/conf/server.xmlADD logging.properties /data1/weibo/conf/logging.propertiesADD ROOT /data1/weibo/webapps/ROOT/RUN chmod +x /data1/weibo/bin/200.sh /data1/weibo/bin/503.sh /data1/weibo/bin/catalina.shWORKDIR /data1/weibo/bin 总结Docker用来解决：（1）微服务化后测试和发布工作量的提升。（2）弹性扩缩容时不同微服务要求的软件运行环境所带来的机器初始化复杂度的提升。但是docker不是银弹，引入docker后同样会产生新的复杂度问题。比如引入docker后旧的针对物理机的运维模式无法适用了，需要针对容器的运维模式。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习19-搭建微服务治理平台","slug":"2018-10-22-微服务架构学习19-搭建微服务治理平台","date":"2019-06-26T03:40:39.126Z","updated":"2019-06-26T03:40:39.126Z","comments":true,"path":"2019/06/26/2018-10-22-微服务架构学习19-搭建微服务治理平台/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-22-微服务架构学习19-搭建微服务治理平台/","excerpt":"","text":"微服务架构学习19-搭建微服务治理平台微服务治理平台的基本功能 服务管理在微服务治理平台，调用注册中心提供的接口实现服务的管理。一般包括： 服务上下线 节点添加/删除 服务查询 服务节点查询 服务治理通过微服务治理平台，可以调用配置中心提供的接口，动态地修改各种配置来实现服务的治理。 限流。系统出现流量爆发时，修改阈值，给核心服务流出充足的冗余度。 降级。 切流量。 服务监控微服务治理一般包括两个层面的监控。一个是整体监控，比如服务依赖的拓扑图。一个是具体服务的监控，比如服务的QPS等指标。整体监控使用服务追踪系统提供的服务依赖拓扑图，具体服务监控可以通过Grafana等监控系统UI展示。 问题定位微服务治理平台实现问题定位，通过两个层面。一个是宏观层面，通过服务监控发掘异常，比如某个服务的平均耗时异常导致调用失败。一个是微观层面，通过服务追踪来具体定位一次用户请求失败具体是因为服务调用链路的哪一层导致的。 日志查询可以通过接入类似ELK，实时查询某个用户的请求详细信息。 服务运维微服务治理平台可以调用容器管理平台，实现常见运维操作。服务运维主要包括： 发布部署。服务功能有变更时，需要重新发布部署时，调用容器管理平台分批按比例进行重新部署，然后发布到线上。 扩缩容。流量增加或减少时，相应增加或缩减服务在线上的部署实例。 如何搭建微服务治理平台微服务治理平台之所以能够实现上面所说的功能，在于它能够封装对微服务架构内的各个基础设施组件的调用，从而对外提供统一的服务操作API，还提供了可视化的界面，方便操作。 治理平台主要包括：Web Portal层、API层以及数据库DB层。 Web Portal 服务管理界面 服务治理界面 服务监控界面 API主要提API供Web Portal调用。一般包括如下接口： 添加服务接口，针对注册中心。 删除服务接口，针对注册中心。 服务降级/限流/切流量接口，针对配置中心。 服务扩缩容接口，调用容器平台提供的扩缩容接口。 服务部署接口，调用容器平台提供的上线部署接口。 DB层治理平台本身要存储一些数据，主要分以下三种： 用户权限。一般分为可浏览、可更改以及管理员权限。可以对权限做下细分，按照不同服务的负责人进行权限划分。 操作记录。记录用户做的变更操作。 元数据。主要用来把服务在各个系统中对应的记录映射到微服务治理平台中，统一进行管理。比如某个服务在监控系统中可能有个特殊标志，在注册中心又用到了另一个标志，为了统一就需要在微服务治理平台进行统一转换，然后进行数据串联。 总结微服务治理平台主要将微服务的各个基础设施整合在一起，目前大部分开源服务框架都没有这个统一的服务治理平台。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习18-服务配置管理","slug":"2018-10-22-微服务架构学习18-服务配置管理","date":"2019-06-26T03:40:39.124Z","updated":"2019-06-26T03:40:39.124Z","comments":true,"path":"2019/06/26/2018-10-22-微服务架构学习18-服务配置管理/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-22-微服务架构学习18-服务配置管理/","excerpt":"","text":"微服务架构学习18-服务配置管理拆分微服务架构前，曾经的单体应用只需要管理一套配置；拆分为微服务后，每一个系统都需要有自己的配置。 配置中心配置中心思路是把服务的各种配置，如代码配置里的各种参数、服务降级开关甚至依赖的资源都在一个统一的地方进行管理。服务启动时，自动从配置中心中拉取所需的配置。如果配置有变化，同样可以自动从配置中心拉取最新的配置信息，服务无需重新发布。配置中心一般包括以下功能： 配置注册功能 配置反注册功能 配置查看功能 配置变更订阅功能 配置存储结构一般配置中心存储是按照Group来存储的，同一类配置放在一个Group下，以K,V键值来存储。 配置注册与反注册配置中心对外提供register接口用来完成配置注册功能，需要传递的参数包括对应分组Group，以及相应的Key、Value值。1curl &quot;http://ip:port/config/service?action=register&quot; -d &quot;group=global.property&amp;key=reload.locations&amp;value=/data1/confs/system/reload.properties&quot; 反注册接口如下：1curl &quot;http://ip:port/config/service?action=unregister&quot;-d &quot;group=global.property&amp;key=reload.locations&quot; 配置查看需要的参数包括Group和Key ，接口实例如下：1curl &quot;http://ip:port/config/service?action=lookup&amp;group=global.property&amp;key=reload.locations&quot; 配置变更订阅配置中心对外提供getSign来配置变更订阅接口， 客户端本地会保存一个配置对象分组Group的sign值，同时每间隔一段时间去配置中心拉取该Group的sign值，与本地保存的sign值做比较。一旦配置中心的sign值与本地的sign值不同，客户端就会从配置中心拉取最新的配置信息。获取sign的接口信息：curl &quot;http://ip:port/config/service？action=getSign&amp;group=global.property&quot;。 实际业务场景资源服务化在应用规模不大的情况下，依赖的资源比如Memcached或者MCQ消息队列数量不多，因此这些资源的IP可以直接写在本地配置里。但是如果核心memcached服务器达到上千台服务器，经常会有个别机器不可用，这时候不可能去客户端本地配置表里去把不可用机器下线。所以只能借助统一的配置中心，从配置中心中kill掉失效机器。 业务动态降级微服务架构下，拆分的服务越多，出现故障的概率越大。因此需要具有服务治理手段，比如动态降级。如果依赖服务出现故障的情况下，可以通过配置中心下达降级的命令。 分组流量切换如果一个IDC出现故障，可以通过配置中心修改路由。 开源配置中心与选型Spring Cloud Config只支持JAVA，配置存储在git中，变更配置也需要通过git操作，如果配置中心有配置变更，需要手动刷新。 Disconf百度开源分布式配置管理平台，只支持JAVA语音，基于Zookeeper 实现配置变更实时推送到订阅的客户端，可以通过统一的管理界面来修改配置中心的配置。 Apollo携程开源分布式配置中心，支持JAVA和.Net，客户端和配置中心通过HTTP长连接实现实时推送，有统一界面管理配置。 实际选择时Spring Cloud Config 只能通过git操作，变更配置后需要手动刷新，除非采用Spring Cloud，否则不建议使用。 Apollo 对Spring boot支持较好。 总结如果业务复杂，配置比较多且经常需要动态改动，则建议使用配置中心管理，这样减少维护成本。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习17-服务调用失败时的处理手段","slug":"2018-10-22-微服务架构学习17-服务调用失败时有哪些处理手段","date":"2019-06-26T03:40:39.121Z","updated":"2019-06-26T03:40:39.121Z","comments":true,"path":"2019/06/26/2018-10-22-微服务架构学习17-服务调用失败时有哪些处理手段/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-22-微服务架构学习17-服务调用失败时有哪些处理手段/","excerpt":"","text":"微服务架构学习17-服务调用失败时的处理手段单体应用改造成微服务后，需要针对服务调用失败进行特殊处理，服务调用失败时的处理手段如下： 超时单体应用被改造成微服务架构后，一次用户调用会被拆分成多个系统之间的服务调用，任何一次服务调用如果发生问题都会导致最后用户调用失败。在微服务架构下，一个系统的问题会影响所有调用这个系统所提供服务的服务消费者，容易造成整个系统的雪崩。在实际项目中，针对服务调用都要设置一个超时时间，以避免依赖的服务迟迟没有调用结果，而把服务消费者拖死。超时时间采取P999或者P9999，就是以99.9%或者99.99%的调用都在多少毫秒内返回为准。 重试有时候会因为网络的偶发原因导致超时，如果换个节点可能会访问成功。概率上一次服务调用失败概率是1%，连续两次失败概率就是0.01%，失败率降为原来的1%。在实际服务调用时，需要设置一个服务调用超时后的重试次数。 双发每次服务消费者发起服务调用时，都同时发起两次服务调用，一方面可以提高调用的成功率，另一方面两次服务调用哪个先返回就用哪个，平均响应时间也要比一次调用要快，这就是双发。 但是这样会给后端服务两倍的压力，所以一般情况下不采用这种双发。考虑采用一个更为聪明的双发，即“备份请求”，大致思想是服务消费者发起一次服务调用时，在给定的时间内没有返回结果，就立刻发起另一次服务调用。需要特别注意的是，这个时间设置的要比超时时间短的多。备份请求需要设置一个重试比例，避免在服务端出现问题时，大部分请求响应都会超过P90的值，导致请求量翻倍，给服务提供者造成更大的压力，一般情况下重试比例建议设置为15%。 熔断上述的各种处理方案对于偶发故障比较有用，如果是服务提供者出现故障，短时间内无法恢复，这时候建议使用熔断方案。 Closed 状态：正常情况下，断路器处于关闭状态，偶发的调用失败也不影响。 Open 状态：服务调用失败次数达到一定阈值时，断路器就会处于开启状态，后续服务调用直接返回，不会向服务提供者发起请求。 Half Open 状态：断路器开启后，每隔一段时间，会进入半打开状态，这时候会向服务提供者发起探测调用，以确定服务提供者是否恢复正常。如果调用成功，断路器就会关闭，如果没有成功，断路器就会保持开启状态，并等待下一个周期重新进入半打开状态。 Hystrix最经典使用也最广泛的莫过于Netflix的Hystrix。Hystrix的断路器包含关闭、打开、半打开三种状态。Hystrix会把每一次服务调用都用HistrixCommand封装起来，会实时记录每一次服务调用的状态，包括成功、失败、超时还是被线程拒绝。Hystrix的工作过程：当一段时间内服务调用的失败率高于设定的阈值后，Hystrix断路器会进入打开状态，新的服务调用会直接返回，不会向服务提供者发起调用。再等待设定的时间间隔后，Hystrix的断路器又会进入半打开状态，新的服务调用又可以重新发给服务提供者，如果一段时间内服务调用的失败率依然高于设定的阈值后，断路器会重新进入打开状态。否则的话，断路器会被重置为关闭状态。Hystrix的工作原理采用滑动窗口机制实现。 总结请求调用如果是非幂等的不能使用重试，例如绝大多数上行请求。至于双发，服务调用的P999能大幅减少，实践证明是提高服务调用成功率的有效手段。熔断能够很好解决服务故障引起的连锁反应。 Hystrix使用","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习16-服务端出现故障如何应对","slug":"2018-10-16-微服务架构学习16-服务端出现故障如何应对","date":"2019-06-26T03:40:39.119Z","updated":"2019-06-26T03:40:39.119Z","comments":true,"path":"2019/06/26/2018-10-16-微服务架构学习16-服务端出现故障如何应对/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-16-微服务架构学习16-服务端出现故障如何应对/","excerpt":"","text":"微服务架构学习16-服务端出现故障如何应对单体应用改造成微服务后一个好处就是可以减少故障影响份的范围，故障被局限在一个微服务系统本身，而不是整个单体应用都崩溃。微服务系统可能出现的故障： 集群故障。某些代码出现bug，整个集群出现故障，不能对外提供服务。 单IDC故障。某个IDC光缆被挖断，导致整个IDC拖网。 单机故障。集群中个别机器出现故障，对全局没有太大影响力。但是到该机器的请求都失败，影响整个系统的成功率。 集群故障一般产生原因：（1）代码bug，导致内存溢出。（2）双11流量太大，导致超出整个系统的承载力。解决方法主要有两种：限流和降级。 限流针对系统中的每个服务分别设置一个阈值，超过阈值的请求被直接抛弃。衡量服务请求量有两个指标：（1）QPS：每秒请求量（2）工作线程数。QPS由于不同服务的响应快慢不同，所以系统承载的QPS相差很大，因此一般选择使用工作线程数作为限流的指标。 降级停止系统的中的某些功能，保证系统整体的可用性。具体说，在系统内存中专门开辟一块区域用来存储开关的状态。 开关一般用在两种地方，一种是新增的业务逻辑，因为这种逻辑相对不成熟。另一种是依赖的服务或资源，因为依赖的服务或资源不总是可靠的，所以最好是有开关能控制是否对依赖服务和资源发起调用。 在实际业务应用中，降级按照对业务的影响程度进行分级，一般分为三级：一级对业务影响小，可以使用自动降级。三级降级是对业务有重大影响的降级。 单IDC故障有的采用同城双活，有的采用异地多活，支付宝采用三地五中心。采用多IDC部署的时候，最大的好处就是可以当一个IDC故障时，把原来的访问故障IDC的流量切换到正常的IDC。 流量切换的方式一般有两种：基于DNS解析的切换，基于RPC分组的流量切换。 基于DNS解析的流量切换通过把请求访问域名解的VIP从一个IDC切换到另一个IDC。 基于RPC分组的流量切换借助服务路由，向配置中心下发命令，把原来流到故障IDC的流量切换到正常IDC。 单机故障处理单机故障的一个有效方法就是自动重启。设置一个阈值，比如以某个接口的平均耗时为准，当监控这个指标超过阈值时，就把这个机器从集群中摘除掉，然后重启服务并加入到集群中。要放置网络抖动造成的接口超时而触发自动重启。一种方法是收集单机接口耗时数据时，多采集几个点，每10s采集一个点，采集5个点，若5个点中有3个点的数据都超过设定的阈值范围，就认为是单机问题。可以设置最大重启次数，超过这个次数就不重启了，而是需要分析具体的问题了。 服务多IDC部署数据同步服务多IDC部署，服务依赖的数据也需要存储在多个IDC内，影响数据的一致性。 弱一致性模型，比如银行转账。一般都会有读写分离，读可以从多个备份中取数据，写必须要数据同步到所有备份之后再返回。 强一致性模型，比如微博发送评论。写的话只要写入主库就可以返回，然后异步同步到其他的备份。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"投资者的未来","slug":"2018-10-15-投资者的未来","date":"2019-06-26T03:40:39.116Z","updated":"2019-06-26T03:40:39.117Z","comments":true,"path":"2019/06/26/2018-10-15-投资者的未来/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-15-投资者的未来/","excerpt":"","text":"投资者的未来最近读了西格尔教授的《投资者的未来》，作者通过分析标普500指数100年来的收益统计，回答了投资何种股票能获得高收益以及如何应对老龄化社会。 投资何种股票能获得高收益 高增长的企业并不一定能带来高收益，前提不要为过快的增速支付高溢价 高增长的行业以及国家也不一定能带来高收益，收益的高低取决于人们的预期跟实际增长的差值。 股息再投是牛市的加速器熊市的安全垫 医药&amp;消费行业是标普500指数100年来收益最高的两个行业 石油能源行业一般能超过人们的预期，所以通过股息再投反而能获得更高的收益 低PE策略同高股息策略一样都能获得不错的业绩 如何应对老龄化社会 发展中国家会提供发达国家需要的商品 发展中国家用得到的外汇购买发达国家的股权等资产，这就给发达国家的退休人口提供了套现机会 要拥抱全球化，发达国家并不吃亏，他们购买商品的价格很便宜 贸易战是闭关锁国，提高了所有国家的成本，造成了浪费 人类的繁荣依靠紧密交流的，交流不畅会导致文明的衰退 中国落后世界起源于明代的专制统治（清朝专制更甚）","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"微服务架构学习15-如何使用服务路由","slug":"2018-10-10-微服务架构学习15-如何使用服务路由","date":"2019-06-26T03:40:39.114Z","updated":"2019-06-26T03:40:39.114Z","comments":true,"path":"2019/06/26/2018-10-10-微服务架构学习15-如何使用服务路由/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-10-微服务架构学习15-如何使用服务路由/","excerpt":"","text":"微服务架构学习15-如何使用服务路由服务路由的定义：服务消费者在发起服务调用时，根据特定的规则来选择服务节点，从而满足特定的需求。 服务路由的应用场景 分组调用。保证服务高可用，实现异地多活，一个服务往外部署在多个数据中心，对于服务消费者来说，选择哪一个分组调用，就必须有相应的路由原则。 灰度发布。服务上线发布过程中，需要现在一小部分规模的服务节点上发布服务，然后验证功能。也叫金丝雀发布。 流量切换。业务运行中，一些因素导致某个机房产生故障，此时需要将此机房服务的流量切换到其他正常的机房。 读写分离。对于大多数互联网来说读多写少。读写可以分开部署。 服务路由的规则条件路由1condition://0.0.0.0/dubbo.test.interfaces.TestService?category=routers&amp;dynamic=true&amp;priority=2&amp;enabled=true&amp;rule=&quot; + URL.encode(&quot; host = 10.20.153.10=&gt; host = 10.20.153.11&quot;) condition://代表了这一段用条件表达式编写的路由规则。host = 10.20.153.10 =&gt; host = 10.20.153.11，=&gt;前面是服务消费者的匹配条件，后面是服务提供者的过滤条件。 机房隔离：host = 172.22.3.* =&gt; host = 172.22.3.* 读写分离：12method = find*,list*,get*,is* =&gt; host =172.22.3.94,172.22.3.95method != find*,list*,get*,is* =&gt; host = 172.22.3.97,172.22.3.98 脚本路由脚本路由基于脚本语言的路由规则，常用脚本语言比如JS、JRuby等。1&quot;script://0.0.0.0/com.foo.BarService?category=routers&amp;dynamic=false&amp;rule=&quot; + URL.encode(&quot;（function route(invokers) &#123; ... &#125; (invokers)）&quot;) 比如下面JS写的route函数，只有IP为10.20.153.10的服务 消费者可以 发起调用。123456789function route(invokers)&#123; var result = new java.util.ArrayList(invokers.size()); for(i =0; i &lt; invokers.size(); i ++)&#123; if(&quot;10.20.153.10&quot;.equals(invokers.get(i).getUrl().getHost()))&#123; result.add(invokers.get(i)); &#125; &#125; return result; &#125; (invokers)）; 服务路由的获取方式本地配置路由规则存储在服务消费者本地上。 配置中心管理所有的服务消费者都是从配置中心获取路由规则，由配置中心统一管理。 动态下发由运维人员通过服务治理平台修改路由规则，服务治理平台调用配置中心接口，把修改后的路由规则持久化到配置中心。因为服务消费者订阅了路由规则的变更，于是从配置中心获取最新的路由规则，按照最新的路由规则执行。建议服务路由在配置中心统一管理。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习14-如何使用负载均衡算法","slug":"2018-10-10-微服务架构学习14-如何使用负载均衡算法","date":"2019-06-26T03:40:39.111Z","updated":"2019-06-26T03:40:39.112Z","comments":true,"path":"2019/06/26/2018-10-10-微服务架构学习14-如何使用负载均衡算法/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-10-微服务架构学习14-如何使用负载均衡算法/","excerpt":"","text":"微服务架构学习14-如何使用负载均衡算法订阅一个服务，从注册中心查询得到了服务的可用节点列表，列表中有几十个节点，这时候需要考虑负载均衡算法从节点列表中选取使用的节点。 常用负载均衡算法随机算法随机从节点列表中选一个进行访问，各节点访问改了大体相等。参考实现代码应用场景：各个节点性能差异不大。 轮询算法按照固定的顺序，把可用的服务节点，挨个访问一次。采用一个循环数组的额方式实现，本次访问序号为0的节点，下次访问序号为1的节点，依次类推。参考实例代码应用场景：各个节点被访问的概率基本相同，主要应用在各个服务节点性能差异不大。 加权轮询算法轮询算法保证所有节点被访问的概率相同，加权轮询算法在此基础上，给每个节点一个权重，从而使每个节点被访问的概率不同，权重大的节点被访问概率大。在序列中每个节点出现的次数，就是它的权重值。比如三个节点：a、b、c，权重分别是3、2、1，那么生成的序列就是{a、a、b、c、b、a}。需要让生成的序列尽可能均与，如果上面的序列前三次都是a，就会导致前面三次访问的节点都是a。参考代码实现 应用场景：主要应用在各个节点性能差异较大情况下。比如新机器和旧机器性能差异较大。 最少活跃链接算法每一次访问都选择连接数最少的节点，不同节点的处理请求速度不同，可以认为节点连接数最大的节点，其访问速度最慢。参考实现 应用场景：主要应用在各个节点性能差异较大，而且不好做到预先定义权重时。 一致性hash算法通过某个hash函数，把同一个来源的请求都映射到同一个节点上。只有当这个节点不可用的时候，才可以分配到其他节点。参考链接 应用场景：适合服务端节点处理不同客户端请求差异较大的场景。 自适应最优选择算法考虑一种复杂业务场景： 服务节点数量众多，性能差异较大 节点列表经常变化，节点增减频繁 客户端服务端之间网络情况复杂，可能跨不同数据中心。上述复杂业务场景考虑使用自适应最优选择算法。算法主要思路：在客户端本地维护一份同每一个服务节点的性能统计快照，每间隔一段时间（1分钟左右）去更新快照。发起请求时，根据二八原则，找出20%响应最慢的节点，降低权重。这样客户端就能够实时的根据自身访问每个节点性能的快慢，动态调整访问最慢的那些节点的权重，减少访问量，从而可以优化长尾请求。实际设定中，设置20%性能较差节点权重为3，其余节点权重为5。注：每个客户端只考虑自身同服务提供者之间的交互性能。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习13-如何识别服务节点是否存活","slug":"2018-10-09-微服务架构学习13-如何识别服务节点是否存活","date":"2019-06-26T03:40:39.109Z","updated":"2019-06-26T03:40:39.109Z","comments":true,"path":"2019/06/26/2018-10-09-微服务架构学习13-如何识别服务节点是否存活/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-09-微服务架构学习13-如何识别服务节点是否存活/","excerpt":"","text":"微服务架构学习13-如何识别服务节点是否存活心跳开关保护机制在网络频繁抖动的情况下，注册中心可用的节点会不断变化，这时候服务消费者会频繁收到服务提供者节点 变更的信息，于是不断请求注册中心拉取最新的可用服务节点信息。当大量服务消费者，同时请求注册中心获取最新的服务提供者节点信息时，可能会把注册中心带宽占满，尤其是注册中心是百兆网卡情况下。针对此种情况，需要一种保护机制，即使在网络频繁抖动的时候，服务消费者也不至于同时去请求注册中心获取最新的服务节点信息。一个可行解决方案：给注册中心设置一个开关，当开关打开时，即使网络频繁抖动，注册中心也不会通知所有的服务消费者有服务信息变更，比如只给10%的服务消费者返回变更，这样的话就能将注册中心的请求量减少到10%。这个开关仅适合在网络频繁抖动的时候打开，如果网络正常，会导致服务消费者感知最新的服务节点信息延迟。 服务节点摘除保护机制服务提供者在进程启动时，会注册服务到注册中心，并间隔一段时间，汇报心跳给注册中心，以标识自己的存活状态。如果隔了一段固定时间后，服务提供者仍然没有汇报心跳给注册中心，注册中心会认为该节点未dead，于是该提供者就会被从可用节点信息中移除出去。 如果遇到网络问题，大批服务提供者节点汇报给注册中心的心跳信息可能会传达失败，注册中心会把他们都从可用节点中移除，造成剩下的可用节点难以承受所有的调用，引起雪崩。但是此种情况下，可能大部分服务提供者节点是可用的，仅仅因为网络原因无法汇报心跳给注册中心就被摘除了。 上述这种场景，可以设定一个阈值比例，遇到这种情况，注册中心不能摘除超过这个阈值比例的节点。 阈值比例可以根据实际业务的冗余度来确定，通常会把这个比例设定在20%。 心跳开关保护机制，为了防止服务提供者节点频繁变更导致的服务消费者同时去注册中心获取最新服务节点信息。服务节点摘除保护机制，为了防止服务提供者节点被大量摘除引起服务消费者可以调用的节点不足。可以都是因为注册中心的节点信息是随时可能发生变化的，所以这种注册中心是动态注册中心 。 静态注册中心直接在服务消费者端根据调用服务提供者是否成功来判断服务提供者是否可用，如果消费者调用某一个服务提供者节点连续失败超过一定次数，就可以在本地内存中将这个节点标记为不可用。并且每隔一段时间，服务消费者都要向标记为不可用的节点发起保活探测，探测成功后，则将不可用的节点恢复为可用状态，重新发起调用。 这样服务提供者节点就不需要向注册中心汇报心跳信息，注册中心中的服务节点信息也不会冬天变化，称之为静态注册中心。 此种情况可以足以应对网络频繁抖动等复杂场景。 总结针对动态注册中心实际线上业务运行时，如果遇到网络不可靠等因素，提出的解决方案心跳保护机制和服务节点摘除保护机制。而静态注册中心的思路，则是基于服务消费者本身调用来判断服务节点是否可用，更加直接和准确。网络出现问题的时候，方案基本不受影响。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习12-如何搭建一套服务追踪系统","slug":"2018-10-08-微服务架构学习12-如何搭建一套服务追踪系统","date":"2019-06-26T03:40:39.107Z","updated":"2019-06-26T03:40:39.107Z","comments":true,"path":"2019/06/26/2018-10-08-微服务架构学习12-如何搭建一套服务追踪系统/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-08-微服务架构学习12-如何搭建一套服务追踪系统/","excerpt":"","text":"微服务架构学习12-如何搭建一套服务追踪系统首先回顾下服务追踪系统的实现，主要包括三个方面： 埋点数据收集，负责在服务端进行埋点，来收集服务调用的上下文数据。 实时数据处理，负责对收集到的链路信息，按照traceId和spanId进行串联和存储。 数据链路展示，把处理后的服务调用数据，按照调用链的形式展示出来。如果要自己实现，实现方案： 需要在业务代码的框架层开发调用拦截程序，在调用前后收集相关信息，把信息传输到一个统一的处理中心。 数据中心实施处理收集到的链路信息，并按照traceId和spanId 进行串联，处理完后存储到合适的存储中。 把存储中存储的信息，以调用链路图或者调用拓扑图的形式对外展示。中小团队建议使用开源解决方案进行处理。 OpenZipkin Collector 负责搜集探针Reporter埋点采集的数据。经过验证处理并建立索引。 Storage 存储服务调用的链路数据，默认使用Cassandra，可以使用Elasticsearch 或者mysql API 将格式化和建立索引的链路数据以API的方式对外提供服务，比如被UI调用 UI 图形化的方式展示服务调用的链路数据。工作原理如下图所示： 具体流程是，通过在业务的HTTP Client前后引入服务追踪代码，在HTTP方法调用前生成trace信息，以及当前时刻的timestamp，然后调用结果返回后，记录下耗时duration，然后再把这些trace信息和duration异步上传给Zipkin Collector。 Pinpointpinpoint是Naver开源的一款深度支持JAVA的服务追踪系统。Pinpoint 主要由四个部分组成： Agent 通过JAVA字节码注入的方式，收集JVM中的调用数据，通过UDP协议传递给Collector，数据采用Thrift协议进行编码。 Collector 收集Agent传过来的数据，然后写到HBase Storage。 Hbase Storage 采用HBase 集群存储服务调用的链路信息。 Pinpoint WEB UI 通过web UI展示服务调用的详细链路信息。 选型对比埋点支持平台广泛性OpenZipkin提供了不同语言的Library，Pinpoint目前只支持JAVA，前者使用范围更广。 系统集成难以程度OpenZipkin需要在项目中增加trace衣物代码，Pinpoint则是通过字节码注入方式实现拦截服务调用，从而收集trace信息，代码不需要任何调用。 字节码注入的方式是JVM在加载class二进制文件时，动态修改加载的class文件，在方法的前后执行拦截器的before()和after()方法里记录trace()信息。应用不需要修改业务代码，只需要在JVM启动时，添加启动参数。123-javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar-Dpinpoint.agentId=&lt;Agent&apos;s UniqueId&gt;-Dpinpoint.applicationName=&lt;The name indicating a same service (AgentId collection) 调用链路数据的精确度OpenZipkin只支持到接口层面，而Pinpoint则可以深入到所关联的数据库信息。PinpointPinpoint db 总结如果业务代码使用JAVA语言实现，推荐使用Pinpoint。常用的其他开源监控：阿里巴巴鹰眼Jaeger)SkyWalking) 参考如何搭建一套适合你的服务追踪系统","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"穿过迷雾-老巴投资经营思想","slug":"2018-10-07-穿过迷雾-老巴投资经营思想","date":"2019-06-26T03:40:39.104Z","updated":"2019-06-26T03:40:39.105Z","comments":true,"path":"2019/06/26/2018-10-07-穿过迷雾-老巴投资经营思想/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-07-穿过迷雾-老巴投资经营思想/","excerpt":"","text":"","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"大数投资","slug":"2018-10-07-大数投资","date":"2019-06-26T03:40:39.103Z","updated":"2019-06-26T03:40:39.103Z","comments":true,"path":"2019/06/26/2018-10-07-大数投资/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-07-大数投资/","excerpt":"","text":"国庆假期研读了《大数投资》第三版，此版本描述了齐东平教授创建的大数投资，读完之后感到很踏实，也更加坚信A股投资可以获得收益，大数投资是科学证券的方法。主要的心得体会： 大数投资首先拟合的是全部上市公司 上市公司拥有的净资产是大数投资的底线是基础 上市公司依靠净资产赚取的利润是大数投资获得收益的前提 借助数学方法论证了，博弈长期看必然赚取不到利润","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"价值投资新工具-股市温度计和股票束","slug":"2018-10-07-价值投资新工具-股市温度计和股票束","date":"2019-06-26T03:40:39.100Z","updated":"2019-06-26T03:40:39.100Z","comments":true,"path":"2019/06/26/2018-10-07-价值投资新工具-股市温度计和股票束/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-10-07-价值投资新工具-股市温度计和股票束/","excerpt":"","text":"国庆假期研读了《价值投资新工具-股市温度计和股票束》，此书主要面向初学者，通过介绍基本会计知识，格林厄姆的思想，如何衡量市场温度向读者传授投资知识。但是作者本质上还是认为投资A股是一个博弈，股票束的选择也有一定的主观性，如果遇到结构性市场，股票束就会失去意义。跟大数投资相比，不管是作者对中国宏观发展大局，还是具体投资策略实践，差距估计有一个长安街。当然大数投资是齐老师理论联系实践发展了20多年的一个投资方法，是持续演进的。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"微服务架构学习11-如何搭建一个可靠的监控系统","slug":"2018-09-27-微服务架构学习11-如何搭建一个可靠的监控系统","date":"2019-06-26T03:40:39.098Z","updated":"2019-06-26T03:40:39.098Z","comments":true,"path":"2019/06/26/2018-09-27-微服务架构学习11-如何搭建一个可靠的监控系统/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-27-微服务架构学习11-如何搭建一个可靠的监控系统/","excerpt":"","text":"微服务架构学习11-如何搭建一个可靠的监控系统监控系统的组成包括四个环节：数据收集、数据传输、数据处理、数据展示。目前比较主流的开源监控系统实现方案主要有： 以ELK 为代表的集中式日志解决方案 以Graphite、TICK、Prometheus为代表的时序数据库解决方案。 ELKELK是Elasticsearch、Logstash、Kibana三个开源软件产品首字母的缩写，架构可以由下面的图片来描述。Logstash负责日志搜集、Elasticsearch负责检索、Kibana负责数据展示。这种架构需要在各个机器上部署Logstash，消耗比较多的CPU和内存。后来又加了Beats作为数据搜集器，Beats占用的资源几乎可以忽略不计，可以装在每台服务器上做轻量型代理。 GraphiteGraphite由三部分组成，Carbon、Whisper、Graphite-Web ，它的架构图如下： Carbon，接收被监控节点的连接、收集各个指标的数据，写入到carbon-cache中并最终持久化到Whisper存储文件中。 Whisper，一个简单的时序数据库、可以按不同时间粒度来存储数据。 Graphite-Web，一个web app，其主要功能绘制报表与展示、即数据展示。Graphite自身并不包含数据采集组件，但是可以接入StatsD等开业数据采集组件进行采集，再传给Carbon。 TICKTICK是由Telegraf、InfluxDB、Chronograf、Kapacitor四个软件首字母的缩写，是由InfluxData开发的一套开源监控工具栈，Kapacitor 负责数据告警。 Prometheus Prometheus Server：用于拉取metrics信息并将数据存储到时间序列服务器。 Jobs/exporters：用于暴露已有的第三方服务metrics，负责数据搜集 Pushgateway：短期jobs Aalertmanager：数据报警 Prometheus web UI：负责数据展示。 选型对比从四个方面进行对比。 数据搜集 ELK 通过在每台服务器上部署Beats代理采集数据 Graphite 本身没有数据采集组件，需要配合使用StatsD TICK 使用Telegraf作为数据采集组件 Prometheus 通过jobs/exporters组件获取StatsD采集过来的数据 数据传输 ELK Beats采集数据传输给Logstash ，经过Logstash清洗后传给ElasticSearch Graphite 通过第三方采集组件传输给Carbon TICK Telegraf采集的数据，传输给InfluxDB Prometheus Prometheus Server每间隔一段时间定期去从jobs/exporters拉取数据前三种采用推数据的方式，Prometheus采用的是拉取数据的方式，所以Prometheus对服务端的侵入最小，不需要在服务端部署数据采集代理。 数据处理 ELK 可以对日志的任意字段索引，适合多维度的数据查询，在存储时间序列数据方面相对时间序列数据库会有额外的开销。 Graphite 通过Graphite-web支持正则表达式匹配，sumSeries求和、alias给监控项重命名等函数功能。 InfluxDB 通过类似SQL语言的InfluxQL，能对监控数据进行复杂操作。 Prometheus 通过私有的PromQL查询语言。 数据展示Graphite、InfluxDB、Prometheus 自带展示的功能比较弱，但是支持Grafana来做数据展示。ELK则使用Kibana展示数据。 总结 ELK 技术栈比较成熟，应用范围广，除了可用作监控系统外，也可以用作日志查询和分析。 Graphite 基于时间序列数据库存储的监控系统，提供强大的聚合函数用于监控分析，对外提供的API也可以接入其他图形化监控系统如Grafana。 TICK 的核心在于时间序列数据库InfluxDB的存储功能强大，类似SQL语言的复杂数据处理。 Prometheus 独特之处在于它采用了拉数据的方式，对业务影响较小。从实时性考虑，时间序列数据库实时性都要好于ELK，实时性敏感的话，建议选择时间序列数据库解决方案。从使用的灵活性考虑，几种时间序列数据库的监控功能都要比ELK更加丰富，使用更加灵活更加现代化。 新监控系统，建议Graphite、TICK或者Prometheus其中之一。Graphite需要搭配StatsD，界面展示建议使用Grafana。 TICK的数据展示功能建议也是使用Grafana。Prometus 适合Docker封装好的云应用使用。 参考从0开始学微服务","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"老唐谈格林厄姆思想","slug":"2018-09-25-老唐谈格林厄姆思想","date":"2019-06-26T03:40:39.096Z","updated":"2019-06-26T03:40:39.096Z","comments":true,"path":"2019/06/26/2018-09-25-老唐谈格林厄姆思想/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-25-老唐谈格林厄姆思想/","excerpt":"","text":"老唐谈格林厄姆思想参考来源","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"微服务架构学习10-开源RPC框架如何选","slug":"2018-09-25-微服务架构学习10-开源RPC框架如何选","date":"2019-06-26T03:40:39.093Z","updated":"2019-06-26T03:40:39.093Z","comments":true,"path":"2019/06/26/2018-09-25-微服务架构学习10-开源RPC框架如何选/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-25-微服务架构学习10-开源RPC框架如何选/","excerpt":"","text":"微服务架构学习10-开源RPC框架如何选业界应用比较广泛的开源RPC框架主要分为两类： 跟某种特定语言平台绑定的 与语言无关即跨语言平台的跟语言平台绑定的开源RPC框架主要有以下几种： Dubbo 仅支持JAVA Motan 微博使用，只支持JAVA Tars 腾讯使用，只支持C++ Spring cloud 国外Pivotal2014年开源，仅支持JAVA 跨语言平台的开源RPC框架主要有以下几种： gRPC，Google2015年开源，支持常用C++、JAVA、python等 Thrift，Facebook开源，支持常用C++、JAVA、python等 如果业务场景仅仅局限于一种语言的话，可以选择跟语言绑定的RPC框架中的一种，如果涉及多个语言平台之间的互相调用，就应该选择跨语言平台的RPC框架。 限定语言平台的开源RPC框架DubboDubbo主要由消费者、生产者、注册中心、监控系统组成，服务者和消费者都需要引用Dubbo的SDK，采用netty作为通信框架。 Motanmotan也需要在客户端和生产者之间引入SDK，也是采用netty作为通信框架。 TarsTars的架构交互主要包括以下流程： 服务发布流程：web系统上传server的发布包到patch，上传成功后，在web上提交发布server请求，由registry服务传到node，然后node拉取server的发布包到本地，拉起server服务。 管理命令流程：web系统上可以提交管理server服务命令请求，由registry服务传到node服务，然后由node向server发送管理命令。 心跳上报流程：server服务运行后，会定期上报统计信息到stat，打印远程日志到log，定期上报属性信息到prop，上报异常信息到notify，从config拉取配置信息。 client访问server流程，client可以通过server对象名Obj间接访问server，client会从registry上拉取server的路由信息，然后根据具体的业务特性访问server。 spring cloud 对比选型spring cloud提供了全家桶配置，Dubbo、motan基本只提供了最基础的RPC框架功能，其他组件都需要自己实现。 跨语言RPC框架gRpc原理是通过IDL文件定义服务接口的参数和返回值类型，然后通过代码生成程序生成服务端和客户端的具体实现代码，在gRpc里客户端应用可以像调用本地对象一样调用另一台服务器上对应的方法。 Thrift通过IDL文件定义服务接口的参数和返回值类型，然后通过代码生成程序生成服务端和客户端的具体实现代码。 对比选型 从成熟度上说Thrift要早于gRPC，支持25种语言。 gRPC采用HTTP/2作为通信协议，ProtoBuf作为数据序列化格式，在移动端设备的应用以及传输带宽比较敏感的场景下具有很大的优势，开发文档丰富，根据ProtoBuf生成的代码要比Thrift更简洁一些，使用难易程度上更占优势。如果两者都能用，采用gRpc更好。 框架演进Dubbo、Motan、Spring Cloud都计划提供Sidecar组件以支持其他多种语言调用。gRPC和Thrift虽然提供跨语言RPC调用，但是缺乏一系列配套的服务化组件，不过好在大多数都有开源实现，可以直接使用。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"设计模式01-设计模式概论","slug":"2018-09-21-设计模式01-设计模式概论","date":"2019-06-26T03:40:39.091Z","updated":"2019-06-26T03:40:39.091Z","comments":true,"path":"2019/06/26/2018-09-21-设计模式01-设计模式概论/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-21-设计模式01-设计模式概论/","excerpt":"","text":"设计模式01-设计模式概论创建型模式概述创建型模式对类的实例化过程进行了抽象，能够将软件模块中对象的创建和对象的使用分离。为了使软件的结构更加清晰，外界对于这些对象只需要知道它们共同的接口，不需要知道具体的实现细节，使得整个系统的设计更加符合单一职责原则。 创建型模式在创建了什么，由谁创建，核实创建等方面都为软件设计者提供了尽可能大的灵活性。创建型模式隐藏了类的实例的创建细节，通过隐藏对象如何被创建和组合在一起达到使整个系统独立的目的。 创建型模式主要包含以下方式： 单例模式 工厂模式 建造者模式 原型模式 结构型模式结构模式：描述如何将类或者对象结合在一起形成更大的结构，就像搭积木，可以通过简单积木的组合形成复杂的、功能更为强大的结构。结构型模式可以分为类结构型模式和对象结构型模式： 类结构型模式关心类的组合，由多个类可以组合成一个更大的系统，在类结构模式中一般只存在继承关系和实现关系。 对象结构型模式关心类与对象的组合，通过关联关系使得在一个类中定义另一个类的实例对象，然后通过该对象调用其方法。根据“合成复用原则”，在系统中尽量使用关联关系来代替继承关系，因此大部分结构型模式都是对象结构型模式。 结构型模式主要分成以下： 适配器模式 桥接模式 组合模式 装饰模式 外观模式 享元模式 代理模式 行为型模式行为型模式是对在不同的对象之间划分责任和算法的抽象。行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。通过行为型模式，可以更加清晰地划分类和对象的职责，并研究系统在运行时实例对象之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。行为型模式主要有两种分类： 类行为型模式，使用继承关系在几个类之间分配行为，类行为模式主要通过多态方式来分配父类和子类的职责。 对象行为型模式，使用对象的聚合关联关系来分配行为，对象行为型模式主要是通过对象关联等方式来分配两个或多个类的职责。根据“合成复用原则”，系统中要尽量使用关联关系来取代继承关系，因此大部分行为型设计模式都属于对象行为型设计模式。具体行为型模式主要有以下实践： 职责链模式 命令模式 解释器模式 迭代器模式 中介者模式 备忘录模式 观察者模式 状态模式 策略模式 模板方法模式 访问者模式 参考Java Guiede 设计模式","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"微服务架构学习09-开源注册中心选型","slug":"2018-09-20-微服务架构学习09-开源注册中心选型","date":"2019-06-26T03:40:39.088Z","updated":"2019-06-26T03:40:39.088Z","comments":true,"path":"2019/06/26/2018-09-20-微服务架构学习09-开源注册中心选型/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-20-微服务架构学习09-开源注册中心选型/","excerpt":"","text":"微服务架构学习09-开源注册中心选型当下主流的服务注册与发现的解决方案，主要有两种： 应用内注册与发现：注册中心提供服务端和客户端的SDK，业务应用通过引入注册中心提供的SDK，通过SDK与注册中心交互，来实现服务的注册和发现。 应用外注册与发现：业务应用本身不需要通过SDK与注册中心打交道，而是通过其他方式与注册中心交互，间接完成服务注册与发现。 应用内注册与发现的注册中心典型案例是Netflix开源的Eureka，架构图如下：上图中Eureka架构图有三个重要组成部分： Eureka Server：注册中心的服务端，实现了服务信息注册，存储一级查询等功能。 服务端的Eureka Client：集成在服务端的注册中心SDK，服务提供者通过调用SDK，实现服务的注册与反注册等功能。 客户端的Eureka Clinet：集成在客户端的注册中心SDK，服务消费者通过调用SDK，实现服务订阅、服务更新等功能。 应用外注册与发现的注册中心应用外实现服务注册和发现，最典型案例是开源注册中心Consul，架构图如下：架构图中采用Consul实现应用外服务注册和发现主要依靠三个重要的组件： Consul：注册中心的服务端，实现服务注册信息的存储，并提供注册和发现服务。 Registrator：一个开源的第三方服务管理器项目，通过监听服务部署的Docker实例是否存活，来负责服务提供者的注册和销毁。 Consul Template： 定时从注册中心服务端获取最新的服务提供者节点列表并刷新LB配置（比如Nginx的upstream），这样服务消费者就能通过访问nginx获取最新的服务提供者信息。 两者兼得区别，应用内的解决方案适用于服务提供者和消费者同属于一个技术体系，应用外的解决方案一般适合服务提供者和服务消费者采用了不同的技术体系。对于容器化的云应用来说，一般不适合采用应用内SDK解决方案，会侵入业务，应用外的解决方案会解决这个问题。 注册中心选型考虑的问题两个最值得关注的问题，一个是高可用性，一个是数据一致性。 高可用性实现高可用性的方法主要有两种： 集群部署，通过部署多个实例组成集群来保证高可用性 多IDC部署，部署不止一个机房。由上图可知采用了多数据中心，每个数据中心采用了多服务器的部署方式。 数据一致性同时满足CAP理论是不可行的。C代表一致性，A代表可用性，P代表分区容错性。 根据满足CAP的问题，注册中心可以分成三类。 CP型注册中心，牺牲可用性来保证数据强一致性，最典型例子ZOO keeper。zoo keeper只有一个leader，这个leader负责写入，并会同步信息到Followers，这个过程保证强一致性，如果多个网络之间网络存在问题，出现多个leader，注册中心就不可用了。 AP型，典型例子Eureka，牺牲掉强一致性。它不用选举leader，每个Eureka服务器单独报错服务器注册地址，因此可能存在数据信息不一致的情况。但是当网络存在问题的时候，每台服务器都可以完成独立的服务。 CA型，典型例子Consul。Consul采用Gossip协议保证最终一致性，解决方案：在一个有界网络中，每个节点随机与其余节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致，从而保证在最终所有节点一致。 对于数据中心来说，最主要的功能是服务的注册和发现。在网络出现问题的时候，可用性的需求远远高于数据一致性。选择AP型或者CA型注册中心更合适。 总结 如果业务体系都是采用JAVA，Eureka是一个不错的选择。作为服务注册与发现方案能够最大程度的保证可用性。即使节点之间出现数据不一致，仍然能够访问Eureka。 如果业务体系语言比较复杂，Eureka也提供了Sidecar解决方案。也可以考虑使用Consul，它支持多种语言接入。 如果业务已经是云原生的应用，可以考虑使用Consul，搭配Registrator 和 Consul Template 实现应用外的服务注册和发现。 参考极客时间-微服务架构学习","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习08-如何将注册中心落地","slug":"2018-09-18-微服务架构学习08-如何将注册中心落地","date":"2019-06-26T03:40:39.086Z","updated":"2019-06-26T03:40:39.086Z","comments":true,"path":"2019/06/26/2018-09-18-微服务架构学习08-如何将注册中心落地/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-18-微服务架构学习08-如何将注册中心落地/","excerpt":"","text":"微服务架构学习08-如何将注册中心落地注册中心如何存储服务信息服务信息除了包含节点信息（IP端口）以外，还包含其他的一些信息。比如请求失败时重试的次数，请求结果是否压缩等信息。服务信息常用JSON字符串存储，包含多个字段，每个字段代表不同的含义。服务一般会分成多个不同的分组，每个分组的目的不同。 核心与非核心。 机房，从机房维度来分。 线上环境与测试环境。注册中心的信息结构大致如下： 具体存储，一般是按照服务-分组-节点信息三层结构来存储。Service代表服务的具体分组，Cluster代表服务的接口名，节点信息用KV存储。 注册中心工作流程服务提供者注册流程 服务提供者反注册流程 服务消费者查询流程 服务订阅者订阅变更流程 注册发现遇到的问题多注册中心消费者能够同时从多个注册中心订阅服务，服务生产者能够同时向多个注册中心注册服务。 并行订阅服务并行订阅，每订阅一个服务就单独用一个线程来处理，这样即使遇到个别服务节点超时，其他服务节点的初始化连接也不受影响。 批量反注册服务偶发的反注册调用会导致不可用的节点残留在注册中心，变成僵尸节点。服务消费者还会把它当做活节点，继续发起调用，导致调用失败。优化反注册逻辑，对于下线机器、节点晓慧场景，通过调用注册中心提供的批量反注册接口，一次调用就可以把该节点提供的所有服务同时反注册掉。 服务变更信息增量更新消费者启动时，除了会查询订阅服务的可用节点做初始化连接，还会订阅服务的变更，每间隔一段时间从注册中心获取最新的服务节点信息标记SIGN，并与本地保存的sign值比对，如果不一样，就会调用注册中心获取最新的服务节点信息。如果网络抖动频繁，服务提供者会上报给注册中心的心跳一会失败一会成功，这是注册中心会频繁更新服务的可用节点信息，导致消费者频繁从注册中心更新可用的节点信息，严重时产生网络风暴，导致注册中心带宽被打满。采取增量更新方式，注册中心只返回变化的那部分节点信息，这样就可以大量减少消费者从注册中心拉取的数据量。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习07-Dubbo框架里的微服务组件","slug":"2018-09-17-微服务架构学习07-Dubbo框架里的微服务组件","date":"2019-06-26T03:40:39.084Z","updated":"2019-06-26T03:40:39.084Z","comments":true,"path":"2019/06/26/2018-09-17-微服务架构学习07-Dubbo框架里的微服务组件/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-17-微服务架构学习07-Dubbo框架里的微服务组件/","excerpt":"","text":"微服务架构学习07-Dubbo框架里的微服务组件本系列文章为极客时间课程《从0开始学微服务》课程记录。微服务的架构主要包括服务描述、服务发现、服务调用、服务监控、服务追踪这几个基本组件。今天以开源微服务框架Dubbo为例讲解这些组件。 服务发布与引用Dubbo主要采用XML配置方式，下面这段代码是服务提供者的XML配置。其中dubbo:service开头的配置项声明了服务提供者要发布的接口，dubbo:protocol开头的配置项声明了服务提供者要发布的接口的协议以及端口号。Dubbo 会把以上配置项解析成下面的url格式。dubbo://host-ip:20880/com.alibaba.dubbo.demo.DemoService然后通过基于扩展点自适应机制，通过URL的dubbo协议头识别，就会调用DubboProtocol的export()方法，打开服务端口20880，就可以把服务demoService暴露到20880端口了。 下面的代码是服务引用的过程。其中dubbo:reference开头的配置项声明了服务消费者要引用的服务，Dubbo会把以上配置项解析成下面的URL格式：dubbo://com.alibaba.dubbo.demo.DemoService，然后基于扩展点自适应机制，通过URL的dubbo://协议头识别，就会调用DubboProtocol的refer()方法，得到demoService引用，完成服务引用过程。 服务注册与发现服务提供者注册服务的过程，继续前面服务提供者的XML配置为例，其中以dubbo://registry开头的配置项声明了注册中心的地址，Dubbo会把以上配置项解析成下面的URL格式：registry://multicast://224.5.6.7:1234/com.alibaba.dubbo.registry.RegistryService?export=URL.encode(&quot;dubbo://host-ip/com.alibabadubbo.demo.DemoService&quot;)。服务消费者的原理类似，Dubbo会把以上配置项解析成下面的URL格式：registry://multicast://224.5.6.7:1234/com.alibaba.dubbo.registry.RegistryService?refer=URL.encode(&quot;consummer://host-ip/com.alibabadubbo.demo.DemoService&quot;)。 服务调用通常将服务消费者称为客户端，服务提供者称为服务端，一次调用服务解决4个问题： 客户端与服务端如何建立网络连接。 服务端如何处理请求 数据传输采用什么协议 数据该如何序列化和反序列化 前两个问题客户端和服务端如何建立连接，以及服务端如何处理请求是通信框架要解决的问题，Dubbo支持多种通信框架，比如Netty4，需要在服务端和客户端的XML配置中添加配置项：服务端&lt;dubbo:protocol server=&quot;netty4&quot;&gt;客户端&lt;dubbo:consumer client=&quot;netty4&quot;&gt;服务端采用NIO的方式来处理客户端的请求。Dubbo还支持其他协议比如hessian ,RMI,HTTP,Web SERVICE 。 下面这张图描述了私有Dubbo协议的协议头约定。 数据序列化和反序列化方面，Dubbo同样支持多种序列化格式，比如Dubbo、Hession2.0、JSON、Java、Kryo、FST，配置方式如下：&lt;dubbo:protocol name=&quot;dubbo&quot; serialization=&quot;kryo&quot;&gt; 服务监控服务监控主要包括四个流程，数据采集、数据传输、数据处理和数据展示，其中服务框架的作用是进行埋点数据采集然后上报监控系统。在Dubbo框架中，不论生产者和消费者，在执行服务调用的时候，都会经过Filter调用链拦截，比如监控数据埋点就通过在Filter调用链上装备了MonitorFilter来实现，代码实现：参考这里。 服务治理服务治理手段包括节点管理、负载均衡、服务路由、服务容错，下面这张图给出了Dubbo框架服务治理的具体实现。图中的Invoker是服务提供者节点的抽象，Invoker封装了服务提供者的地址和接口信息。 节点管理：Directory 负责从注册中心获取服务节点列表，并封装成多个Invoker，可以把它看做是List&lt;Invoker&gt;，它的值可能是动态变化的，比如注册中心推送变更时需要更新。 负载均衡：LoadBalance负责从多个Invoker中选出一个用于发起调用，选择时可以供多个负载均衡算法。 服务路由：Router 负责从多个Invoker中按路由规则选出子集，比如读写分离、机房隔离。 服务容错：Cluster将Directory中的多个Inboker伪装成一个Invoker，对上层透明，伪装过程包含了容错逻辑，比如采用Failover策略，调用失败后，会选择另一个Invoker，重试请求。 一次服务调用的过程 服务发布与引用，Proxy服务代理层。 服务注册与发现，Registry注册中心层。 服务调用，protocol层。 服务监控，对应实现层是Filter调用链层。 服务治理，Cluster层。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习06-微服务治理的手段","slug":"2018-09-13-微服务架构学习06-微服务治理的手段","date":"2019-06-26T03:40:39.081Z","updated":"2019-06-26T03:40:39.081Z","comments":true,"path":"2019/06/26/2018-09-13-微服务架构学习06-微服务治理的手段/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-13-微服务架构学习06-微服务治理的手段/","excerpt":"","text":"微服务架构学习06-微服务治理的手段本系列文章为极客时间课程《从0开始学微服务》课程记录。微服务改造之后，一次服务调用，服务提供者、注册中心、网络三者可能都会存在问题，此时消费者如何确保调用能够成功呢？这就是服务治理所要解决的问题。 节点管理注册中心主动摘除机制服务提供者定时向注册中心汇报心跳，注册中心根据服务提供者最近一次的汇报心跳与上一次汇报心跳时间做出比较，如果超出一定时间，就认为服务提供者出现问题。然后把节点从服务列表中摘除，并把最近可用服务节点列表推送给服务消费者。 服务消费者摘除机制如果消费者调用生产者节点失败，就把这个节点从内存中保存的可用服务提供者节点列表中移除。理解是hystrix。 负载均衡服务生产者的机器配置可能不同，尽量让配置好的多承担访问量。常用负载均衡算法： 随机算法调用量比较均匀。 轮询算法按照固定权重，对可用服务节点进行轮询。 最少活跃调用算法选择连接数最小的节点发起调用，也就是选择调用量最小的服务节点，性能理论上是最优的。 一致性hash相同参数的请求总是发送到同一服务节点，当一个节点出现故障时，原本发往该节点的请求，基于虚拟节点机制，平衡到其他节点。 服务路由对于消费者而言，内存中的服务节点列表选择哪个节点不仅由负载均衡算法起作用，还由路由规则起作用。路由规则：通过一定的规则如条件表达式或者正则表达式来限定服务节点的选择范围。 路由规则存在的原因 业务存在灰度发布的需求，根据用户尾号 多机房就近访问，根据ip制定路由规则 路由规则配置方式 静态配置。服务消费者本地存放服务调用的路由规则。 动态规则。路由规则存放在注册中心，消费者定期去请求注册中心同步。 服务容错服务容错的常用手段： FailOver 失败自动切换服务消费者发现调用失败或者超时后，自动从可用的服务节点列表中选择下一个节点重新发起调用，也可以设置重试次数。要求服务调用幂等，不管调用多少次，只要是同一个调用，返回结果必须相等，适合读请求。 FailBack 失败通知消费者调用失败后，不再重试。而是根据失败的详细信息，决定后续的执行策略。对于非幂等的调用场景，不能重试，而是应该查询服务端状态，看调用到底是否实际生效。 FailCache 失败缓存服务消费者调用失败或者超时后，不立即重试，而是间隔一段时间再次尝试发起调用。 FailFast 快速失败消费者调用一次失败后，不再重试。一般非核心业务调用，采用快速失败，记录日志后就返回。 一般幂等的使用FailOver或者FailCache，不是幂等调用选择FailBack或者FailFast。 常用开源服务治理手段一般都会在服务框架中默认集成了，比如阿里的Dubbo，微博开源的服务框架Motan，不需要业务代码实现。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习05-如何追踪微服务调用","slug":"2018-09-12-微服务架构学习05-如何追踪微服务调用","date":"2019-06-26T03:40:39.078Z","updated":"2019-06-26T03:40:39.078Z","comments":true,"path":"2019/06/26/2018-09-12-微服务架构学习05-如何追踪微服务调用/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-12-微服务架构学习05-如何追踪微服务调用/","excerpt":"","text":"微服务架构学习05-如何追踪微服务调用本系列文章为极客时间课程《从0开始学微服务》课程记录。 微服务架构下，由于进行了服务拆分，每个服务可能由不同的团队开发，使用了不同的编程语言，可能部署在不同的机器上，分布在不同的数据中心。下图为用户访问微博首页，一次请求所涉及的服务。 服务追踪系统，可以跟踪用户请求都进行了哪些调用，经过哪些服务处理，并且记录每一次调用所涉及的服务的详细信息，这时候如果发生调用失败，就可以通过这个日志快速定位是哪个环节出了问题。 服务追踪作用优化系统瓶颈通过记录调用经过的每一条链路上的耗时，快速定位整个系统的瓶颈点。 优化链路调用通过服务追踪可以分析调用经过的路径是否合理。比如一个服务调用调用下游多个服务，通过调用链分析，可以评估是否每个依赖是否必要，是否可以通过业务优化来减少服务依赖。通过对链路进行分析，可以找出跨数据中心的调用，从而进行优化，尽量避免此种情况发生。 生成网络拓扑通过链路信息，可以生成一张系统的网络调用拓扑图。可以反映系统都依赖哪些服务，以及服务之间的调用关系是什么样的。 透明传输数据业务经常有需求，想把一些用户数据，从调用的开始一直往下传递，以便各个系统都能获取这个信息。 服务追踪系统原理首先推荐一篇论文Dapper，大规模分布式系统的跟踪系统。核心理念就是调用链，通过一个全局唯一的ID将分布在各个服务节点上的同一次请求串联起来，从而还原原有的调用关系，可以追踪系统问题、分析调用并统计各种系统指标。比较有名的服务追踪系统，Zipkin，阿里的鹰眼、美团的MTrace。 服务追踪基本概念traceid，spanid，annonation。 traceId用于标识某一次具体的请求id。当用户的请求进入系统后，会在RPC调用网络的第一层生成一个全局唯一的traceId，并且会随着每一层RPC的调用，不断往后传递，这样的话traceID就可以把一次用户请求在系统中调用的路径串联起来。 spanId标识一次RPC调用在分布式请求中的位置。当用户请求进入系统后，处在RPC调用网络的第一层A时spanid为0，进入下一层RPC调用B的时候spanid为0.1，继续进入下一层C时spanid是0.1.1，而与B同一层RPC调用E的spanid是0.2，这样的话通过spanid就可以定位某一次RPC请求在系统调用中所处的位置，以及它的上下游依赖分别是谁。 annotation用于业务自定义埋点。可以是业务感兴趣的想传到后端的数据，比如一次请求的用户UID。 服务追踪系统实现由上图可知，数据分三层，数据采集层、数据处理层、数据展示层。 数据采集层在系统的各个不同的模块中进行埋点，采集数据并上传给数据处理层进行处理。埋点示意图如下：图中红色方框中的调用过程如下： CS（Client Send）阶段 : 客户端发起请求，并生成调用上下文。 SR（Server Recieve）阶段 :服务端接收请求，并生成上下文。 SS（Server Send）阶段 : 服务端返回请求，并将服务端上下文数据上报。 CR（Client Receive）阶段：客户端接收返回数据，并将客户端上下文数据上报。 数据处理层将数据采集层上报的数据按需计算，然后落地存储供查询使用。数据处理的需求一般分为两类，一类是实时需求，一类是离线计算需求。 实时数据处理一般采用Storm或者Spark Streaming 对链路数据进行实时聚合加工，存储一般采用OLTP数据仓库，比如HBase。使用traceid作为Rowkey，能够把一条调用链聚合在一起，提高查询效率 离线数据处理一般通过运行Map reduce或者Spark 批处理程序来对链路数据进行离线计算，存储一般采用Hive。 数据展示层将处理后的链路信息以图形化的方式展示给用户，常用的一般是调用链路图，一种是调用拓扑图。 调用链路图主要被用来故障定位。 调用拓扑图通过这张图可以看出系统内包含哪些应用，他们之间是什么关系，一级以来调用的QPS、平均耗时情况。 用作全局监控，发现系统中的异常点，从而快速做出决策。比如一个服务出现异常，造成调用链路拓扑图中可以看出对这个服务的调用耗时都变高了，可以用红色的图样标出来，用作监控报警。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习04-如何监控微服务调用","slug":"2018-09-10-微服务架构学习04-如何监控微服务调用","date":"2019-06-26T03:40:39.076Z","updated":"2019-06-26T03:40:39.076Z","comments":true,"path":"2019/06/26/2018-09-10-微服务架构学习04-如何监控微服务调用/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-10-微服务架构学习04-如何监控微服务调用/","excerpt":"","text":"微服务架构学习04-如何监控微服务调用本系列文章为极客时间课程《从0开始学微服务》课程记录。与单体应用相比，在微服务架构下，一次用户调用会因为服务化拆分后，变成多个不同的服务之间调用，需要对拆分后的每个服务都进行监控。监控主要涉及三个方面：监控对象、监控指标、监控维度。 监控对象主要分成是个层次。 用户端监控业务直接对用户提供的功能进行监控。 接口监控指业务提供的功能所依赖的具体RPC接口的监控。 资源监控比如redis 、 mongodb。 基础监控服务器本身的健康状况进行的监控。CPU、内存、IO、网卡带宽。 监控指标请求量。分两个维度，一个是实时请求量，一个是统计请求量。实时请求量用QPS每秒查询次数来衡量。统计请求量用PV即一段时间内用户的访问量来衡量。响应时间一段时间内所有调用的平均耗时来反映请求的响应时间。但是它只代表了请求的平均快慢的情况，有时候更关心慢请求的数量。为此需要将响应时间划分成多个区间，0-10ms、10ms-50ms、50ms-100ms、100ms-500ms、500ms以上这五个区间。500ms以上代表了慢查询请求量。在正常情况下，最后这个区间查询数量应该为0。 错误率错误率的监控通常用一段时间内调用失败的次数占调用总次数的比率来衡量，对于接口的错误率一般用接口返回错误码为503的比率来表示。 监控维度全局维度从整体角度监控对象的请求量、平均耗时以及错误率。 分机房维度为了业务的高可用性，服务通常部署在不止一个机房，不同机房地域不同，同一个监控对象的各种指标相差很大。 单机维度同一个机房内部，不同机器对同一个监控对象各种指标也会有很大的差异。 时间维度同一个监控维度，在每天的同一时刻各种指标也会不同，可能是业务变更导致，可能是运营活动导致。 核心维度区分核心业务和非核心业务。 监控系统原理监控系统包括四个环节：数据采集、数据传输、数据处理和数据展示。 数据采集通常有两种收集方式： 服务主动上报业务代码中或者服务框架加入数据收集代码，每一次服务调用完成后，主动上报。 代理收集服务调用后把调用的详细信息记录到本地日志文件中，然后通过代理去解析本地日志文件，然后上报服务调用的信息。上述两种方式，需要考虑采集数据的频率。在系统比较空闲的时候加大采用率，在系统负债比较高的时候减小采样率。 数据传输一般采用两种方式。 UDP传输 Kafka传输数据采集后发送到指定的Topic，然后数据处理单元再订阅对应的Topic，就可以从Kafka消息队列中读取到对应的数据。数据传输一般采用二进制和文本协议，前者传输效率高，后者可读性好。 数据处理通常有两个维度： 接口维度聚合按接口进行聚合，这样可以得到每个接口的实时请求量、平均耗时等信息。 机器维度聚合把收集的数据按照调用的节点维度聚合在一起，可以从单机维度查看每个接口的实时请求量、平均耗时等信息。数据处理后进行数据持久化，一般采用如下两种数据库： 索引数据库、Elasticsearch 时序数据库、OpenTSDB 数据展示数据展示是把处理后的数据以Dashboard方式展示给用户。 曲线图展示监控变化趋势。 饼状图监控占比分布 格子图展示一些细粒度监控，比如不同机器的接口调用数量和耗时情况 总结微服务改造过程中，没有强大的监控能力，在调用失败时，如果不能快速发现系统问题，对于业务来说是一场灾难。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"巴菲特致股东的信1956-1969","slug":"2018-09-10-巴菲特致股东的信1956-1969","date":"2019-06-26T03:40:39.073Z","updated":"2019-06-26T03:40:39.073Z","comments":true,"path":"2019/06/26/2018-09-10-巴菲特致股东的信1956-1969/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-10-巴菲特致股东的信1956-1969/","excerpt":"","text":"巴菲特致股东的信1956-19691956年这是第一封信开始的时候。 1960年老巴从1956年开始写信，1960年是第4年。 投资哲学相对标普的超额收益不会总是稳定的，但是力争做到股票市场稳定或者下跌的情况下，获得高于平均水平的投资收益。股票市场上涨的情况下，获得与平均水平持平或者略低的投资业绩。投资总会遇到好时候和坏时候，不能因为好时候而非常热情，也不能因为坏时候非常沮丧。 1961年投资第5年。 投资组合组成投资组合分成三个部分。 第一部分价值被低估的证券 并不确切的知道低估股票增值的原因（知道了就不会跌了）。 买入时机的选择要优于卖出时机，不指望能赚到每一分钱，价格合理就卖出。 虽然便宜，但是仍然会继续下跌。 第二部分套利性投资 诸如公司的合并、清算、重组、分拆。 利润较稳定，很大程度上与道琼斯表现无关。 相对安全，可以用借来的钱投资。利用概率下赌注套利。 第三部分 控股公司并施加影响 一年或以上的时间才会见成效。 与市场指数表现无关。（后面会提到，这部分股票价格不采用市场价格估值） 有限合伙公司资金的增加，对这方面的投资是正面的。 对待保守投资的看法 买中长期国债或者类似产品并不是保守，因为这最终导致投资组合的最终实际购买力的下降。 坚持自己的看法，不趋同，不迷信权威。 1956-1961年总结 形成了自己的投资哲学 形成了自己的投资组合策略 上述两种投资方法都是贯穿投资生涯始终。 1962年 认为5年是一个合适的时间段，用来评比投资业绩。3年是最少最少的一个投资绩效时间段。 投资基于价值而不是市场的流行观点。 买下溢价公司后，市场的价格波动不再重要，重要的是这家公司的资金到底价值几何（投股权就是投实业）。 1963年收益率计算因为投资组合投资标的不同，所以今年的收益率是计算全部净资产的变化得到。控股型投资开始的时候受大盘波动影响，但是之后会独立大盘表现，最后结果是比套利独立性还高。 1964年市场正确反映价值的时间无法确定。只要公司能够持续改善盈利情况增加其资产价值，同时股价没什么起色，仍会持续购入。 1965年对于第一种投资和第三种投资，老巴非常苛刻。会挑选价格远远低于价值的时候买入，一旦或得控股权，只要稍微改善公司情况就能保证合理利润。即便不能控股，也降低了长期持股的风险。 投资第9年认为面临投资额过大的情况了。 对控股企业伯克希尔公司，不再当作市场证券，而是当作生意了。即使股价上涨5元，也不会有所收益。同样下跌5元也不会遭受任何损失。 买入相对低估的证券本身就降低了风险，而在之后随着市场某一时刻的修正，潜在地放大收益就将会在未来的某个时点显现出来。 只要认为该投资意味着巨大回报，同时使得该投资标的的价值发生剧烈改变的可能性很小，会将净资产的40%投入进去。（笔者：普通人还是放弃这种想法吧，老巴不世出，本身就是一个幸存者偏差。跟着他学一些普世价值即可，学个股分析这个太需要天赋了）。 投资标的越多，投资业绩结果和预期的年度波动就会越小，但是所预期的收益水平也会降低。 1966年 我们所投资的标的所拥有的价值在我眼中都是被低估的，否则也不会持有了。 随着市场的上涨或下跌，公司的核心价值并不会有很大改变，我们所要做到的就是利用市场的非理性而获利。（散户学学这个还行） 1967年 老巴的个人目标是超越道琼斯指数10%。 认为自己具有高利润性洞察力，认为更确定的利润来自明确的定量决策。 认为现在定量的便宜货几乎没了。 1968年 评估伯克希尔价值美股为31元，虽然价格是37元，但是对BOKEXIER的估值仍然是31元。 1969年 过去20年中，基于定量分析所获得的机会已经逐渐干涸了，到了今年已经完全枯竭了。 不想让自己成为一个永远管理着资金，追逐投资收益的疯狂的兔子。唯一放缓脚步的方法，就是将其停止（考虑退休）。 认为优异的管理人往往也只能取得略胜于市场平均水平的业绩。 把股票当作实业而非股票，如果长期而言实业的业绩良好，那么股票也会有相同表现。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"微服务架构学习03-如何实现RPC远程服务调用","slug":"2018-09-08-微服务架构学习03-如何实现RPC远程服务调用","date":"2019-06-26T03:40:39.071Z","updated":"2019-06-26T03:40:39.071Z","comments":true,"path":"2019/06/26/2018-09-08-微服务架构学习03-如何实现RPC远程服务调用/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-08-微服务架构学习03-如何实现RPC远程服务调用/","excerpt":"","text":"微服务架构学习03-如何实现RPC远程服务调用本系列文章为极客时间课程《从0开始学微服务》课程记录。RPC调用：服务者和消费者运行在两台不同物理机上的不同进程内，，他们之间的调用相比本地方法调用，称之为RPC。RPC调用需要解决以下四个问题。 客户端和服务端如何建立网络连接 HTTP通信 socket通信 服务端如何处理请求 同步阻塞方式。客户端每发一次请求，服务端就生成一个线程去处理。直到线程数到达系统瓶颈。适合于连接数比较小的场景。 同步非阻塞。客户端每发一次请求，服务端并不是每次都创建一个新线程去处理，而是通过IO多路复用技术处理。适合于连接数比较多且请求消耗比较轻的场景，比如聊天服务器。 异步非阻塞。客户端发起一个IO操作后立即返回，等IO操作完成后，客户端会得到通知。适用于连接数比较多且请求消耗比较重的业务场景，比如涉及IO操作的相册。开发难度最大。 上述场景建议还是使用稳妥的开源方案，Netty和MINA。 数据传输采用什么协议一般主要使用http和dubbo 数据如何序列号和反序列化对数据编码解码主要为了减小传输量。常用的有两类：文本类如XML/JSON，二进制类如PB/Thrift。服务化框架Dubbo 不仅支持Dubbo协议，还支持ROMI HTTP JSON Hession2.0 一级JAVA序列化。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习02-微服务的注册和发现","slug":"2018-09-05-微服务架构学习02-服务注册和发现","date":"2019-06-26T03:40:39.069Z","updated":"2019-06-26T03:40:39.069Z","comments":true,"path":"2019/06/26/2018-09-05-微服务架构学习02-服务注册和发现/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-05-微服务架构学习02-服务注册和发现/","excerpt":"","text":"微服务架构学习02-微服务的注册和发现本系列文章为极客时间课程《从0开始学微服务》课程记录。 服务协议想要构建微服务，首先要解决的问题是，服务提供者如何发布一个服务，服务消费者如何引用这个服务。具体来说就是服务接口名是什么，调用这个服务传递的参数是什么，接口的返回值是什么？最常见的是： RESTful API XML配置 IDL文件三者之间的优缺点： 服务注册和发现注册中心：将部署服务的机器地址记录到注册中心，服务消费者在有需求的时候，只需要查询注册中心，输入提供的服务名，就可以得到地址，从而发起调用。 注册中心原理微服务架构下，主要有三种橘色：服务提供者、服务消费者、服务注册中心，三者相互关系看下图。 RPC Server 提供服务，在启动时，根据服务发布文件中的配置信息，向注册中心注册自身服务，并向注册中心定期发送心跳汇报存活状态。 RPC Client 调用服务，在启动时，根据服务引用文件中配置的信息，向注册中心订阅服务，把服务注册中心返回的服务节点列表缓存在本地内存中，并与服务提供者建立连接。 当RPC Server节点发生变更时，注册中心同步变更，RPC Client感知后会刷新本地内存中缓存的服务节点列表。 RPC Client从本地缓存的服务节点列表中，基于负载均衡算法选择一台RPC Server发起调用。 注册中心实现机制注册中心需要体用的接口，如何部署，如何存储服务信息，如何监控服务提供者的存活，如果服务提供者节点有变化如何通知服务消费者，以及如何控制注册中心的访问权限。 注册中心API 服务注册接口，服务提供者通过调用服务注册接口完成服务注册。 服务反注册接口，服务提供者调用反注册接口完成服务注销。 心跳汇报接口，服务提供者通过心跳汇报接口完成节点存货状态上报。 服务订阅接口，服务消费者通过此接口，完成服务订阅，获取可用服务提供者节点列表。 服务变更查询接口，服务消费者调用此接口，获取最新可用服务节点列表。 后台服务查询接口，方便查询注册中心当前注册了哪些服务信息。 服务修改接口，修改注册中心中某一服务的信息。 集群部署注册中心非常重要，一般使用集群部署来保证高可用性，并通过分布式一致性协议来确保不同节点之间的数据保持一致。 目录存储注册中心存储服务信息一般采用层次化的目录结构。 服务健康状态检查zookeeper是通过服务端和客户端的长连接和会话超时控制机制实现服务健康状态检测的。 服务状态变更通知zookeeper通过watcher机制，实现服务状态变更通知给服务消费者。 白名单机制注册中心可以提供一个白名单机制，只有添加到注册中心白名单内的消费者，才能够调用注册中心的注册接口。防止测试环境与生产环境之间错乱。 总结注册中心解耦了生产者和消费者。注册中心还提供了弹性，可以任意伸缩节点。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务架构学习01-初探微服务架构","slug":"2018-09-05-微服务架构学习01-初探微服务架构","date":"2019-06-26T03:40:39.066Z","updated":"2019-06-26T03:40:39.066Z","comments":true,"path":"2019/06/26/2018-09-05-微服务架构学习01-初探微服务架构/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-05-微服务架构学习01-初探微服务架构/","excerpt":"","text":"微服务架构学习01-初探微服务架构本系列文章为极客时间课程《从0开始学微服务》课程记录。 微服务整体架构依据上图微服务架构主要依赖以下基本组件： 服务描述 注册中信 服务框架 服务监控 服务追踪 服务治理服务描述对外提供一个服务，那么服务名是什么，调用服务需要提供哪些信息，调用服务的返回结果是什么，如何解析？这是服务描述所要解决的问题。常用服务描述包括Restful API,XML以及IDL文件三种。 Restful API常用wiki或者Swagger进行管理。XML配置方式主要用作RPC协议服务描述。IDL文件常用作Thrift和gRPC跨语言服务调用。 注册中心有了服务接口的描述，下一步要解决服务的发布和订阅，服务提供者将自己提供的服务以及地址登记到注册中心，服务消费者则从注册中心查询所需要调用的服务的地址，然后发起调用。工作流程： 服务提供者启动时，根据服务发布文件中配置的发布信息向注册中心注册服务。 服务消费者启动时，根绝消费者配置文件中配置的服务信息向注册中心订阅自己需要的服务。 注册中心返回服务提供者地址列表给服务消费者。 当服务提供者发送变化，比如节点新增或者销毁，注册中心将变更通知到服务消费者。 服务框架发起调用前需要解决的问题如下： 服务通信采用什么协议？TCP UDP还是HTTP。 数据传输采用什么方式，同步异步？ 数据压缩采用什么格式，JSON、java对象序列化、Protobuf序列化等。 服务监控服务监控主要包括三个流程： 子表收集。把每一次调用的耗时以及成功与否收集起来，上传到集中的数据处理中心。 数据处理。有了上一步收集到的信息，就可以计算出每秒服务请求量、平均耗时以及成功率。 数据展示。将数据展示在Dashborad上，每隔10秒自动刷新展示，用作业务监控和报警等。 服务追踪除了对服务调用情况进行监控外，需要记录服务调用的每一层链路，以便进行问题追踪和故障定位。工作原理： 消费者发起调用前，本地生成reqid,发起调用时，将reqid作为请求参数一部分传给生产者。 生产者接收到请求后，记录下这次请求的reqid，然后处理请求。如果生产者继续请求其他服务，本地再生成自己的一个reqid，然后当做请求参数继续往下传。 服务治理服务监控发现问题，服务追踪能够定位问题，解决问题就需要服务治理了。服务治理就是通过一系列手段保证各种意外情况下，服务调用仍然能够正常进行。 单机故障服务治理通过一定的策略，自动摘除故障节点，不需要人工干预，就能保证单机故障不会影响业务。 单IDC故障服务治理可以通过自动切换故障IDC的流量到其他正常IDC。 依赖服务不可用服务治理可以通过限流 ，在依赖服务异常的情况下，一段时间内停止发起调用而直接返回。一方面保证消费者能够不被拖垮，另一方面也给服务提供者减少了访问压力。 总结在引入微服务脚骨之前，团队必须掌握这些基本组件的原理并具备开发能力，中小团队建议直接用开源方案。 参考 初探微服务架构","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记12","slug":"2018-09-04-微服务设计笔记12","date":"2019-06-26T03:40:39.064Z","updated":"2019-06-26T03:40:39.064Z","comments":true,"path":"2019/06/26/2018-09-04-微服务设计笔记12/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-04-微服务设计笔记12/","excerpt":"","text":"微服务设计笔记12本篇笔记为微服务设计系列读书笔记的第十二篇，对之前写的11篇文章进行了总结。 微服务的原则 围绕业务概念建模围绕业务的限界上下文定义的接口，比围绕技术概念定义的接口更加稳定。 接受自动化文化微服务引入了复杂性，不得不管理大量的服务。解决问题的一个关键方法：拥抱自动化文化。前期花费一定的成本，构建支持微服务的工具是很有意义的。自动化测试必不可少，调用统一的命令行，以相同方式将系统部署到各个环境很有用，CICD是 对每次提交后的产品质量进行快速反馈的一个关键部分。 隐藏内部实现细节服务应当隐藏掉数据库，避免陷入数据库耦合。尽量选择与技术无关的API，这样就能自由选择使用不同的技术栈。请考虑使用REST，它将内部和外部的实现细节分离规范化。 去中心化为了最大化微服务能带来的自治性，需要持续给拥有服务的团队委派决策和控制权。像企业服务总线和服务编配系统这样的方案，会导致业务逻辑的中心化和哑服务，应当避免使用。 可独立部署使用RPC集成时，避免使用JAVA RMI提供的那种使用桩代码、紧密绑定客户端/服务器的技术。更改单个服务，然后把它部署到生产环境，无需联动地部署其他任何服务，这应该是常态。 隔离失败微服务架构比单块架构更具有弹性，如果我们不考虑调用下游可能会失败的事实，系统会遭受灾难性级联故障。心中有反脆弱的信条，预期任何地方都会发生故障。正确设置超时、了解何时使用舱壁和断路器，来限制故障组件的连带影响。 高度可观察需要从整体看待正在发生的事情，聚合日志和数据方便分析，关联标识可以帮助跟踪系统间的调用。 什么时候不用微服务不了解一个单块系统领域的话，在划分服务前，第一件事情就是了解系统做什么，尝试识别出清晰的模块边界。单块系统稳定后，再拆分。微服务规模化后，面临的挑战会更加严峻。构建微服务前，需要构建基础架构。 参考《微服务设计》","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记11","slug":"2018-09-04-微服务设计笔记11","date":"2019-06-26T03:40:39.061Z","updated":"2019-06-26T03:40:39.061Z","comments":true,"path":"2019/06/26/2018-09-04-微服务设计笔记11/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-04-微服务设计笔记11/","excerpt":"","text":"微服务设计笔记11本篇笔记为微服务设计系列读书笔记的第十一篇，主要记录规模化微服务。讲述了功能降级、服务发现、文档服务、缓存、CAP定理。 故障无处不在从统计学上看，规模化后故障将成为必然事件。所以需要假定故障肯定会发生，没必要阻止故障发生。设计系统时需要考虑的一些指标： 响应时间/延迟（每秒处理200个并发连接时，90%响应时间在2秒内） 可用性 （可接受停机时间） 数据持久性 （多大比例的数据丢失可以接受？数据应该保存多久） 功能降级做功能设计时，需要问自己“如果这个微服务宕机会发生什么”。实例：如果购物车微服务宕掉之后，需要考虑功能降级的具体方式：（1）购物车控件变成一个可下订单的电话号码（2）或者只展示马上回来。 系统需要具有弹性。 架构性安全措施有一些模式，组合起来称为架构性安全措施，可以确保事情出错之后，不会引起严重的级联影响。处理系统缓慢要比处理系统快速失败困难的多。在分布式系统中，延迟是致命的。问题解决方法：（1）正确设置超时（2）实现舱壁隔离不同的连接池并实现一个断路器。实际开发中的一个案例：短线发送业务，短线服务宕机。 反脆弱的组织《反脆弱》塔勒布认为事务实际上是受益于失败和混乱。在生产环境中通过编写程序引发故障。混乱猴子：在一天的特定时段随机停掉服务器。混乱大猩猩:随机关闭整个AWS可用区。延迟猴子：在系统之间注入网络延迟。系统分布在多台机器上，通过网络通信，这都会使系统更脆弱，而不是更见状。 超时 等待太久再决定调用失败，整个系统会被拖慢。 超时太短，会把正常调用当做失败处理。 完全没有超时，一个宕掉的下游系统可能会让整个系统挂起。解决方案：给所有的跨进程调用设置超时，并选择一个默认的超时时间。当超时发生时，记录到日志中，并相应地调整他们。 断路器使用断路器时，对下游的资源请求发生一定数量的失败后，断路器会打开。所有的请求在断路器打开状态下会快速失败。一段时间后，客户端发送一些请求查看下游服务是否已经恢复，如果得到正常响应，将重置断路器。疑问：断路器如何判断下游服务已经恢复？ 舱壁把自己从故障中隔离开的一种方式。断路器可以看做密封一个舱壁的自动机制。对所有同步的下游调用都使用断路器。 幂等所谓幂等，多次执行所产生的影响，均与一次执行的影响相同。如果操作是幂等的，可以对其重复多次调用，而不必担心有不利影响。 扩展扩展的目的：（1）帮助处理失败，担心有东西会失败，多一些这样的东西会有帮助。（2）性能扩展。 更强大的主机换更好的CPU和内存，改善延迟和吞吐量，垂直扩展。虚拟化机器比较方便使用此方法。 拆分负载比如报表服务负载较高，可以拆分出来，单独部署。 分散风险微服务部署在多个主机上，甚至部署在多个不同的数据中心。 负债均衡基于worker的系统重新设计考虑10倍容量的增长，但是超过100倍容量时就要重写了。比如，从同步请求转换成基于事件的系统，采用新的部署平台，改变整个技术栈。花了6个月开发了一个项目，但是却没人用。倒不如在网页上挂个链接，看到底有没有人点击，再决定是否需要做。 扩展数据库服务的可用性和数据的持久性区分服务的可用性和持久性，写入主数据库的所有数据，都复制到备用副本数据库。如果主数据库出现故障，应该需要有一个机制让主数据库恢复或者提升副本为主数据库。 扩展读取考虑模式只读副本。最终一致性：服务可以在单个主节点上进行所有的写操作，但是读取被分发到一个或多个只读副本。从主数据库复制到副本，是在写入后的某个时刻完成的，这种技术有时候看到的可能是失效的数据，但是最终能读取到一致的数据。 扩展写操作使用分片的方法，将数据存储到多个数据库节点。写操作的复杂性来自于查询处理。查找单个记录很容易，可以应用哈希函数找到数据应该在哪个实例上，然后从正确的分片获取它。如果查询跨越了多个节点，往外采用异步机制，将查询的结果放进缓存。比如mongo使用,ap/reduce机制。使用分片系统会出现一个问题，增加一个额外的数据库节点该怎么办？需要大量的宕机时间，现在Cassandra可以不停机增加分片，但是仍然需要充分测试。写入分片会扩展容量，但是不会提高弹性。综上所述，扩展数据库写分片可能非常棘手，长远开可能需要Cassandra,mongo 这样的数据库系统。 缓存客户端、代理和服务器端缓存客户端缓存：如果想改变缓存方式，让大批的消费者全都变化是很困难的。 HTTP缓存HTTP header中的cache-control Expires， 为写使用缓存使用后写式，可以先写入本地缓存中，并在之后的某个时刻将缓存中的数据写入下游、可能更规范化的数据源中。 当有爆发式的写操作，或者同样的数据可能会被写入多次，可以尝试使用本方法。 为弹性使用缓存缓存可以在故障时实现弹性，使用客户端缓存，如果下游服务不可用，客户端可以先使用缓存中可能失效了的数据。 隐藏源服务保护源服务，在后台异步重建缓存。有些源服务只能处理一部分流程，因为大部分的请求已经被前面的缓存处理了，如果整个缓存区小时，源服务就容易被搞崩溃。 保持简单缓存越多，越难评估任何数据的新鲜程度，保持简单，先在一处使用缓存，在添加更多的缓存前慎重考虑。 CAP定理CAP定理：分布式系统中有三方面需要作出权衡：一致性、可用性、分区容忍性。我们最多只能保证三个中的两个。 一致性：当访问多个节点时能得到同样的值。 可用性：每个请求都能获得响应。 分区容忍性：集群中的某些节点在无法联系后，集群整体还能继续进行服务的能力。 考虑上图中双向数据同步中断失败的情况。 牺牲一致性不停用库存服务，导致DC1和DC2的数据不一致。但是系统仍然可以使用，两个节点在系统分区后仍然能够服务，但是失去一致性，AP系统。系统放弃一致性以保证分区容忍性和可用性的这种方法，称为最终一致性。 牺牲可用性我选采取拒绝响应服务，牺牲可用性。系统是一致且分区容忍的即CP。在这种模式下，我们的服务必须考虑如何做到功能降级，直到分区恢复以及数据库节点之间可以重新同步。建议最好还是放弃一致性，去努力构建一个最终一致性的AP系统。 牺牲分区容忍性CA系统是不存在的。如果系统没有区分容忍性，就不能跨网络运行，即需要在本地一个进程内运行。 AP还是CPAP更简单，CP系统因为要支持分布式一致性会遇到更多的挑战。对于库存系统，一个记录过时了5分钟可以接受，那么这个系统是一个AP系统。但是对于银行查询余额聚德考虑CP系统了。 不一定是全部系统作为一个整体，不需要全部是AP或者CP的。目录服务可能是AP的，但是库存服务需要是CP的，因为我们不想给客户一个不存在的东西。 我们要做的是把CAP定理的权衡，推到单独服务的每个功能中去。 动态服务注册zookeepr配置管理、服务间数据同步、leader选举、消息队列和命名服务。zookeeper核心提供了一个用于村粗信息的分层命名空间。我们可以在这个结构中存储服务位置的信息，并且可以作为一个客户端接收更改信息。经验：不应该实现自己的分布式协调系统，使用已有的可工作的选择是非常明智的。 Consul也是提供配置管理和服务发现。 Eureka具有基本负载均衡的功能，可以支持服务实例的基本轮询调度查找。 文档服务正确进行了服务发现，能够知道东西在哪。但是如何知道这些东西的用处，如何使用它们？一个明显的选择就是API稳定。文档总会过时，理想的情况下确保文档总是和最新的微服务API同步，并能够方便提供大家查阅。两种不同的技术Swagger和HAL都可以达到目的。 Swagger产生一个友好的用户界面，可以查看文档并通过web浏览器与API交互。 wiki使用wiki记录API。 参考《微服务设计中文版》","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践07-配置中心服务化以及高可用","slug":"2018-09-04-微服务实践07-配置中心服务化以及高可用","date":"2019-06-26T03:40:39.058Z","updated":"2019-06-26T03:40:39.058Z","comments":true,"path":"2019/06/26/2018-09-04-微服务实践07-配置中心服务化以及高可用/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-04-微服务实践07-配置中心服务化以及高可用/","excerpt":"","text":"微服务实践07-配置中心服务化以及高可用前两篇介绍，客户端都是直接调用配置中心的server端来获取配置信息的。这样客户端与服务端的耦合性太高，如果server端要做集群，客户端只能通过原始的方式来路由。server端改IP地址的时候，客户端也需要修改配置，不符合springcloud 服务治理的理念。我们只需要将config server端当做一个服务注册到eureka中，client端去eureka中获取配置中心server端的服务即可。 运行eureka注册中心使用《微服务实践01-服务中心eureka》文章中的代码生成运行jar包。 123java -jar eureka.jar --spring.profiles.active=peer1java -jar eureka.jar --spring.profiles.active=peer2java -jar eureka.jar --spring.profiles.active=peer3 config server端改造添加依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; 配置文件新增eureka server地址1eureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/ 启动类123456789@EnableConfigServer@SpringBootApplication@EnableDiscoveryClientpublic class GitConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GitConfigServerApplication.class, args); &#125;&#125; 客户端改造添加依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; 配置文件bootstrap.properties 123456spring.cloud.config.name=configspring.cloud.config.profile=devspring.cloud.config.label=masterspring.cloud.config.discovery.enabled=truespring.cloud.config.discovery.serviceId=spring-cloud-config-servereureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/ 启动类12345678@SpringBootApplication@EnableDiscoveryClientpublic class ConfigClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigClientApplication.class, args); &#125;&#125; 见下图所示，服务提供者和消费者都已经连接上注册中心。 测试http://localhost:9002/hello 参考springcloud(八)：配置中心服务化和高可用","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践06-配置中心git自动refresh","slug":"2018-09-04-微服务实践06-配置中心git自动refresh","date":"2019-06-26T03:40:39.056Z","updated":"2019-06-26T03:40:39.056Z","comments":true,"path":"2019/06/26/2018-09-04-微服务实践06-配置中心git自动refresh/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-04-微服务实践06-配置中心git自动refresh/","excerpt":"","text":"微服务实践06-配置中心git自动refreshSpring Cloud Config分服务端和客户端，服务端负责将git（svn）中存储的配置文件发布成REST接口，客户端可以从服务端REST接口获取配置。但客户端并不能主动感知到配置的变化，从而主动去获取新的配置。客户端如何去主动获取新的配置信息呢，springcloud已经给我们提供了解决方案，每个客户端通过POST方法触发各自的/refresh 实现客户端刷新配置添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; 配置文件1234spring.application.name=spring-cloud-config-clientserver.port=8002management.security.enabled=falsemanagement.endpoints.web.exposure.include=refresh,health,info 开启客户端更新机制12345678910111213141516@RestController@RefreshScope // 使用该注解的类，会在接到SpringCloud配置中心配置刷新的时候，自动将新的配置更新到该类对应的字段中。public class HelloController &#123; @Value(&quot;$&#123;name&#125;&quot;) private String hello; @RequestMapping(&quot;/hello&quot;) public String hello() &#123; return this.hello; &#125; @RequestMapping(&quot;/index&quot;) public String index() &#123; return &quot;Hello World&quot;; &#125;&#125; 注意上述代码中的RefreshScope。 测试更新以post请求的方式来访问http://localhost:8002/refresh 就会更新修改后的配置文件。 首先github更改后，http://127.0.0.1:8001/config/dev浏览器返回信息显示，服务端已经获得最新的配置信息。浏览器输入http://127.0.0.1:8002/hello，此时浏览器仍然显示旧配置。 执行curl -X POST http://localhost:8002/actuator/refresh。123λ curl -X POST http://localhost:8002/actuator/refresh返回[] 遗留问题客户端需要手动执行refresh接口，才会获得最新的配置文件。可以结合github 的webhook功能，但是需要配置一堆hook配置，这样处理不够优雅。 参考springcloud(七)：配置中心svn示例和refresh","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践05-配置中心git","slug":"2018-09-02-微服务实践05-配置中心git","date":"2019-06-26T03:40:39.053Z","updated":"2019-06-26T03:40:39.053Z","comments":true,"path":"2019/06/26/2018-09-02-微服务实践05-配置中心git/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-09-02-微服务实践05-配置中心git/","excerpt":"","text":"微服务实践05-配置中心git线上的项目变的日益庞大，每个项目都散落着各种配置文件。如果采用分布式的开发模式，需要配置的文件随着服务增加不断增多。每一个基础服务信息变更，都会引起一系列的更新和重启。配置中心主要解决此类问题。目前市面上开源的配置中心： QConf diamond disconf Apache Commons Configuration owner cfg4jSpring Cloud Config配置中心提供的功能： 提供服务端和客户端支持 集中管理各环节的配置文件 配置文件修改之后，可以快速的生效 可以进行版本管理 支持大的并发查询 支持各种语言 Spring cloud config完美支持以上所有需求，包含client和server两个部分，server提供配置文件的存储、以接口形式将撇子文件的内容提供出去，client通过接口获得数据，并依据此数据初始化自己的应用。 创建配置文件首先在github上面创建了一个文件夹config-repo用来存放配置文件，为了模拟生产环境，我们创建以下三个配置文件：confid-dev.properties、confid-prod.properties、confid-test.properties。每个配置文件中增加一个字段name，值为hello-dev/prod/test。 server端添加依赖123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 配置文件123456789101112server.port=8001spring.application.name=spring-cloud-config-server#服务的git仓库地址spring.cloud.config.server.git.uri=https://github.com/dumingcode/mySpringCloud/#配置文件所在的目录spring.cloud.config.server.git.search-paths=config-repo#配置文件所在的分支spring.cloud.config.label=master#git仓库的用户名spring.cloud.config.username=dumingcode#git仓库的密码spring.cloud.config.password=xxx 启动类启动类添加@EnableConfigServer，激活对配置中心的支持。12345678@SpringBootApplication@EnableConfigServerpublic class GitConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GitConfigServerApplication.class, args); &#125;&#125; 测试访问urlhttp://localhost:8001/config/dev,返回内容如下：1&#123;&quot;name&quot;:&quot;config&quot;,&quot;profiles&quot;:[&quot;dev&quot;],&quot;label&quot;:null,&quot;version&quot;:&quot;d90a6696a0669f8c17b65a33742d0ff3d650fe39&quot;,&quot;state&quot;:null,&quot;propertySources&quot;:[&#123;&quot;name&quot;:&quot;https://github.com/dumingcode/mySpringCloud//config-repo/config-dev.properties&quot;,&quot;source&quot;:&#123;&quot;name&quot;:&quot;hello-dev&quot;&#125;&#125;]&#125; 上述的返回的信息包含了配置文件的位置、版本、配置文件的名称以及配置文件中的具体内容，说明server端已经成功获取了git仓库的配置信息。 证明配置服务中心可以从远程程序获取配置信息，http请求地址和资源文件映射如下:，可参考 /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties Client端添加依赖12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件需要配置两个配置文件，application.properties和bootstrap.properties。application.properties:12spring.application.name=spring-cloud-config-clientserver.port=8002 bootstrap.properties:1234spring.cloud.config.name=configspring.cloud.config.profile=devspring.cloud.config.uri=http://localhost:8001/spring.cloud.config.label=master 特别注意：上面这些与spring-cloud相关的属性必须配置在bootstrap.properties中，config部分内容才能被正确加载。因为config的相关配置会先于application.properties，而bootstrap.properties的加载也是先于application.properties。 启动类1234567@SpringBootApplicationpublic class ConfigClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigClientApplication.class, args); &#125;&#125; 测试输入http://localhost:8002/hello，页面上显示hello-dev 修改config直接在github上修改config-dev.properties，这时候config-server会刷新最新值，但是config-client仍然是之前的旧值，因为spring boot项目自有在启动的时候才会获取配置文件中的值。 整体架构图 参考spring cloud config client代码server代码springcloud(六)：配置中心git示例","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"被遗忘的士兵-一个德国士兵的苏德战争回忆录","slug":"2018-08-28-被遗忘的士兵","date":"2019-06-26T03:40:39.051Z","updated":"2019-06-26T03:40:39.051Z","comments":true,"path":"2019/06/26/2018-08-28-被遗忘的士兵/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-28-被遗忘的士兵/","excerpt":"","text":"被遗忘的士兵-一个德国士兵的苏德战争回忆录10天的时间睡前读完了这本纪实文学，主人公作为一个德国士兵参与了远征俄国、从俄国败退、本土防卫等整个战争进程。文中描述了战争双方士兵交战的惨烈、双方平民的死伤，这场战争就没有胜利者，所有的参与者都付出了很大的牺牲。 文中作者多次提到了俄国人的人口优势，可见真到了危急关头，人口众多还是有优势的。看完此文希望网上的愤青还是闭嘴吧，永远不要战争。 暂时写这么一点点文字，尽量把读过的书，都做个记录。读后感悟多的就多写，感悟少或者不适合放在网上的就少写。 韬光养晦，闷声发财。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"ELK日志聚合监控","slug":"2018-08-27-elk日志聚合监控","date":"2019-06-26T03:40:39.048Z","updated":"2019-06-26T03:40:39.049Z","comments":true,"path":"2019/06/26/2018-08-27-elk日志聚合监控/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-27-elk日志聚合监控/","excerpt":"","text":"ELK-STACK安装本文尝试使用elasticsearch、filebeat、kibana打造个人应用系统的聚合监控体系。 Elasticsearch安装123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.tar.gztar -xzf elasticsearch-6.3.2.tar.gzcd elasticsearch-6.3.2/ 创建elk执行用户 建立elk用户组：groupadd elk 新建elk用户，属于elk用户组：useradd -g elk elk 修改elk用户的密码：passwd elk 按照系统户会提示你输入密码并确认密码 切换帐号： su elk 前台运行elasticsearch12./bin/elasticsearch 执行命令curl -X GET &quot;localhost:9200/&quot;结果如下：1234567891011121314151617&#123; &quot;name&quot; : &quot;VLkRNOA&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;PYO6FOREQPq7oQjcx5glLw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.3.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;053779d&quot;, &quot;build_date&quot; : &quot;2018-07-20T05:20:23.451332Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.3.1&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 后台运行elasticsearch1elasticsearch -d -p /var/run/elasticsearch.pid 创建索引1curl -X PUT &quot;localhost:9200/beat/&quot; 安装kibana1234wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-linux-x86_64.tar.gzshasum -a 512 kibana-6.3.2-linux-x86_64.tar.gz tar -xzf kibana-6.3.2-linux-x86_64.tar.gzcd kibana-6.3.2-linux-x86_64/ 配置kibana12345[root@master-node ~]# vim /etc/kibana/kibana.yml # 增加以下内容server.port: 5601 # 配置kibana的端口server.host: 127.0.0.1 # 配置监听ipelasticsearch.url: &quot;http://127.0.0.1:9200&quot; # 配置es服务器的ip，如果是集群则配置该集群中主节点的iplogging.dest: /var/log/kibana.log # 配置kibana的日志文件路径，不然默认是messages里记录日志 运行kibana后台运行nohup bin/kibana &amp; ，前台运行直接bin/kibana 检查运行情况12netstat -lntp |grep 5601tcp 0 0 127.0.0.1:5601 0.0.0.0:* LISTEN 24330/./../node/bin nginx设置kibana身份验证需要使用yum install httpd安装命令htpasswd。然后htpasswd -c /etc/nginx/.htpasswd username根据提示输入密码。 1234567891011121314151617181920server &#123; listen 80; server_name kibana.xxxx.site; client_max_body_size 60M; client_body_buffer_size 512k; if ( $host != &apos;kibana.xxxxx.site&apos; ) &#123; return 404; &#125; location / &#123; port_in_redirect on; proxy_pass http://kibana; auth_basic &quot;kibana login auth&quot;; auth_basic_user_file &quot;/usr/local/nginx/secure/kibana.pwd&quot;; proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; &#125; &#125; 安装FileBeat1wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.2-linux-x86_64.tar.gz fileBeat运行配置文件注意设置的日志扫描路径以及输出到Elasticsearch的配置1234567891011121314151617181920212223242526272829- type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /usr/local/nginx/logs/*.log - /var/log/redis.log - /usr/local/mongodb/logs/*.log - /var/log/www/*.log - /var/log/low/*.log - /var/log/datacenter/*.log - /var/log/daily/*.log #- c:\\programdata\\elasticsearch\\logs\\*#-------------------------- Elasticsearch output ------------------------------setup.template.name: &quot;app&quot; setup.template.pattern: &quot;app-*&quot;output.elasticsearch: # Array of hosts to connect to. hosts: [&quot;localhost:9200&quot;] index: &quot;beat&quot; # Optional protocol and basic auth credentials. #protocol: &quot;https&quot; #username: &quot;elastic&quot; #password: &quot;changeme&quot; fileBeat运行1nohup ./filebeat -c ./filebeat.yml -path.logs logs/ &amp; kibana 界面设置elk stack搭建完毕后，进入kibana设置 visualize 和 dashboard。需要先设置visualzi，然后再去dashboard将设置的visualize视图加入。 参考Installing the Elastic Stack搭建ELK日志分析平台","categories":[],"tags":[{"name":"CICD","slug":"CICD","permalink":"http://yoursite.com/tags/CICD/"}]},{"title":"微服务设计笔记10","slug":"2018-08-23-微服务设计笔记10","date":"2019-06-26T03:40:39.046Z","updated":"2019-06-26T03:40:39.046Z","comments":true,"path":"2019/06/26/2018-08-23-微服务设计笔记10/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-23-微服务设计笔记10/","excerpt":"","text":"微服务设计笔记10本篇笔记为微服务设计系列读书笔记的第十篇，主要记录康威定律和组织架构。康威定律：任何组织在设计一套系统时，所交付的设计方案在结构上都与该组织的沟通结构保持一致。如果你有四个小组开发一个编译器，那你会得到一个四步编译器。 松耦合和紧耦合组织紧耦合组织：商业产品公司。松耦合组织：分布式开源社区。组织的耦合度越低，其创建的系统的模块化就越好，耦合就越低。windows vista统计数据，与组织结构相关联的指标和软件质量的相关度最高。Amazon和Netflix 信奉组织和架构应该一致。两个比萨团队：没有一个团队应该大到两个披萨不够吃。Netflix确保其本身由多个小而独立的团队组成，以保证他们创建的服务也能独立于彼此。我们可以做什么？ 适应沟通途径单一的 物理位置上在一起的团队，能够进行频繁的细粒度的沟通。参与开发系统的开发人员之间存在地理位置的差异，是一个应该对服务进行拆分的很明显的信号。 共享服务的原因服务所有权：拥有服务的团队负责对该服务进行更改。共享服务所有权的模式，这种效果不佳。 难以分割拆分服务成本太高是多个团队负责单个服务的原因之一。 特性团队特性团队：一个小团队负责开发一系列特性需要的素有功能，即使这些功能需要跨越组件的边界。特性团队是对传统IT组织进行的一种修正，比如一个团队负责用户界面，一个团队负责应用程序逻辑，一个团队负责数据库。这个微服务是不同的，微服务是按照业务逻辑拆分。 交付瓶颈共享服务另一个关键原因是，可以避免交付配瓶颈。解决共享服务的一种途径：内部开源。 内部开源守护者的角色内部采用跟标准开源项目同样的模式，分离出一组受信任的提交者。好的守护者需要花费大量的精力与提交者进行清晰的沟通，并在工作中进行引导。 成熟开源项目在第一个版本的核心代码前，往往不允许不受信任的提交者提交代码。 工具git，良好的构建和部署流水线 CICD。 小结康威定律强调了视图让系统设计跟组织结构不匹配所导致的危险。服务所有权与同地团队相匹配。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记09","slug":"2018-08-22-微服务设计笔记09","date":"2019-06-26T03:40:39.044Z","updated":"2019-06-26T03:40:39.044Z","comments":true,"path":"2019/06/26/2018-08-22-微服务设计笔记09/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-22-微服务设计笔记09/","excerpt":"","text":"微服务设计笔记09本篇笔记为微服务设计系列读书笔记的第八篇，主要记录微服务系统的安全。 身份验证与授权主体：抽象地讨论进行身份验证的人或事时。不希望每个人使用不同的用户名和密码来登陆不同的系统。目的是要有一个单一的标识且只需要一次验证。 常见的单点登录实现企业级领域中占据统治低位单SAML和Openid Connect，后者是未来的方向。 单点登录网关可以使用服务与外部世界的网关作为代理，而不是让每个服务管理者与身份提供者进行握手。基本想法：集中处理重定向用户的行文，并且只在一个地方执行握手。 解决下游服务如何接受主体信息的问题，比如用户名和角色。可以把信息放到http header上，参考工具shibboleth。 细粒度的授权网关可以提供粗粒度的身份验证。 服务间的身份验证和授权 在边界内允许一切在边界内对任何调用都是默认可信的。 HTTPS基本身份验证 使用SAML和OpenID 客户端证书 TLS HTTP之上的HMAC API秘钥 代理问题 静态数据的安全基本的需要遵循的东西： 使用众所周知的加密算法，密码需要考虑使用加盐密码哈希。 一切皆与秘钥相关。 深度防御 防火墙 日志（聚合多个系统的日志） 入侵检测和预防系统 网络隔离。把微服务放在不同的网段，控制服务间的通信。 操作系统，及时打补丁。 一个示例针对浏览器无需安全保护的内容使用标准的HTTP，以便能够缓存。对于有安全需要，登陆后才可访问的页面，所有的内容通过HTTPS传输。 黄金法则 不要实现自己的加密算法。 不要发明自己的安全协议。 自动化工具 ZAP 跨站脚本攻击 OWASP","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记08","slug":"2018-08-20-微服务设计笔记08","date":"2019-06-26T03:40:39.042Z","updated":"2019-06-26T03:40:39.042Z","comments":true,"path":"2019/06/26/2018-08-20-微服务设计笔记08/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-20-微服务设计笔记08/","excerpt":"","text":"微服务设计笔记08本篇笔记为微服务设计系列读书笔记的第八篇，主要记录微服务系统的监控。微服务：监控小的服务，然后聚合起来看整体。 单一服务，单一服务器监控的内容： CPU、内存等主机数据（Nagios） 服务器本身的日志。 单一服务，多个服务器 需要监控有关主机的数据，查看所有主机的数据，还要能查看单个主机自己的数据。 日志，多个服务器登陆查看日志，比较枯燥。 负载均衡器本身的数据信息。 多个服务，多个服务器从日志到应用程序指标，集中收集和聚合尽可能多的数据到我们手上。 日志，更多的日志 logstash 可以解析多种日志文件。 Kibana 基于ElasticSearch的查看日志的系统。 多个服务的指标跟踪CPU负债增加20%，网站每秒有50条http错误代码，这代表问题吗？ 解决方法：搜集系统指标足够长的时间，直到清晰的模式浮现。服务实例数据聚合Graphite，根据数据产生报告或仪表盘。 服务指标linux主机上安装collected并让他们指向Graphite，OS会生成大量指标。 WEB服务最低限度应该暴露响应时间和错误率指标，尽可能的多暴露。 关联标识用户的任何功能都由大量的服务配合提供，一个初始调用会触发多个下游的服务调用。实际想像栈跟踪那样，查看调用链的上游。一个方法是使用关联标识（ID）。在触发第一个调用时，生成一个GUID。然后把它传递给所有的后续调用。类似日志级别和日期，可以把关联标识以结构化的方式写入日志。 Zipkin可以跨多个系统边界跟踪调用，还有一个界面。但是需要自定义客户端并且支持收集系统，稍微重量级。可以利用已经聚合过的日志进行分析。若是使用HTTP作为通信协议，只需要包装标准的HTTP客户端库，添加代码确保在HTTP头传递关联标识即可。 小结 最低限度要跟踪请求响应时间，后续可以跟踪错误率以及应用程序级别的指标。 最低限度跟踪所有下游服务的健康指标。包括下游调用的响应时间，最好能跟踪错误率。Hystrix。 标准化如何收集指标以及存储指标。 标准格式将日志记录到一个标准的位置。 监控底层OS。 参考Graphite","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践04-熔断监控Hystrix Dashboard和Turbine","slug":"2018-08-19-微服务实践04-熔断监控Hystrix Dashboard和Turbine","date":"2019-06-26T03:40:39.039Z","updated":"2019-06-26T03:40:39.039Z","comments":true,"path":"2019/06/26/2018-08-19-微服务实践04-熔断监控Hystrix Dashboard和Turbine/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-19-微服务实践04-熔断监控Hystrix Dashboard和Turbine/","excerpt":"","text":"微服务实践04-熔断监控Hystrix Dashboard和TurbineHystrix-dashboard是一款针对Hystrix进行实时监控的工具，通过Hystrix Dashboard我们可以在直观地看到各Hystrix Command的请求响应时间, 请求成功率等数据。但是只使用Hystrix Dashboard的话, 你只能看到单个应用内的服务信息, 这明显不够. 我们需要一个工具能让我们汇总系统内多个服务的数据并显示到Hystrix Dashboard上, 这个工具就是Turbine。 Hystrix Dashboard在熔断示例项目eureka-consumer的基础上更改，重新命名为：eureka-consumer-hystrix-dashboard。 添加依赖12345678910111213 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 启动类1234567891011@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients@EnableHystrixDashboard@EnableCircuitBreakerpublic class EurekaConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaConsumerApplication.class, args); &#125;&#125; 解决404异常actuator/hystrix.stream 打开后404错误，需要新建bootstrap.yml内容如下，具体解决方法见参考文章。12345management: endpoints: web: exposure: include: &apos;*&apos; 测试访问http://localhost:9101/actuator/hystrix.stream,页面会出现如下所示ping：123ping: data: &#123;&quot;type&quot;:&quot;HystrixCommand&quot;,&quot;name&quot;:&quot;HelloRemote#hello(String)&quot;,&quot;group&quot;:&quot;spring-cl 然后进入页面http://localhost:9101/hystrix/，在页面中的input输入框中，输入http://localhost:9101/actuator/hystrix.stream，相当于将此链接的返回json数据图形化，见下图：Hystrix Dashboard Wiki上详细说明了图上每个指标的含义，如下图：到此单个应用的熔断监控已经完成。 Turbine复杂的分布式系统中，相同服务的节点经常需要部署上百甚至上千个，很多时候，运维人员希望能够把相同服务的节点状态以一个整体集群的形式展现出来，这样可以更好的把握整个系统的状态。 为此，Netflix提供了一个开源项目（Turbine）来提供把多个hystrix.stream的内容聚合为一个数据源供Dashboard展示。 pom文件新增依赖如下：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-turbine&lt;/artifactId&gt; &lt;/dependency&gt; 配置文件1234567spring.application.name=hystrix-dashboard-turbineserver.port=9101eureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/feign.hystrix.enabled=trueturbine.appConfig=node01,node02turbine.aggregator.clusterConfig= defaultturbine.clusterNameExpression= new String(&quot;default&quot;) 启动类添加EnableTurbine123456789101112@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients@EnableHystrixDashboard@EnableCircuitBreaker@EnableTurbinepublic class EurekaConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaConsumerApplication.class, args); &#125;&#125; 新建两个node消费者node1配置文件1234spring.application.name=node01server.port=9001feign.hystrix.enabled=trueeureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/ node2配置文件1234spring.application.name=node02server.port=9002feign.hystrix.enabled=trueeureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/ 测试依次启动eureka-server ,turbine,node1-consumer,node2-consumer。访问页面http://localhost:9100/turbine.stream，可以看到不停的ping123: ping: pingdata: &#123;&quot;reportingHostsLast10Seconds&quot;:0,&quot;name&quot;:&quot;meta&quot;,&quot;type&quot;:&quot;meta&quot;,&quot;timestamp&quot;:1534662688727&#125; 且会不断刷新以获取实时的监控数据，说明和单个的监控类似，返回监控项目的信息。进行图形化监控查看，输入：http://localhost:9100/hystrix，输入： http://localhost:9100/turbine.stream，然后点击 Monitor Stream ,可以看到出现了俩个监控列表。 参考springcloud(五)：熔断监控Hystrix Dashboard和Turbinespring-cloud-netflixhystrix-dashboard 报错 /actuator/hystrix.stream 404 Not Found本文github地址","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践03-熔断器histrix","slug":"2018-08-18-微服务实践03-熔断器Hystrix","date":"2019-06-26T03:40:39.036Z","updated":"2019-06-26T03:40:39.036Z","comments":true,"path":"2019/06/26/2018-08-18-微服务实践03-熔断器Hystrix/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-18-微服务实践03-熔断器Hystrix/","excerpt":"","text":"微服务实践03-熔断器histrix雪崩效应在微服务架构中通常会有多个服务层调用，基础服务的故障可能会导致级联故障，进而造成整个系统不可用的情况，这种现象被称为服务雪崩效应。服务雪崩效应是一种因“服务提供者”的不可用导致“服务消费者”的不可用,并将不可用逐渐放大的过程。如果下图所示：A作为服务提供者，B为A的服务消费者，C和D是B的服务消费者。A不可用引起了B的不可用，并将不可用像滚雪球一样放大到C和D时，雪崩效应就形成了。 熔断器熔断器的原理很简单，如同电力过载保护器。它可以实现快速失败，如果它在一段时间内侦测到许多类似的错误，会强迫其以后的多个调用快速失败，不再访问远程服务器，从而防止应用程序不断地尝试执行可能会失败的操作，使得应用程序继续执行而不用等待修正错误，或者浪费CPU时间去等到长时间的超时产生。熔断器也可以使应用程序能够诊断错误是否已经修正，如果已经修正，应用程序会再次尝试调用操作。 熔断器模式就像是那些容易导致错误的操作的一种代理。这种代理能够记录最近调用发生错误的次数，然后决定使用允许操作继续，或者立即返回错误。 熔断器开关相互转换的逻辑如下图：熔断器就是保护服务高可用的最后一道防线。 Hystrix特性断路器机制断路器很好理解, 当Hystrix Command请求后端服务失败数量超过一定比例(默认50%), 断路器会切换到开路状态(Open). 这时所有请求会直接失败而不会发送到后端服务. 断路器保持在开路状态一段时间后(默认5秒), 自动切换到半开路状态(HALF-OPEN). 这时会判断下一次请求的返回情况, 如果请求成功, 断路器切回闭路状态(CLOSED), 否则重新切换到开路状态(OPEN). Hystrix的断路器就像我们家庭电路中的保险丝, 一旦后端服务不可用, 断路器会直接切断请求链, 避免发送大量无效请求影响系统吞吐量, 并且断路器有自我检测并恢复的能力. FallbackFallback相当于是降级操作. 对于查询操作, 我们可以实现一个fallback方法, 当请求后端服务出现异常的时候, 可以使用fallback方法返回的值. fallback方法的返回值一般是设置的默认值或者来自缓存. 资源隔离在Hystrix中, 主要通过线程池来实现资源隔离. 通常在使用的时候我们会根据调用的远程服务划分出多个线程池. 例如调用产品服务的Command放入A线程池, 调用账户服务的Command放入B线程池. 这样做的主要优点是运行环境被隔离开了. 这样就算调用服务的代码存在bug或者由于其他原因导致自己所在线程池被耗尽时, 不会对系统的其他服务造成影响. 但是带来的代价就是维护多个线程池会对系统带来额外的性能开销. 如果是对性能有严格要求而且确信自己调用服务的客户端代码不会出问题的话, 可以使用Hystrix的信号模式(Semaphores)来隔离资源. Feign Hystrix因为熔断只是作用在服务调用这一端，因此我们根据上一篇的示例代码只需要改动spring-cloud-consumer项目相关代码就可以。因为，Feign中已经依赖了Hystrix所以在maven配置上不用做任何改动。 配置文件application.properties 增加以下代码1feign.hystrix.enabled=true 创建回调类创建HelloRemoteHystrix类继承与HelloRemote实现回调的方法12345678@Componentpublic class HelloRemoteHystrix implements HelloRemote &#123; @Override public String hello(@RequestParam(value = &quot;name&quot;) String name) &#123; return &quot;hello&quot; +name+&quot;, this messge send failed &quot;; &#125;&#125; 添加fallback属性在HelloRemote类添加指定fallback类，在服务熔断的时候返回fallback类中的内容。12345@FeignClient(name= &quot;spring-cloud-producer&quot;,fallback = HelloRemoteHystrix.class)public interface HelloRemote &#123; @RequestMapping(value = &quot;/hello&quot;) public String hello(@RequestParam(value = &quot;name&quot;) String name);&#125; 测试一次运行eureka-server ,producer,consumer,输入地址http://127.0.0.1:9101/hello/duming,运行结果为hello duming，this is first messge。此时将producer关闭，然后再次请求，结果显示helloduming, this messge send failed。这时候表明熔断生效。 参考资料springcloud(四)：熔断器Hystrix本文代码github","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践02-eureka服务提供者和消费者","slug":"2018-08-18-微服务实践02-eureka服务提供者和消费者","date":"2019-06-26T03:40:39.034Z","updated":"2019-06-26T03:40:39.034Z","comments":true,"path":"2019/06/26/2018-08-18-微服务实践02-eureka服务提供者和消费者/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-18-微服务实践02-eureka服务提供者和消费者/","excerpt":"","text":"微服务实践02-eureka服务提供者和消费者上一篇文章介绍了eureka服务注册中心的搭建，这篇文章介绍一下如何使用eureka服务注册中心，搭建一个简单的服务端注册服务，客户端去调用服务使用的案例。 案例中有三个角色：服务注册中心、服务提供者、服务消费者，其中服务注册中心就是我们上一篇的eureka cluster版启动既可，流程是首先启动注册中心，服务提供者生产服务并注册到服务中心中，消费者从服务中心中获取服务并执行。 服务提供者假设服务提供者有一个hello方法，可以根据传入的参数，提供输出“hello xxx，this is first messge”的服务。 pom包配置123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件123spring.application.name=spring-cloud-producerserver.port=9000eureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/,http://localhost:8001/eureka/ 启动类12345678@SpringBootApplication@EnableDiscoveryClientpublic class ProducerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProducerApplication.class, args); &#125;&#125; controller类12345678@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String index(@RequestParam String name) &#123; return &quot;hello &quot;+name+&quot;，this is first messge&quot;; &#125;&#125; 打包运行打包后直接运行jar文件，然后进入服务中心页面如下所示。 服务消费者pom文件123456789101112131415161718192021222324&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 配置文件123spring.application.name=spring-cloud-consumerserver.port=9101eureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/ 启动类123456789@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class EurekaConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaConsumerApplication.class, args); &#125;&#125; FeinClinet定义接口12345@FeignClient(name= &quot;spring-cloud-producer&quot;)public interface HelloRemote &#123; @RequestMapping(value = &quot;/hello&quot;) public String hello(@RequestParam(value = &quot;name&quot;) String name);&#125; FeignClient 由spring封装，比较方便的使用rest http client。 controller123456789101112@RestControllerpublic class ConsumerController &#123; @Autowired HelloRemote helloRemote; @RequestMapping(&quot;/hello/&#123;name&#125;&quot;) public String index(@PathVariable(&quot;name&quot;) String name) &#123; return helloRemote.hello(name); &#125;&#125; 运行依次启动 server、producer、consumer。访问http://127.0.0.1:9101/hello/4344，显示如下：1hello 4344，this is first messge 负债均衡修改producer代码controller代码修改如下：first变为second12345678@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String index(@RequestParam String name) &#123; return &quot;hello &quot;+name+&quot;，this is second messge&quot;; &#125;&#125; 配置文件改变端口123spring.application.name=spring-cloud-producerserver.port=9001eureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/ 启动后注意下图，producer此时算上之前已启动的，出现了2个。 然后多次访问消费者地址http://127.0.0.1:9101/hello/4344，此时注意交替出现hello 4344，this is second messge，和hello 4344，this is first messge。这就说明在服务中心下，服务生产者自动实现了负载均衡的作用。 参考springcloud(三)：服务提供与调用本文github地址-consumer本文github地址-producer","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务实践01-服务中心eureka","slug":"2018-08-18-微服务实践01-服务中心eureka","date":"2019-06-26T03:40:39.031Z","updated":"2019-06-26T03:40:39.031Z","comments":true,"path":"2019/06/26/2018-08-18-微服务实践01-服务中心eureka/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-18-微服务实践01-服务中心eureka/","excerpt":"","text":"微服务实践01-服务中心eureka服务中心背景服务中心又称注册中心，管理各种服务功能包括服务的注册、发现、熔断、负载、降级等，比如dubbo admin后台的各种功能。如上图所示，各种服务都注册到了服务中心，这样就为以下的高级功能创造了条件。 可以为几个相同的服务做均衡负载； 监控服务器调用成功率来做熔断，移除服务列表中的故障点； 监控服务调用时间，对不同的服务器设置不同的权重等等。 Eurekaspring cloud封装了Netflix的eureka框架来做服务的注册和发现。Eureka 采用了 C-S 的设计架构。Eureka Server作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用 Eureka 的客户端连接到 Eureka Server，并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。Spring Cloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。 Eureka由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。Netflix在其生产环境中使用的是另外的客户端，它提供基于流量、资源利用率以及出错状态的加权负载均衡。 上图简要描述了Eureka的基本架构，由3个角色组成： Eureka Server提供服务注册和发现 Service Provider服务提供方，将自身服务注册到Eureka，从而使服务消费方能够找到 Service Consumer 服务消费方，从Eureka获取注册服务列表，从而能够消费服务 项目实践添加POM依赖1234567891011121314151617181920212223242526&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Finchley.SR1&lt;/spring-cloud.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启动代码中添加@EnableEurekaServer注解12345678@SpringBootApplication@EnableEurekaServerpublic class SpringcloudEurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringcloudEurekaApplication.class, args); &#125;&#125; eureka集群使用application.yml内容如下：123456789101112131415161718192021222324252627282930313233343536373839---spring: application: name: spring-cloud-eureka profiles: peer1server: port: 8000eureka: instance: hostname: peer1 client: serviceUrl: defaultZone: http://peer2:8001/eureka/,http://peer3:8002/eureka/---spring: application: name: spring-cloud-eureka profiles: peer2server: port: 8001eureka: instance: hostname: peer2 client: serviceUrl: defaultZone: http://peer1:8000/eureka/,http://peer3:8002/eureka/---spring: application: name: spring-cloud-eureka profiles: peer3server: port: 8002eureka: instance: hostname: peer3 client: serviceUrl: defaultZone: http://peer1:8000/eureka/,http://peer2:8001/eureka/ 打开hosts文件添加如下内容：123127.0.0.1 peer1127.0.0.1 peer2127.0.0.1 peer3 对项目使用maven打包，分别以peer1、peer2、peer3的配置参数启动eureka注册中心。123java -jar eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=peer1java -jar eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=peer2java -jar eureka-0.0.1-SNAPSHOT.jar --spring.profiles.active=peer3 运行后打开peer1:8000，可以看到DS Replicas，registered-replicas，available-replicas。 参考springcloud(二)：注册中心Eureka本文github地址-producer","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记07","slug":"2018-08-17-微服务设计笔记07","date":"2019-06-26T03:40:39.029Z","updated":"2019-06-26T03:40:39.029Z","comments":true,"path":"2019/06/26/2018-08-17-微服务设计笔记07/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-17-微服务设计笔记07/","excerpt":"","text":"微服务设计笔记07本篇笔记为微服务设计系列读书笔记的第七篇，主要记录微服务系统的测试。 测试类型首先引入测试象限的概念，参见下图。注意图中下方，面向技术的测试，这些测试都可以使用自动工具完成。 测试范围 单元测试单元测试通常只测试一个函数或者方法调用。单元测试彼此独立，分别覆盖一些小范围的代码。 服务测试绕开用户界面，直接针对服务的测试。多个服务的系统，一个服务测试只测试一个单独的服务的功能。需要给所有的外部合作者打桩。 端到端测试端到端测试会覆盖整个系统。 测试比例从测试金字塔向上，测试覆盖的范围越来越大，但是测试要花的时间，测试定位问题的难度也越来越大。顺着测试金字塔，从金字塔向下，下面一层的测试数量要不上面一层多一个数量级。 实现服务测试作者推荐了Mountebank mock服务器。 脆弱的测试有些测试失败不是因为功能失败，比如多线程之间的竞争，临时网络故障。脆弱测试的定义：包含在测试中的服务数量越多，测试就会越脆弱，不确定性就会越强。如果测试失败后每个人都只想重新运行一遍测试，然后希望可能会通过，那么这种测试就是脆弱的。 测试场景把测试系统的重心放到少量核心的场景上来。把任何在这些核心场景之外的功能放在相互隔离的服务测试中覆盖。 部署后再测试 蓝绿部署前提条件：能够切换生产流量到不同的主机，最好能够零宕机部署。 金丝雀发布金丝雀发布：将部分生产流量引流到新部署的系统，来验证系统是否按预期执行。金丝雀发布与蓝绿发布的不太之处：新旧版本共存时间更长，而且经常会调整流量。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记06","slug":"2018-08-14-微服务设计笔记06","date":"2019-06-26T03:40:39.026Z","updated":"2019-06-26T03:40:39.026Z","comments":true,"path":"2019/06/26/2018-08-14-微服务设计笔记06/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-14-微服务设计笔记06/","excerpt":"","text":"微服务设计笔记06本篇笔记为微服务设计系列读书笔记的第六篇，主要记录微服务系统的部署。 持续集成简介CI(Continuous Integration持续集成)。CI能够保证新提交的代码与已有代码进行集成，从而让所有人保持同步。CI服务器会检测到代码已提交并签出，并能验证代码是否通过编译以及测试是否通过。判断是否真正理解CI三个问题： 是否每天签入代码到主线。需要频繁把代码捡入到单个主线分支中。 需要有一组测试数据用来验证修改。没有对代码进行验证的CI不是真正的CI。 构建失败后，团队是否把修复CI当作第一优先级的事情来做。 把持续集成映射到微服务如何在微服务、CI构建、以及源代码之间建立合适的映射？建议采用下图所示的方式，每个微服务有一个单独的代码库，并分别于相应的CI绑定。这样就可以在将该服务部署到生产环境之前有一个快速的验证。每个微服务相关的测试也应该和其本身的代码放在一起。 构建流水线和持续交付构建流水线：（1）第一个阶段运行快速测试（2）第二个阶段运行耗时测试。CD（continous delivery）CD检查每次提交是否满足部署到生产环境的要求。 服务与主机之间的映射单个主机上部署多个服务，增加了对单个服务进行扩展的复杂性。应用程序容器缺点： 限制技术栈的选择，限制自动化和系统管理技术的选择。 多个程序在一个同一个进程中，分析资源的使用和线程也非常复杂。每个主机一个服务：简化问题排查。推荐使用Docker。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"穷查理宝典语句摘抄","slug":"2018-08-13-穷查理宝典语句摘抄","date":"2019-06-26T03:40:39.024Z","updated":"2019-06-26T03:40:39.024Z","comments":true,"path":"2019/06/26/2018-08-13-穷查理宝典语句摘抄/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-13-穷查理宝典语句摘抄/","excerpt":"","text":"穷查理宝典语句摘抄 绝在占地一层的巨型办公室里，见面要层层通报，过五关斩六将，谁都不能轻易接触到。这 样就与现实生活脱节了 迅速歼灭不该做的事情，接着对该做的事情发起熟练的、跨学科的攻击，然后， 当合适的机会来临——只有当合适的机会来临——就采取果断的行动。 （购买）股价公道的伟 大企业比（购买）股价超低的普通企业好。 《枪炮、病菌与钢铁》《自私的基因》《冰河世纪》和《达尔文的盲点》都有关注前面提到的“竞争性毁灭”问题，研究为什么有些事物能够适应环境， 存活下来，甚至在经过很长的时间之后占据统治地位。 测算合适的安全边际 如果你因为一样东西的 价值被低估而购买了它，那么当它的价格上涨到你预期的水平时，你就必须考虑把它卖掉。 那很难。但是，如果你能购买几个伟大的公司，那么你就可以安坐下来啦。那是很好的事情。我们偏向于把大量的钱投放在我们不用再另外作决策的地方。如果你因为一样东西的 价值被低估而购买了它，那么当它的价格上涨到你预期的水平时，你就必须考虑把它卖掉。 那很难。但是，如果你能购买几个伟大的公司，那么你就可以安坐下来啦。那是很好的事情。 听取他们的教导，人们将能够更加深刻地理解人类的本性、世界的现状、如何理性思考，以 及最重要的，如何更好地过上一种正直、幸福、善良的生活（提示：这三种要素是相辅相成我们成功的关键 优秀企业是什么？ 有这么两类企业：第一类每年赚12%，你到年底可以把利润拿走。第二类每年赚 12%， 但所有多余的现金必须进行再投资——它总是没有分红。 这样的错误可以分为两类：1.什么也不做，沃伦称之为“吮吸我的大拇指”；2.有些股票本来应该买很多，但是只买了一点。 当股价下跌时买进 过去几十年来，我们经常这么做：如果某家我们喜欢的企业的股票下跌，我们会买进更多。有时候会出现一些情况，你意识到你错了，那么就退出好了。但如果你从自己的判断中发展出了正确的自信，那么就趁价格便宜多买一些吧。 拥有常识不但意味着有能力辨认智慧，也意味着有能力拒绝愚蠢。如果排除了许多事情，你就不会把自己搞得一团糟。 我们看很多书。我认识的聪明人没有不看很多书的。但光看书还不够：你必须拥有一种 能够掌握思想和做合理事情的性格。 你拥有的基本知识越多，你需要吸取的新知识就越少。 如果你在生活中惟一的成功就是通过买卖股票发财，那么这是一种失败的生活。生活不仅 仅是精明地积累财富。 生活和生意上的大多数成功来自于你知道应该避免哪些事情：过早死亡、糟糕的婚姻等。避免染上艾滋病、在路口和火车抢道以及吸毒。 培养良好的心理习惯。 避免邪恶之人，尤其是那些性感诱人的异性。 他总是必须成为最好的，无法容忍其他人在这些投资领域击败他。 索罗斯无法忍受其他人从科技产业赚钱而自己没有赚到，他亏得一塌糊涂。我们根本就 不在意（别人在科技产业赚了钱） 关注别人赚钱（比你）更快的想法是一种致命的罪行。 妒忌真的是一种愚蠢的罪行， 每天起床的时候，争取变得比你从前更聪明一点。认真地、出色地完成你的任务。慢慢 地，你会有所进步，但这种进步不一定很快。但你这样能够为快速进步打好基础 每天慢 慢向前挪一点。到最后——如果你足够长寿的话——大多数人得到了他们应得的东西。 我觉得你要是想让人们认识有用的伟大概念，最好是将这些概念和提出它们的伟人的生活和个性联系起来。我想你要是能够和亚当·斯密交朋友，那你的经济学肯定可以学得更好。和“已逝的伟人”交朋友，这听起来很好玩。 避免在生活中拥有大量愚蠢的需求——你不需要很多物质的商品。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"微服务设计笔记05","slug":"2018-08-09-微服务设计笔记05","date":"2019-06-26T03:40:39.021Z","updated":"2019-06-26T03:40:39.022Z","comments":true,"path":"2019/06/26/2018-08-09-微服务设计笔记05/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-09-微服务设计笔记05/","excerpt":"","text":"微服务设计笔记05本篇笔记为微服务设计系列读书笔记的第五篇，主要记录分解单块系统。 关键是接缝从接缝处可以抽取出相对独立的一部分代码，对这部分代码的修改不会影响系统的其他部分。限界上下文就是一个非常好的接缝。某单块系统的实例： 产品目录 财务 仓库 推荐分解单块系统的原因增量方式分解单块系统比较合适。指导因素： 改变的速度。接缝抽出来某个功能，成为一个单独的服务，后期开发速度大大加快。 团队结构。不同团队维护的代码分离出来。 安全。特定服务进行监控，传输数据的保护和静态数据的保护。 技术。 杂乱的依赖。单独拉取出来的接缝应该尽量少地被其他组件所依赖。若几个接缝之间可以形成一个有向无环图，就能够识别哪些接缝比较难处理。 数据库是所有依赖的源头。 打破外键关系上图中两个微服务之间共享了数据表。上图财务模块通过访问产品模块API的方式，避免了直接数据访问的缺陷。主外键关系需要由数据库层面实现，变为代码中实现。 数据共享 共享静态数据，比如国家代码，可以放入配置文件或者代码中。 共享可变数据。多个微服务向同一个表写入数据。比如财务服务和仓库服务都会向客户表中写入数据，于是这里缺失的领域概念是客户。需要将抽象的客户概念具象化，形成一个清晰的客户服务。 数据库重构想要在一次发布中把单块服务直接变成两个服务，并且每个服务有各自的数据库结构，事实上会推荐先分离数据库结构。 事务边界下单后更新订单表的同时，也应该调用仓库服务通知派发订单。跨事务边界的操作，见下图。 最终一致性知道订单被捕获处理就足够了，后面再对仓库的提取表做一次插入操作。可以把这部分操作放在一个队列，之后再进行触发。 终止整个操作利用补偿事务去抵消之前的操作，但是需要考虑补偿事务失败的情况。比如采用后台定时任务清除，提供界面供维护人员处理等。 分布式事务使用一个叫作事务管理器的工具来统一编配其他底层系统中运行的事务。分布式事务常用算法：两阶段提交。 投票阶段，每个参与者会告诉事务管理器它是否应该继续，如果事务管理器收到的所有投票都是成功，则会告知他们进行提交操作。收到一个否定的投票，事务管理器就会让所有的参与者回退。上述过程中，会假定认为事务管理器不会发生故障，消息发送也肯定能成功。 综上最终一致性较为容易，分布式事务实现复杂性较大。 报表数据服务调用获取数据展示过去15分钟内下的订单数量，这种通过API调用实现。但是访问大数据量的数据，这种方法就失效了。 ODS数据导出把各个微服务的数据，统一推送到一个统一的数据库中。这种是前面数据库集成的特例。 参考《微服务设计》","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记04","slug":"2018-08-08-微服务设计笔记04","date":"2019-06-26T03:40:39.019Z","updated":"2019-06-26T03:40:39.019Z","comments":true,"path":"2019/06/26/2018-08-08-微服务设计笔记04/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-08-微服务设计笔记04/","excerpt":"","text":"微服务设计笔记04本篇笔记为微服务设计系列读书笔记的第四篇，主要记录微服务集成。 寻找理想的集成技术SOAP，XMLRPC，REST，Protocol Buffers？ 避免破坏性修改对某个服务的一些修改导致服务消费方也随之发生改变，应该着力避免此种情况。 保证API的技术无关性新技术、新工具层出不穷，这样我们的工作更高效。所以保证微服务之间通信方式的技术无关性非常重要。 使你的服务易于消费方使用理想情况下，消费方可以使用任何技术实现。 隐藏内部实现细节不希望消费方与服务的内部实现细节绑定在一起，因为这会增加耦合。所有倾向于暴露内部实现细节的技术都不应该被采用。 数据库集成其他服务想从一个服务获取信息，可以直接访问数据库。非常直接，所以是最快的集成方式。缺点如下： 外部系统能够查看实现细节，并与其绑定。如果修改表结构，则会影响其他服务。 消费方与特定的技术绑定在一起，比如使用一段时间的关系型数据库后，发现NOSQL才是更合适的。但是因为消费方与客户服务紧密地绑定在了一起，所以不能轻易替换。 对数据库操作的服务有很多，如果修复一个bug，可能会修复多个地方。 同步与异步同步：发起一个远程服务调用后，调用方会阻塞自己并等待整个操作完成。异步：调用方不需要等待操作完成就可以返回，甚至可能不需要关系这个操作是否完成。 基于事件协作方式客户端不是发起请求，二是发布一个事件，期待其他的协作者接收消息。基于事件的系统天生就是异步的。整个系统都很聪明，业务逻辑并非集中存在于某个核心大脑，二是平均分布在不同的协作者中，所以耦合性低。一些跨服务边界的流程，这些流程可能会运行很长时间，所以会涉及到同步和异步的选择。 编排与协同编排实例所谓编排(orchestration)，我们会依赖某个中心大脑来指导并驱动整个流程。上述流程编排的做法是：让客户服务作为中心大脑，创建客户时，他会跟积分账户、电子邮件服务、邮政服务通过请求响应的方式进行通信。编排方式的缺点：客户服务作为中心控制点承担了太多的职责。其他与他交互的服务变为贫血的，基于CRUD的服务。 协同实例从客户服务中使用异步的方式触发一个事件。电子邮件服务、邮政服务、积分账户简单订阅这些事件并做相应处理。这就意味着需要一些额外的工作监控流程，以保证正确执行。协同方式耦合性小，而且对系统的改动更加灵活，所以建议使用协同方式。针对请求/响应方式，可以考虑使用两种技术：RPC和REST。 远程过程调用远程调用允许进行一个本地调用，但是实际上结果是某个远程服务器产生的。比如SOAP,Thrift，protocl buffers。技术核心特点：使用本地调用的方式和远程进行交互。JAVA RMI ，Thrift,protocol buffers 使用二进制，而SOAP使用XML作为消息格式。RPC实现会自动实现服务端和客户端的桩代码，基本不用花时间，就可以在服务之间进行内容交互了。但是RPC存在以下缺点： 技术的耦合RMI与特定的平台紧密绑定，对服务端和客户端的技术选型造成一定的限制(RMI的服务端和客户端都绑定在JVM上了)。Thrfits和protocol buffers对不同语言支持较好。 本地调用和远程调用并不相同RPC隐藏了远程调用的复杂性，RPC会花大量时间对负荷进行封装和解封装，还有网络通信需要的时间。网络是不可靠的。 脆弱性针对JAVA RMI若是修改远程调用接口或者修改实例对象的字段，则服务端和客户端都必须修改，即使修改的实例对象的字段生产环境不使用。 RPC 很糟糕吗JAVA RMI 避免使用。但是一些更现代的RPC机制，protocol buffers Thrift可以使用。 不要对远程调用过度抽象，以至于网络因素被隐藏起来。 可以独立升级服务端，而不是强迫客户端升级。 相对数据库集成，RPC是一个巨大的进步。RESTREST 并不限制底层协议，事实上最常用HTTP。 RESTHTTP本身提供很多功能，功能对REST风格非常有用。JSON、XML还是其他。有些框架直接将数据库对象反序列化成进程内的对象，然后直接暴露给外部，这样的设计耦合性较高。 实现基于事件的异步协作方式技术选择 RabbitMQ消息代理，尽量让中间件保持简单，业务逻辑放在自己的服务中。 ATOM HTTP传播事件。 异步架构的复杂性短生命周期的异步操作比较容易管理。设计最大重试次数和消息医院（死信队列）。 微服务中的DRY和代码重用的风险DRY: DoNot Repeat yourself，避免重复代码的意思。在微服务内部不要违反DRY，但是在跨服务的情况下可以适当违反DRY。 版本管理服务接口改变，如何管理这些改变。 尽可能推迟或不做这种修改。 及早发现破坏性修改。 使用语义化的版本管理。MAJOR MINOR PATCH。MAJOR改变意味着其中包含向后不兼容的修改。MINOR 有新功能产生，但是向后兼容。PATCH对已有功能的缺陷修复。 不同的接口共存。在同一个服务上，新老接口同时存在。一旦消费者不再访问老接口，则可以删除老接口。内部对所有V1的请求，转换为请求V2，使用这种方式，删除哪些代码会比较清楚。可以在请求中添加版本信息。/v1/customer ，/v2/customer。 用户界面 走向数字化。 约束。PC 手机 平板使用方式不同。移动情况下网络不同。 API组合。gateway(API入口)多个底层调用会被聚合成为一个调用。 UI片段的组合。服务直接暴露出一部分UI，简单把这部分UI组合在一起就可以创建出整体UI。 为前端服务的后端。与后端交互比较频繁的界面或者需要给不同设备提供不同内容的界面，常见解决方案，使用服务端的聚合接口或API入口。这样会得到一个聚合所有服务的巨大的层。所有东西放在一起，失去了不同用户界面之间的隔离性，限制了彼此独立发布的能力。作者推荐的模式如下： 这种模式有时也叫做BFF(Backends for frontends)。 小结 无论如何避免数据库集成 理解REST和RPC之间的取舍。但是总是使用REST作为请求/响应模式的起点。 相比编排，优先选择协同。 避免破坏性修改，理解POstel 使用容错性读取器。 将用户界面看做一个组合层。 参考《微服务设计》","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"Promise学习04","slug":"2018-08-04-promise学习04","date":"2019-06-26T03:40:39.017Z","updated":"2019-06-26T03:40:39.017Z","comments":true,"path":"2019/06/26/2018-08-04-promise学习04/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-04-promise学习04/","excerpt":"","text":"Promise学习04本文章主要记录Mocha测试框架与Promise配合的情况。 使用done的Promise测试1234567891011121314151617181920var assert = require(&apos;power-assert&apos;);describe(&apos;Basic Test&apos;, function () &#123; context(&apos;When Callback(high-order function)&apos;, function () &#123; it(&apos;should use `done` for test&apos;, function (done) &#123; setTimeout(function () &#123; assert(true); done(); &#125;, 0); &#125;); &#125;); context(&apos;When promise object&apos;, function () &#123; it(&apos;should use `done` for test?&apos;, function (done) &#123; var promise = Promise.resolve(1); promise.then(function (value) &#123; assert(value === 1); done(); &#125;); &#125;); &#125;);&#125;); 对异常Promise测试1234567it(&quot;should use `done` for test?&quot;, function(done) &#123; var promise = Promise.resolve(); promise.then(function(value) &#123; assert(false); // =&gt; throw AssertionError done(); &#125;); &#125;); 错误信息如下：1Error: Timeout of 2000ms exceeded. For async tests and hooks, ensure &quot;done()&quot; is called; if returning a Promise, ensure it resolves. (D:\\Workspaces\\promise\\promiseTest.js) 上述代码抛出异常后，done并没有被执行。为了保证done一定被执行，修改代码为：1234567it(&quot;should use `done` for test?&quot;, function(done) &#123; var promise = Promise.resolve(); promise.then(function(value) &#123; throw new Error(); done(); &#125;).catch(done, done); &#125;); mocha对Promise的支持在对Promise进行测试的时候，不使用 done() 这样的回调风格的代码编写方式，而是返回一个promise对象。123456789var assert = require(&apos;power-assert&apos;);describe(&apos;Promise Test&apos;, function () &#123; it(&apos;should return a promise object&apos;, function () &#123; var promise = Promise.resolve(1); return promise.then(function (value) &#123; assert(value === 1); &#125;); &#125;);&#125;); 上述代码主要特点： 删除了done 返回结果为Promise对象这样就从本质上避免了.then(done,done) 编写可控测试代码可控测试定义待测试的promise对象 如果编写预期为Fulfilled状态的测试的话 Rejected的时候要 Fail assertion 的结果不一致的时候要 Fail 如果预期为Rejected状态的话 结果为Fulfilled 测试为 Fail assertion 的结果不一致的时候要 Fail综上一个测试用例应该包括下面的测试内容。 结果满足 Fulfilled or Rejected 之一 对传递给assertion的值进行检查 helper函数shouldRejected函数1234567891011121314151617function shouldRejected(promise) &#123; return &#123; &apos;catch&apos;: function(fn) &#123; return promise.then(function() &#123; throw new Error(&apos;Expected promise to be rejected but it was fulfilled&apos;); &#125;, function(reason) &#123; fn.call(promise, reason); &#125;); &#125; &#125;;&#125;it(&apos;should be rejected&apos;, function() &#123; var promise = Promise.reject(new Error(&apos;human error&apos;)); return shouldRejected(promise).catch(function(error) &#123; console.log(error.message === &apos;human error&apos;); &#125;);&#125;); 在 shouldRejected 外部，都是类似如下、和普通的promise处理大同小异的代码。 将需要测试的promise对象传递给 shouldRejected 方法 在返回的对象的 catch 方法中编写进行onRejected处理的代码 在onRejected里使用assertion进行判断 在使用 shouldRejected 函数的时候，如果是 Fulfilled 被调用了的话，则会throw一个异常，测试也会失败。 shouldFulfilled函数123456789101112131415161718var assert = require(&apos;power-assert&apos;);function shouldFulfilled(promise) &#123; return &#123; &apos;then&apos;: function (fn) &#123; return promise.then(function (value) &#123; fn.call(promise, value); &#125;, function (reason) &#123; throw reason; &#125;); &#125; &#125;;&#125;it(&apos;should be fulfilled&apos;, function () &#123; var promise = Promise.resolve(&apos;value&apos;); return shouldFulfilled(promise).then(function (value) &#123; assert(value === &apos;value&apos;); &#125;);&#125;); 这和上面的 shouldRejected-test.js 结构基本相同，只不过返回对象的 catch 方法变为了 then ，promise.then的两个参数也调换了。 参考promise教程","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"Promise学习03","slug":"2018-08-04-promise学习03","date":"2019-06-26T03:40:39.014Z","updated":"2019-06-26T03:40:39.014Z","comments":true,"path":"2019/06/26/2018-08-04-promise学习03/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-04-promise学习03/","excerpt":"","text":"promise学习03Promise.allPromise.all 接收一个 promise对象的数组作为参数，当这个数组里的所有promise对象全部变为resolve或reject状态的时候，它才会去调用 .then 方法。12345678910111213141516171819202122232425262728293031323334function getURL(URL) &#123; return new Promise(function (resolve, reject) &#123; var req = new XMLHttpRequest(); req.open(&apos;GET&apos;, URL, true); req.onload = function () &#123; if (req.status === 200) &#123; resolve(req.responseText); &#125; else &#123; reject(new Error(req.statusText)); &#125; &#125;; req.onerror = function () &#123; reject(new Error(req.statusText)); &#125;; req.send(); &#125;);&#125;var request = &#123; comment: function getComment() &#123; return getURL(&apos;http://azu.github.io/promises-book/json/comment.json&apos;).then(JSON.parse); &#125;, people: function getPeople() &#123; return getURL(&apos;http://azu.github.io/promises-book/json/people.json&apos;).then(JSON.parse); &#125; &#125;;function main() &#123; return Promise.all([request.comment(), request.people()]);&#125;// 运行示例main().then(function (value) &#123; console.log(value);&#125;).catch(function(error)&#123; console.log(error);&#125;); Promise.all 接收 promise对象组成的数组作为参数。在上面的代码中，request.comment() 和 request.people() 会同时开始执行，而且每个promise的结果（resolve或reject时传递的参数值），和传递给 Promise.all 的promise数组的顺序是一致的。 也就是说，这时候 .then 得到的promise数组的执行结果的顺序是固定的，即 [comment, people]。 123main().then(function (results) &#123; console.log(results); // 按照[comment, people]的顺序&#125;); 检验promise代码是否同时执行实例如下：1234567891011121314151617181920// `delay`毫秒后执行resolvefunction timerPromisefy(delay) &#123; return new Promise(function (resolve) &#123; setTimeout(function () &#123; resolve(delay); &#125;, delay); &#125;);&#125;var startDate = Date.now();// 所有promise变为resolve后程序退出Promise.all([ timerPromisefy(1), timerPromisefy(32), timerPromisefy(64), timerPromisefy(128)]).then(function (values) &#123; console.log(Date.now() - startDate + &apos;ms&apos;); // 約128ms console.log(values); // [1,32,64,128]&#125;); 从上述结果可以看出，传递给 Promise.all 的promise并不是一个个的顺序执行的，而是同时开始、并行执行的。 Promise.racePromise.all 在接收到的所有的对象promise都变为 FulFilled 或者 Rejected 状态之后才会继续进行后面的处理(是指执行then函数)， 与之相对的是 Promise.race 只要有一个promise对象进入 FulFilled 或者 Rejected 状态的话，就会继续进行后面的处理。实例如下：1234567891011121314151617// `delay`毫秒后执行resolvefunction timerPromisefy(delay) &#123; return new Promise(function (resolve) &#123; setTimeout(function () &#123; resolve(delay); &#125;, delay); &#125;);&#125;// 任何一个promise变为resolve或reject 的话程序就停止运行Promise.race([ timerPromisefy(1), timerPromisefy(32), timerPromisefy(64), timerPromisefy(128)]).then(function (value) &#123; console.log(value); // =&gt; 1&#125;); 下面我们再来看看在第一个promise对象变为确定（FulFilled）状态后，它之后的promise对象是否还在继续运行。12345678910111213141516171819var winnerPromise = new Promise(function (resolve) &#123; setTimeout(function () &#123; console.log(&apos;this is winner&apos;); resolve(&apos;this is winner&apos;); &#125;, 4); &#125;);var loserPromise = new Promise(function (resolve) &#123; setTimeout(function () &#123; console.log(&apos;this is loser&apos;); resolve(&apos;this is loser&apos;); &#125;, 1000); &#125;);// 第一个promise变为resolve后程序停止Promise.race([winnerPromise, loserPromise]).then(function (value) &#123; console.log(value); // =&gt; &apos;this is winner&apos;&#125;);this is winnerthis is winnerthis is loser Promise.race 在第一个promise对象变为Fulfilled之后，并不会取消其他promise对象的执行。 总结 使用promise.then(onFulfilled, onRejected) 的话在 onFulfilled 中发生异常的话，在 onRejected 中是捕获不到这个异常的。 在 promise.then(onFulfilled).catch(onRejected) 的情况下then 中产生的异常能在 .catch 中捕获。 .then 和 .catch 在本质上是没有区别的需要分场合使用。 参考promise教程","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"Promise学习02","slug":"2018-08-04-promise学习02","date":"2019-06-26T03:40:39.012Z","updated":"2019-06-26T03:40:39.012Z","comments":true,"path":"2019/06/26/2018-08-04-promise学习02/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-04-promise学习02/","excerpt":"","text":"promise学习02promise.resolve静态方法Promise.resolve(value)，可以认为是new Promise()方法的快捷方式。Promise.resolve(value)的返回值也是一个promise。123Promise.resolve(42).then(function(value)&#123; console.log(value);&#125;); Promise.resolve作为new Promise()的快捷方式，在进行promise对象的初始化或者编写测试代码的时候非常方便。 promise.rejectPromise.reject(error)是和Promise.resolve(value)类似的静态方法，是new Promise()方法的快捷方式。调用代码实例：123Promise.reject(new Error(&quot;BOOM!&quot;)).catch(function(error)&#123; console.error(error);&#125;); Promise只能做异步操作？在使用Promise.resolve(value)等方法的时候，如果promise对象立刻进入resolve状态的话，.then里面指定的方法是同步调用吗？但是，实际上.then方法调用是异步进行的。12345678var promise = new Promise(function (resolve)&#123; console.log(&quot;inner promise&quot;); // 1 resolve(42);&#125;);promise.then(function(value)&#123; console.log(value); // 3&#125;);console.log(&quot;outer promise&quot;); // 2 执行顺序为:123inner promise // 1outer promise // 242 // 3 即使在调用promise.then注册回调函数的时候promise对象已经是确定状态，promise也会以异步的方式调用该回调函数，这是promise设计上的规定方针。 Promise#thenpromise可以写成方法链的形式1234567aPromise.then(function taskA(value)&#123;// task A&#125;).then(function taskB(vaue)&#123;// task B&#125;).catch(function onRejected(error)&#123; console.log(error);&#125;); 如果把在then中注册的每个回调函数称为task的话，可以通过Promise方法链方式来编写能以taskA-&gt;taskB这种流程处理的逻辑了。 promise chain下面看一个稍长的例子。12345678910111213141516171819202122function taskA() &#123; console.log(&quot;Task A&quot;);&#125;function taskB() &#123; console.log(&quot;Task B&quot;);&#125;function onRejected(error) &#123; console.log(&quot;Catch Error: A or B&quot;, error);&#125;function finalTask() &#123; console.log(&quot;Final Task&quot;);&#125;var promise = Promise.resolve();promise .then(taskA) .then(taskB) .catch(onRejected) .then(finalTask); 在promise chain中，由于在 onRejected 和 Final Task 后面没有 catch 处理了，因此在这两个Task中如果出现异常的话将不会被捕获，这点需要注意一下。 taskA产生异常的例子123456789101112131415161718192021222324function taskA() &#123; console.log(&quot;Task A&quot;); throw new Error(&quot;throw Error @ Task A&quot;)&#125;function taskB() &#123; console.log(&quot;Task B&quot;);// 不会被调用&#125;function onRejected(error) &#123; console.log(error);// =&gt; &quot;throw Error @ Task A&quot;&#125;function finalTask() &#123; console.log(&quot;Final Task&quot;);&#125;var promise = Promise.resolve();promise .then(taskA) .then(taskB) .catch(onRejected) .then(finalTask); Task AError: throw Error @ Task AFinal Task promise chain如何传递参数如果想taskA给taskB传递一个参数，则在taskA中return对应的返回值，会在taskB 执行时传给它。12345678910111213141516171819function doubleUp(value) &#123; return value * 2;&#125;function increment(value) &#123; return value + 1;&#125;function output(value) &#123; console.log(value);// =&gt; (1 + 1) * 2&#125;var promise = Promise.resolve(1);promise .then(increment) .then(doubleUp) .then(output) .catch(function(error)&#123; // promise chain中出现异常的时候会被调用 console.error(error); &#125;); 每个方法中 return 的值不仅只局限于字符串或者数值类型，也可以是对象或者promise对象等复杂类型。 return的值会由 Promise.resolve(return的返回值); 进行相应的包装处理，因此不管回调函数中会返回一个什么样的值，最终 then 的结果都是返回一个新创建的promise对象。 Promise#catchPromise#catch只是promise.then(undefined, onRejected);方法的一个别名而已。也就是说这个方法用来注册当promise对象状态变为Rejected时的回调函数。IE8下catch函数可能会遇到问题。这时候可以用then代替catch函数。1234var promise = Promise.reject(new Error(&quot;message&quot;));promise.then(undefined, function (error) &#123; console.error(error);&#125;); 每次调用then都会返回一个新创建的promise对象从代码上乍一看， aPromise.then(...).catch(...) 像是针对最初的 aPromise 对象进行了一连串的方法链调用。 然而实际上不管是 then 还是 catch 方法调用，都返回了一个新的promise对象。1234567891011var aPromise = new Promise(function (resolve) &#123; resolve(100);&#125;);var thenPromise = aPromise.then(function (value) &#123; console.log(value);&#125;);var catchPromise = thenPromise.catch(function (error) &#123; console.error(error);&#125;);console.log(aPromise !== thenPromise); // =&gt; trueconsole.log(thenPromise !== catchPromise);// =&gt; true 123456789101112131415161718192021222324252627// 1: 对同一个promise对象同时调用 `then` 方法var aPromise = new Promise(function (resolve) &#123; resolve(100);&#125;);aPromise.then(function (value) &#123; return value * 2;&#125;);aPromise.then(function (value) &#123; return value * 2;&#125;);aPromise.then(function (value) &#123; console.log(&quot;1: &quot; + value); // =&gt; 100&#125;)// vs// 2: 对 `then` 进行 promise chain 方式进行调用var bPromise = new Promise(function (resolve) &#123; resolve(100);&#125;);bPromise.then(function (value) &#123; return value * 2;&#125;).then(function (value) &#123; return value * 2;&#125;).then(function (value) &#123; console.log(&quot;2: &quot; + value); // =&gt; 100 * 2 * 2&#125;); 第1种写法中并没有使用promise的方法链方式，这在Promise中是应该极力避免的写法。这种写法中的 then 调用几乎是在同时开始执行的，而且传给每个 then 方法的 value 值都是 100 。 第2中写法则采用了方法链的方式将多个 then 方法调用串连在了一起，各函数也会严格按照 resolve → then → then → then 的顺序执行，并且传给每个 then 方法的 value 的值都是前一个promise对象通过 return 返回的值。 promise 错误写法then的错误使用方法12345678function badAsyncCall() &#123; var promise = Promise.resolve(); promise.then(function() &#123; // 任意处理 return newVar; &#125;); return promise;&#125; 上述的问题，首先promise.then中产生的异常不能被外界捕获，也不能得到then的返回值。上述问题正确的写法：1234567function anAsyncCall() &#123; var promise = Promise.resolve(); return promise.then(function() &#123; // 任意处理 return newVar; &#125;);&#125; promise-anti-patterns 参考promise教程","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"Promise学习01","slug":"2018-08-04-promise学习01","date":"2019-06-26T03:40:39.009Z","updated":"2019-06-26T03:40:39.009Z","comments":true,"path":"2019/06/26/2018-08-04-promise学习01/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-04-promise学习01/","excerpt":"","text":"Promise学习01Promise定义Promise是抽象异步对象以及对其进行各种操作的组件，并不发祥于JS。下面给出了一个使用Promise进行异步处理的实例。12345678----var promise = getAsyncPromise(&quot;fileA.txt&quot;); promise.then(function(result)&#123; // 获取文件内容成功时的处理&#125;).catch(function(error)&#123; // 获取文件内容失败时的处理&#125;);---- promise的功能是将复杂的异步处理进行模式化。 Promise简介Constructor要想创建一个promise对象，可以使用new来调用promise的构造器进行实例化。1234var promise = new Promise(function(resolve, reject) &#123; // 异步处理 // 处理结束后、调用resolve 或 reject&#125;); Instance Method对通过new生成的promise对象为了设置其在resolve/reject时调用的回调函数，可以使用promise.then()方法。1promise.then(onFulfilled, onRejected) 当resole成功时，onFulfilled调用。当reject失败时，onRejected调用。onFulfilled, onRejected都为可选参数，若只需要处理异常或者处理正常情况，只需要提供一个参数。 promise workflow1234567891011121314function asyncFunction() &#123; return new Promise(function(resolve, reject) &#123; setTimeout(function() &#123; resolve(&apos;Async Hello world&apos;); &#125;, 1600); &#125;);&#125;asyncFunction().then(function(value) &#123; console.log(value); // =&gt; &apos;Async Hello world&apos;&#125;).catch(function(error) &#123; console.log(error);&#125;); 以上示例代码执行顺序。 Promise构造器后，会返回一个promise对象。 setTimeout延时1.6秒后，then方法就会调用。这时候会打印出字符。 Promise状态 Fulfilled resolve成功时，此时会调用onFulfilled。 Rejected reject(失败)时，此时会调用onRejected。 Pending Promise对象被调用时的初始状态。上述状态都是内部状态，外部没有API访问。 编写Promise代码创建Promise对象 new Promise(fn)返回一个promise对象 在fn中指定异步等处理。处理正常，调用resolve()。处理失败，调用reject()。 创建promise对象123456789101112131415161718192021222324function getURL(URL) &#123; return new Promise(function (resolve, reject) &#123; var req = new XMLHttpRequest(); req.open(&apos;GET&apos;, URL, true); req.onload = function () &#123; if (req.status === 200) &#123; resolve(req.responseText); &#125; else &#123; reject(new Error(req.statusText)); &#125; &#125;; req.onerror = function () &#123; reject(new Error(req.statusText)); &#125;; req.send(); &#125;);&#125;// 运行示例var URL = &quot;http://httpbin.org/get&quot;;getURL(URL).then(function onFulfilled(value)&#123; console.log(value);&#125;).catch(function onRejected(error)&#123; console.error(error);&#125;); getURL只有取得结果为200时才会调用resolve方法，其他情况则会调用reject方法。 编写promise对象处理123456var URL = &quot;http://httpbin.org/status/500&quot;; getURL(URL).then(function onFulfilled(value)&#123; console.log(value);&#125;).catch(function onRejected(error)&#123; console.error(error);&#125;); 虽然上述代码也可以使用getURL(URL).then(onFulfilled, onRejected),但是使用catch将resolve和reject处理分开比较推荐。 总结 用new Promise方法创建promise对象。 用.then和.catch添加promise对象处理函数。 参考promise工具书","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"微服务设计笔记03","slug":"2018-08-03-微服务设计笔记03","date":"2019-06-26T03:40:39.007Z","updated":"2019-06-26T03:40:39.007Z","comments":true,"path":"2019/06/26/2018-08-03-微服务设计笔记03/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-03-微服务设计笔记03/","excerpt":"","text":"微服务设计笔记03本篇笔记为微服务设计系列读书笔记的第三篇，主要记录如何构建服务。 好服务松耦合 能够独立修改及部署单个服务而不需要修改系统的其他部分。 应该限制两个服务之间不同调用形式的数量，过度的通信会导致紧耦合。 高内聚把相关行为聚集在一起，把不相关的放在别处。如果改变某个行为，最好能够在一个地方修改，然后尽可能快地发布。 限界上下文一个由显式边界限定的特定职责。下图中的仓库和财务就是两个限界上下文。 模块和服务共享特定模型，而不应该共享内部表示。同一个进程内使用模块来减少彼此之间的耦合也是一种选择。一旦发现领域内部的限界上下文，一定要使用模块对其进行建模，同时使用共享和隐藏边界。这些模块边界就可以成为绝佳的微服务候选。微服务应该清晰的和限界上下文保持一致。熟练之后，可以省掉在单块系统中先使用模块这个步骤，而是直接使用单独的服务。但是对于新系统，还是建议先使用单块系统。 过早划分过早将一个系统划分成微服务的代价非常高，尤其面向新领域时。很多时候，将一个已有的代码库划分成微服务，要比从头开始构建微服务要简单。 业务功能思考组织内的限界上下文时，不应该从共享数据的角度考虑，而应该从这些上下文能提供的功能来考虑。高层次的限界上下文可以使用嵌套的方法或者完全分离的方法。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记02","slug":"2018-08-02-微服务设计笔记02","date":"2019-06-26T03:40:39.004Z","updated":"2019-06-26T03:40:39.005Z","comments":true,"path":"2019/06/26/2018-08-02-微服务设计笔记02/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-02-微服务设计笔记02/","excerpt":"","text":"微服务设计笔记02本系列博客为阅读《微服务设计》中文版做的记录。 本篇笔记主要侧重于演化式架构师。 架构师的演化视角架构师必须放弃那种一开始就要设计出完美产品的想法，我们应该设计出一个合理的框架，在这个框架下慢慢演化出正确的系统。与其对所有变化进行预测，不如做一个允许变化的计划。架构必须和团队真正坐在一起，进行实际开发工作。不能只进行电话沟通或者看看团队代码。 分区/服务边界担心服务之间的交互，而不需要过于关注各个服务内部发生的事情。 要求的标准 监控。日志功能和监控情况都需要集中式管理。 接口。举例：URL中使用动词还是名词，如何处理资源分页，如何处理不同版本API。 架构安全性。保证每个服务都可以应对下游服务的错误请求。 代码治理提供范例和服务代码模板。前端eslint限制。 范例。 裁剪服务代码模板。 技术债务发布一些紧急特性，会忽略掉一些约束。偏离技术愿景短期收益，长期需要付出代价。走捷径和系统目标改变都会产生技术债务。 例外管理举例大多数场景下使用Mysql，如果是数据快速增长的场景，可以使用Cassandra。建议拥有更好自治型微服务团队。 集中治理和领导架构师需要制定一组指导开发的原则，需要了解新技术，知道在什么时候做怎样的取舍。 建设团队架构师应该帮助队友成长，微服务提供了一个帮助成长的形式。因为微服务架构中存在多个自治的代码库，每个代码库有自己独立的生命周期，这就提供给更多人提供了单个服务负责的机会。这些人在单个服务上获得足够锻炼后，可以给他们更多的责任。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务设计笔记01","slug":"2018-08-01-微服务设计笔记01","date":"2019-06-26T03:40:39.002Z","updated":"2019-06-26T03:40:39.002Z","comments":true,"path":"2019/06/26/2018-08-01-微服务设计笔记01/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-08-01-微服务设计笔记01/","excerpt":"","text":"微服务设计笔记01本系列博客为阅读《微服务设计》中文版做的记录。本篇文章侧重于记录微服务的优缺点。 微服务定义微服务就是一些协同工作的小而自治的服务。 很小，专注做好一件事。足够小即可，不要过小。小团队无法正常维护时，代码库就比较大了。服务越小，微服务带来的优点和缺点就会越明显。 自治性。一个微服务就是一个独立的实体。 微服务好处 技术异构性。不同的服务，可以使用最适合该服务的技术。微服务可以帮助更快地使用新技术。 弹性。系统中一个组件不可用，并不会导致级联故障。 扩展。可以只对需要扩展的服务进行扩展。不需要扩展的服务运行在性能稍差的机器上。 简化部署。 与组织架构相匹配。避免过大的代码库，从而获得理想的团队大小及生产力。 可组合性。 对可替代性的优化。微服务代码库相对小，可以轻易重写。 面向服务的架构SOA,微服务架构是SOA的一种特定方法。 没有银弹微服务不是银弹。享受微服务带来好处的同时，需要在部署、测试和监控等方面做很多的工作。考虑如何扩展，以保持弹性。需要处理分布式事务与CAP相关的问题。","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"微服务限流","slug":"2018-07-30-微服务限流","date":"2019-06-26T03:40:38.999Z","updated":"2019-06-26T03:40:38.999Z","comments":true,"path":"2019/06/26/2018-07-30-微服务限流/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-07-30-微服务限流/","excerpt":"","text":"微服务限流常用限流架构 在接入层(api-gateway)集成限流功能。分布式或者单实例，如果多实例部署需要考虑使用分布式限流算法。 限流功能封装成RPC服务。微服务接收到请求后，会通过限流服务暴露的RPC接口查询请求是否超过阈值。需要单独部署限流服务，增长运维成本。限流服务本身可能会成为系统性能的瓶颈。 限流功能集成在微服务系统内。需要集成在切面层，尽量与业务代码解耦。限流算法 固定、滑动时间窗口限流。适合微服务接口。 令牌桶、漏桶限流算法。适合阻塞限流。后台一些JOB类限流，超过最大访问频率后，请求不会被拒绝，而是阻塞 到有令牌后再继续执行。 限流熔断策略 记录日志 发送告警 服务降级上线初期适合采用日志记录和告警，待通过日志分析限流达到效果后，再进一步升级为其他限流熔断策略。 验证是否有效针对每次请求需要记录下，对应接口、请求时间点、限流结果（通过还是熔断），通过记录数据绘制图表。 限流开发库-Ratelimiter4j限流开发库需要具有以下特点： 低延迟，不能或者较小影响接口本身的响应时间。 高度容错，限流框架异常不能影响微服务的可用性。 高TPS。限流框架的TPS至少要大于微服务本身的接口TPS。 参考微服务接口限流的设计与思考（附GitHub框架源码）Ratelimiter4j","categories":[],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"一个人的朝圣","slug":"2018-07-30-一个人的朝圣","date":"2019-06-26T03:40:38.997Z","updated":"2019-06-26T03:40:38.997Z","comments":true,"path":"2019/06/26/2018-07-30-一个人的朝圣/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-07-30-一个人的朝圣/","excerpt":"","text":"一个人的朝圣读后感今年做记录的第五本书。 故事梗概 利用半个月的时间读了这本小说，这本小说写的很朴实。讲述了哈罗德与妻子莫琳几十年的感情纠葛。夫妻之间因为儿子的去世而心生隔膜几十年，即使同住一个屋檐下，心却不在一起了。事情起因于哈罗德收到恩人奎妮（替哈罗德背过锅，而丢掉了工作）的来信，告诉他罹患癌症时日无多。哈罗德冲动地拨去电话，请护工转告奎妮，他会步行1000公里去看她，请她一定等着他。说走就走，哈罗德什么都没准备就出发了。期间遇到了很多陌生人给予的帮助，让我印象比较深的是，旅途中哈罗德的几次放弃。 第一次放弃，行程10%处，大概是身体吃不消，后来在一个外科护士的帮助下身体恢复了。 第二次放弃，行程60%处，信念受到动摇，待与疗养院确认奎妮健在且期待他的到来时恢复。 第三次放弃，行程98%处，心灰意冷，失去了动机。在老婆的帮助下，恢复动机。可见坚持做一件事好难，坚持与保持动机才是成功的关键。旅途中通过描写夫妻之间的心理活动，双方都对过去的事情进行了反思，哈罗德也反思自己对儿子教育方式。两认多次都重温了初次见面的情景，感情的根还在。他在旅途中对待一位问题青年的照顾，更像是对儿子的弥补。最终哈罗德还是到达了目的地，但是他与恩人的见面描写的很朴实，很平静。恩人去世的时候，护工说像是在说什么话，像意识到有人来看她了。 莫林最后也来与他回合，两人找回了失去的感情。 感想 心存正念，正如齐老师说的，但行好事莫问前程。 绝大多数人还是普通人，平平淡淡过一生。 心存信念，保持动机。动机不是永恒的，短暂的褪去很正常。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"刻意练习读书笔记","slug":"2018-07-18-刻意练习读书笔记","date":"2019-06-26T03:40:38.994Z","updated":"2019-06-26T03:40:38.994Z","comments":true,"path":"2019/06/26/2018-07-18-刻意练习读书笔记/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-07-18-刻意练习读书笔记/","excerpt":"","text":"刻意练习好久没写博客了，工作忙了一个月，github上也是只有周日才能贡献代码了。光输出迟早变成穷光蛋，还是得持续的输入，读书、看大牛的技术blog。我发现读书的时候，手里还是有个笔记本，能记录下当时的想法。这篇文章基本是我kindle笔记的标注的印象笔记深刻的语句。 我印象最深的一点：还是作者认为的，某项技能都有个最低的入门门槛，一旦达到了入门门槛，以后的成绩跟基因无关。另外这本书给了我对孩子教育的一些想法： 尽早对孩子进行兴趣培养，比如3岁左右可以让她接触舞蹈。 一定坚持鼓励孩子的进步 针对孩子的培训，一定得坚持刻意练习，不能为了克服自己的焦虑，就乱上培训班。 舍得花钱上好老师。 经典语句 一万小时定律前提：刻意练习。 年轻人的大脑比成年人更强，有些能力只能在6岁、12岁、18岁之前培养（或更适宜），但是成年人仍然能通过正确训练掌握新的能力。 天生才能被高估，机会、动机、努力被低估。 5年的程序员不一定比3年的技能更高，原因是没有刻意地提高，天天CRUD提高不了。 最杰出的人是那些在各种有目的的练习中花费时间最多的人。 刻意练习的最佳方法：找到优秀的导师。 如果发现自己不能再快速进步，去找一位新的导师。 瓶颈期 试着做不同的事情，而非更难的事情 并非达到极限，而是动力不足 走出舒适区 人类的身体有一种偏爱稳定性的倾向。 练习改变大脑结构 经常性的训练会使大脑中受到挑战的区域发生改变，大脑通过重新布线的方式来适应这些挑战。 传统学习方法与刻意练习关键差别：传统方法并不是专门用于挑战体内平衡。 心理表征 杰出人物和其他人区别开来的因素：心理表征的质量和数量。 刻意练习的目的就是创建有效的心理表征 刻意练习的特点 刻意练习发展的技能，拥有一整套行之有效的训练方法。 发生在舒适区之外。 有良好定义的特定目标。 有意而为。 包含反馈 既产生有效的心理表征，又依靠有效的心理表征。 构建或修改过去已经获取的技能，逐步改进。 三个F创建有效心理表征 专注focus 反馈feedback 纠正fix 保持动机 刻意练习最终面对的最大问题就是保持动机 意志力和天生才华，是人们在事实发生之后再赋予某人的优点。 刻意练习的人应该每天花费1小时，专心练习那些需要全神贯注才能做好的事情。 刻意练习是孤独的追求。 孩子教育 第一阶段让孩子产生兴趣，从玩耍中追求卓越。（小时候给孩子读书，长大了让他们自己读） 第二阶段：变的认真，激励孩子。 第三阶段：全力投入。杰出人物一般在12-13或15-16要付出巨大的投入。 第四阶段：开拓创新。 怎样解释天生才华 孩子刚开始学习某项技能时，智商高的学得快。 往往有种趋势：智商较低的孩子，练习的更刻苦。 从长远看占上风的是那些练习更勤奋的人，而不是一开始就展露才华的人。 换个角度看基因差异 某些行业或者领域，对从业者的能力或者智商有个最低要求。 最低要求达到之后，以上的成就就跟先天基因无关了。 不存在音乐基因、象棋基因。 相信天生才华是危险的。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"iview使用async-validator校验数字范围","slug":"2018-06-10-iview使用validator校验数字范围","date":"2019-06-26T03:40:38.992Z","updated":"2019-06-26T03:40:38.992Z","comments":true,"path":"2019/06/26/2018-06-10-iview使用validator校验数字范围/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-06-10-iview使用validator校验数字范围/","excerpt":"","text":"iview使用async-validator校验数字范围自定义验证在实际开发中使用iview input组件被要求实现一个功能：校验输入的数字小于xx或者大于xxx，看了一下async-validator API，自定义实现的validator能够通过rule对象的options变量传入参数。自定义校验函数如下：123456789101112131415161718//验证输入的数字比min小export const validateNumberMin = (rule, value, callback) =&gt; &#123; let val = Number(value) if (val != &apos;&apos; &amp;&amp; rule.options.min != &apos;&apos; &amp;&amp; (val &lt; rule.options.min)) &#123; return callback(new Error(&apos;number&apos;)) &#125; else &#123; callback() &#125; &#125; //验证输入的数字比max大export const validateNumberMax = (rule, value, callback) =&gt; &#123; let val = Number(value) if (val != &apos;&apos; &amp;&amp; rule.options.max != &apos;&apos; &amp;&amp; (val &gt; rule.options.max)) &#123; return callback(new Error(&apos;number&apos;)) &#125; else &#123; callback() &#125;&#125; 实际调用validator函数：123456789101112&#123; validator: validateNumberMin, message: &quot;应大于等于100&quot;, trigger: &quot;blur&quot;, options: &#123; min: 100 &#125;&#125;&#123; validator: validateNumberMax, message: &quot;应小于等于200&quot;, trigger: &quot;blur&quot;, options: &#123; max: 100 &#125;&#125; 注意：options实际上是rule对象中的一个变量。","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"travis阿里云自动部署","slug":"2018-06-02-travis自动部署阿里云","date":"2019-06-26T03:40:38.990Z","updated":"2019-06-26T03:40:38.990Z","comments":true,"path":"2019/06/26/2018-06-02-travis自动部署阿里云/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-06-02-travis自动部署阿里云/","excerpt":"","text":"travis阿里云自动部署之前写过一篇关于travis自动部署的文章，但是文章里关于使用ssh auto deploy的描述不太清楚，现在继续重新一篇。 生成部署服务器ssh key直接登陆阿里云部署服务器，执行下面的脚本。注意这段脚本的执行目录可以自己定义，不一定非要在~/.ssh中。第二行travis encrypt-file是加密私钥，其中的-r后跟自己的github仓库名。第三行ssh-copy-id 是将生成的公钥加入到部署主机的~/.ssh/authorized_keys公钥文件中。脚本执行完毕可以将私钥文件和公钥文件删除，切记需要将加密后的私钥文件放入代码目录中。 1234567ssh-keygen -t rsa -b 4096 -C &apos;build@travis-ci.org&apos; -f ./deploy_rsatravis encrypt-file deploy_rsa -r dumingcode/my-fintech-frontendssh-copy-id -i deploy_rsa.pub &lt;ssh-user&gt;@&lt;deploy-host&gt;rm -f deploy_rsa deploy_rsa.pubgit add deploy_rsa.enc .travis.yml 部署前执行脚本先上脚本：12345678addons: ssh_known_hosts: &lt;deploy-host&gt;before_deploy:- openssl aes-256-cbc -K $encrypted_&lt;...&gt;_key -iv $encrypted_&lt;...&gt;_iv -in deploy_rsa.enc -out /tmp/deploy_rsa -d- eval &quot;$(ssh-agent -s)&quot;- chmod 600 /tmp/deploy_rsa- ssh-add /tmp/deploy_rsa 此脚本实际运行在travis服务器中，上述脚本的主要作用： addons防止travis登陆部署主机时，需要用户输入一些交互信息。 openssl这行相当于解密私钥文件，注意私钥文件解密在tmp目录下，这个是指travis build主机的目录，不是部署主机的目录。 ssh-add把专用密钥添加到 ssh-agent 的高速缓存，deploy阶段在部署服务器执行命令时，不需要再输入秘钥相关信息。部署脚本123456deploy: provider: script skip_cleanup: true script: rsync -r --delete-after --quiet $TRAVIS_BUILD_DIR/&lt;dir&gt; &lt;ssh-user&gt;@&lt;deploy-host&gt;:path/to/files on: branch: master 此段脚本实际是travis服务器ssh登录到部署服务器后，在部署服务器中执行的命令。$TRAVIS_BUILD_DIR/&lt;dir&gt; 此目录指的是travis build主机中的地址，如果你要部署hexo博客，那dir指的是public，如果要部署的是vue应用则此dir指的是dist目录。 完整版travis.yml12345678910111213141516171819202122sudo: falselanguage: node_jsnode_js: - 8cache: directories: - node_modulesscript: - npm run buildaddons: ssh_known_hosts: 39.107.119.46 # 请替换成自己的服务器IPbefore_deploy: - openssl aes-256-cbc -K $encrypted_ed86006c7fbb_key -iv $encrypted_ed86006c7fbb_iv -in deploy_rsa.enc -out /tmp/deploy_rsa -d - eval &quot;$(ssh-agent -s)&quot; - chmod 600 /tmp/deploy_rsa - ssh-add /tmp/deploy_rsadeploy: provider: script skip_cleanup: true script: rsync -r --delete-after --quiet $TRAVIS_BUILD_DIR/dist $deploy_user@39.107.119.46:$DEPLOY_PATH on: branch: master 参考SSH deploys with Travis CI","categories":[],"tags":[{"name":"CICD","slug":"CICD","permalink":"http://yoursite.com/tags/CICD/"}]},{"title":"成功动机目标读书笔记","slug":"2018-05-27-成功动机目标读书笔记","date":"2019-06-26T03:40:38.988Z","updated":"2019-06-26T03:40:38.988Z","comments":true,"path":"2019/06/26/2018-05-27-成功动机目标读书笔记/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-27-成功动机目标读书笔记/","excerpt":"","text":"成功、动机与目标成功者与自制力悖论 区分为什么和是什么 长期困难的目标采用“为什么”制定目标 简单的目标采用“是什么”制定目标，强调即刻动手去做。 智力是否是增长的 坚信智力、个性、体质不是固定不变的，都是可以成长的。 环境中部署一些提示点和触发点，让潜意识为达到目标而努力工作。 注重成长还是表现聚焦“进步”，享受旅途。把绩效目标转换成长期成长目标，这样能降低忧虑。个人感受：在公司就要努力工作，但是不能一味埋头，也要考虑自己的职业生涯。 幸福的三个要素满足人类对关联感、胜任力、与自主权这三种基本需求的目标，能带来幸福。 关联感增强人际关系。目标建立或巩固人际关系以及回报社会，关联感的需求就会得到满足。 胜任力开发新技能。追求的目标应该与个人成长、从经验中学习以及开发性机能有关。 自主权反映出热情所在。追求符合个人兴趣爱好以及核心价值的目标。 过度追求名誉财富可能带来困扰。 内在动力可以燎原，自由选择的目标能够产生内在动力。但是注意的是，这种动力能被任何管制因素破坏，包括奖励、惩罚、期限以及监控。 自主权是动力的燃料。将任务转成个人目标，称为内化。 进攻型目标和防御性目标 进攻给为了获得，防御为了不失去。 乐观精神适合进取型目标，不适合防御性目标。 进攻使人热血沸腾，防御使人如释重负。 进攻目标偏爱风险，防御目标偏爱谨慎。 目标感染 使目标个人化。给他人尽可能多的达标方式的选择。 无个人选择时，公众承诺。 运用适当的触发因素。 有效计划 制定如果…..就…..计划。如果是周一到周五晚上，我就不吃完饭。诸如此类的计划 决定具体行动，目标计划要具体，时点地点要详细。 瞄准障碍。对待诱惑也制定出如果就计划。如果周一到周五晚上有人叫我吃饭，我就不去。 不冒无畏的风险 不进则退，意志力这个东西类似人的肌肉，一旦不锻炼就会消退。但是一直锻炼则会加强。 启动自制力。日常接受一些小的挑战，比如周一到周五晚节食。 休息必不可少。自制力是损耗品。 别冒无谓的风险。尊重自制肌的局限性，别同时接受两个很难的挑战。比如节食和戒烟。 乐观精神 适度乐观有益。 把注意力从能力上移走，转移到努力、坚持与计划上来。 懂的何时放手 学会坚韧。长期投入、不退缩。 坚韧起来的方法，进步型目标以及自主选择的目标。 旧的不去，新的不来。放弃困难太大或者代价太高的目标，但是需要找个新目标去代替旧的。从而保持参与感与使命感。 评价 说实话，人们要为自己缺少勤奋和错误策略而负责。 正面、实用的建议。 真诚的赞美。 表扬对事不对人。 避免比较，不能在孩子之间、学生、职员之间比较。 领悟与总结","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"github_travis_coveralls自动集成自动部署","slug":"2018-05-15-github_travis_coveralls自动集成自动部署","date":"2019-06-26T03:40:38.985Z","updated":"2019-06-26T03:40:38.985Z","comments":true,"path":"2019/06/26/2018-05-15-github_travis_coveralls自动集成自动部署/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-15-github_travis_coveralls自动集成自动部署/","excerpt":"","text":"github_travis_coveralls自动集成自动部署近期准备开发一个指数定投金额计算的小项目，源码托管在github上。之前都是本地测试通过后，使用XSHELL将代码传输到部署服务器。后来自己研究了下jenkins，并借助jenkins实现将hexo博客自动部署，详见之前写的文章。但是首先jenkins需要自己部署，而且毕竟代码托管在github，干脆秉承一切都开源的精神，折腾一下将github,travis,coveralls结合起来，实现CICD。 travis配置注册配置travis直接使用github的账号登陆travis，登陆之后如下图所示添加自己的github repo。添加完毕后，setting如下图所示。 travis自动集成简单说这个功能就是实现代码上传travis后，能够travis自动执行测试过程。本人使用的是nodejs开发项目，所以先在package.json文件中定义了npm test命令，travis通过执行npm test命令完成自动构建。 travis mocha测试异步代码不退出在测试过程中，遇到一个大坑折腾很久，项目中含有异步调用测试案例，导致travis测试通过后一直运行不退出，后来只能在npm test脚本命令中添加--exit。另外travis默认异步调用等待时间是2秒，所以需要在命令中使用-t 10000人为延长travis等待时间。具体npm test命令为：12345678&quot;name&quot;: &quot;my-fintech-datacenter&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;data-center&quot;, &quot;main&quot;: &quot;schedule.js&quot;, &quot;scripts&quot;: &#123; &quot;test&quot;: &quot;mocha -t 10000 ./test/tasktest.js --exit&quot; &#125; &#125; redis服务设置秘钥travis支持在测试前启用mysql redis mongodb等服务，但是我的代码中对redis设置了秘钥，而且犯懒了不太想改代码，然后找到了如下解决方案，在travis.yml配置文件中添加:before_script: sudo redis-server /etc/redis/redis.conf --requirepass $redis_password ，注意命令中的$redis_password是在travis setting中设置的加密环境变量。后面还会讲到travis还会针对文件进行加密，比如一些项目配置文件以及SSH秘钥都需要进行文件加密。 travis代码覆盖率集成这个功能主要是借助coveralls实现。首先package.json文件中,定义一条命令：1&quot;test-cov&quot;: &quot;./node_modules/istanbul/lib/cli.js cover ./node_modules/mocha/bin/_mocha -- --timeout 10000 -R spec ./test/ --exit&quot; 测试依赖包如下所示：12345678&quot;devDependencies&quot;: &#123; &quot;chai&quot;: &quot;^4.1.2&quot;, &quot;coveralls&quot;: &quot;^3.0.1&quot;, &quot;istanbul&quot;: &quot;^0.4.5&quot;, &quot;istanbul-harmony&quot;: &quot;^0.3.16&quot;, &quot;mocha&quot;: &quot;^5.1.1&quot;, &quot;supertest&quot;: &quot;^3.0.0&quot; &#125; coveralls网站配置登陆coveralls，coveralls也是关联github的账号直接登陆。如下图所示添加github中的repo。添加repo完毕后，需要记录下下图所示的token。 travis.yml 配置123after_success: - npm run test-covafter_script: cat ./coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js -repotoken $COVERALLS_TOKEN 注意上述代码中的$COVERALLS_TOKEN，同样也是在travis setting中加密。 自动部署在上面两步自动构建完毕之后，需要将测试通过的代码直接提交到部署服务器，然后重启部署服务器的服务。从travis构建服务器到部署服务器，需要使用SSH协议。travis服务器中需要存储有私钥(_rsa files)，而远端部署服务器需要部署公钥(_rsa.pub files)。但是私钥肯定是不能存储到git repo或者显示在travis的构建日志中。幸运的是，travis提供了对文件的加密功能，可以借助此功能，将加密后的私钥放到github repo中。注：此处说的ssh 公钥 私钥都是在部署服务器中生成的。私钥加密后传到github，公钥继续留在部署服务器。travis文件加密需要安装travis客户端，推荐linux上安装。目标（想登陆的机器）主机存公钥，源机器存私钥。 travis客户端安装12345678yum install ruby ruby-develyum install gemgem update --system#添加源gem sources --add https://gems.ruby-china.org/gem install travistravis login输入github用户密码即可 deploy server生成ssh key登陆deploy server，执行下面的代码。注意：私钥需要经过travis加密，然后传到github repo 。对待公钥则直接执行ssh-copy-id命令。12345ssh-keygen -t rsa -b 4096 -C &apos;build@travis-ci.org&apos; -f ./deploy_rsatravis encrypt-file deploy_rsa --addssh-copy-id -i deploy_rsa.pub &lt;ssh-user&gt;@&lt;deploy-host&gt;rm -f deploy_rsa deploy_rsa.pub travis加密文件的执行步骤如下所示：1234567891011121314[root@iz2ze1fd7d8ota0f9ysaazz .ssh]# [root@iz2ze1fd7d8ota0f9ysaazz .ssh]# travis encrypt-file id_rsa -r dumingcode/my-fintech-datacenterencrypting id_rsa for dumingcode/my-fintech-datacenterstoring result as id_rsa.encstoring secure env variables for decryptionPlease add the following to your build script (before_install stage in your .travis.yml, for instance): openssl aes-256-cbc -K $encrypted_383bc2ea2d21_key -iv $encrypted_383bc2ea2d21_iv -in id_rsa.enc -out id_rsa -dPro Tip: You can add it automatically by running with --add.Make sure to add id_rsa.enc to the git repository.Make sure not to add id_rsa to the git repository.Commit all changes to your .travis.yml. 还有一点要注意 travis第一次登录远程服务器会出现 SSH 主机验证，这边会有一个主机信任问题。官方给出的方案是添加 addons 配置：12addons: ssh_known_hosts: your-ip 自动部署脚本自动部署脚本如下所示。本人是采用的rsync命令从travis上将有变化的文件传输到部署服务器，这个命令比scp命令要好。after_deploy命令执行的是pm2 restart，要求nodejs服务至少之前在服务器上执行一次，要不restart要报错。1234567891011deploy: provider: script skip_cleanup: true script: rsync -r --delete-after --quiet $TRAVIS_BUILD_DIR $deploy_user@39.107.119.46:$DEPLOY_PATH on: branch: masterafter_deploy: - ssh $deploy_user@your-ip &quot;pm2 restart datacenter&quot;addons: ssh_known_hosts: your-ip 完整代码下面贴上travis.yml和package.json文件的全部信息。项目github的地址为github。travis.yml文件:12345678910111213141516171819202122232425262728293031sudo: falselanguage: node_jsnode_js: - 8before_install: - openssl aes-256-cbc -K $encrypted_1fc90f464345_key -iv $encrypted_1fc90f464345_iv -in ./config/config.js.enc -out ./config/config.js -d - openssl aes-256-cbc -K $encrypted_383bc2ea2d21_key -iv $encrypted_383bc2ea2d21_iv -in id_rsa.enc -out ~/.ssh/id_rsa -d - chmod 600 ~/.ssh/id_rsabefore_script: sudo redis-server /etc/redis/redis.conf --requirepass $redis_passwordnotifications: email: recipients: - jake1036@126.com on_success: always on_failure: alwaysscript: - npm testafter_success: - npm run test-covdeploy: provider: script skip_cleanup: true script: rsync -r --delete-after --quiet $TRAVIS_BUILD_DIR $deploy_user@39.107.119.46:$DEPLOY_PATH on: branch: masterafter_deploy: - ssh $deploy_user@39.107.119.46 &quot;pm2 restart datacenter&quot;addons: ssh_known_hosts: 39.107.119.46 # 请替换成自己的服务器IPafter_script: cat ./coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js -repotoken $COVERALLS_TOKEN package.json文件内容：12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;name&quot;: &quot;my-fintech-datacenter&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;data-center&quot;, &quot;main&quot;: &quot;schedule.js&quot;, &quot;scripts&quot;: &#123; &quot;test&quot;: &quot;mocha -t 10000 ./test/tasktest.js --exit&quot;, &quot;test-cov&quot;: &quot;./node_modules/istanbul/lib/cli.js cover ./node_modules/mocha/bin/_mocha -- --timeout 10000 -R spec ./test/ --exit&quot; &#125;, &quot;repository&quot;: &#123; &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;git+https://github.com/dumingcode/my-fintech-datacenter.git&quot; &#125;, &quot;keywords&quot;: [ &quot;datacenter&quot; ], &quot;author&quot;: &quot;duming&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;bugs&quot;: &#123; &quot;url&quot;: &quot;https://github.com/dumingcode/my-fintech-datacenter/issues&quot; &#125;, &quot;homepage&quot;: &quot;https://github.com/dumingcode/my-fintech-datacenter#readme&quot;, &quot;dependencies&quot;: &#123; &quot;axios&quot;: &quot;^0.18.0&quot;, &quot;bunyan&quot;: &quot;^1.8.12&quot;, &quot;ioredis&quot;: &quot;^3.2.2&quot;, &quot;node-schedule&quot;: &quot;^1.3.0&quot; &#125;, &quot;devDependencies&quot;: &#123; &quot;chai&quot;: &quot;^4.1.2&quot;, &quot;coveralls&quot;: &quot;^3.0.1&quot;, &quot;istanbul&quot;: &quot;^0.4.5&quot;, &quot;istanbul-harmony&quot;: &quot;^0.3.16&quot;, &quot;mocha&quot;: &quot;^5.1.1&quot;, &quot;supertest&quot;: &quot;^3.0.0&quot; &#125;&#125; 参考感谢以下各位大神。Travis CI 系列：自动化部署博客SSH deploys with Travis CIBuilding Better npm Modules with Travis and Coveralls","categories":[],"tags":[{"name":"CICD","slug":"CICD","permalink":"http://yoursite.com/tags/CICD/"}]},{"title":"koa2学习之路-02-静态资源&cookie&jsonp&测试","slug":"2018-05-11-koa2学习之路-02","date":"2019-06-26T03:40:38.980Z","updated":"2019-06-26T03:40:38.980Z","comments":true,"path":"2019/06/26/2018-05-11-koa2学习之路-02/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-11-koa2学习之路-02/","excerpt":"","text":"koa2学习之路-02-静态资源-cookie-jsonp-测试koa2针对静态资源提供了一个koa-staticpackage。1234567891011121314var Koa = require(&apos;koa&apos;)var KoaStatic = require(&apos;koa-static&apos;)var path = require(&apos;path&apos;)// 静态资源目录对于相对入口文件index.js的路径const staticPath = &apos;./static&apos;var app = new Koa()app.use(KoaStatic( path.join(__dirname, staticPath)))app.listen(3000, () =&gt; &#123; console.log(path.join(__dirname, staticPath))&#125;) koa2实现cookie直接用上下文cookie对象操作cookie。123456789101112131415161718192021222324var Koa = require(&apos;koa&apos;)var app = new Koa()app.use(async(ctx) =&gt; &#123; if (ctx.url === &apos;/&apos;) &#123; ctx.cookies.set(&apos;cid&apos;, &apos;hello world&apos;, &#123; domain: &apos;localhost&apos;, // 写cookie所在的域名 path: &apos;/index&apos;, // 写cookie所在的路径 maxAge: 10 * 60 * 1000, // cookie有效时长 expires: new Date(&apos;2019-02-15&apos;), // cookie失效时间 httpOnly: false, // 是否只用于http请求中获取 overwrite: false // 是否允许重写 &#125;) ctx.body = &apos;cookie set ok&apos; &#125; else &#123; ctx.body = &apos;hello world&apos; &#125;&#125;)app.listen(3000, () =&gt; &#123; console.log(&apos;server is running on port 3000&apos;)&#125;) 注意上述代码中，cookies.set(name,value,option)的函数参数，第一个是key，第二个是vlaue，第三个是option。 sessionkoa2原生功能只提供了cookie的操作，但是没有提供session操作。session就只用自己实现或者通过第三方中间件实现。在koa2中实现session的方案有一下几种 如果session数据量很小，可以直接存在内存中 如果session数据量很大，则需要存储介质存放session数据 模板引擎koa使用koa-views在服务器端渲染视图，另外采用ejs模板。 123456789101112131415161718const Koa = require(&apos;koa&apos;)const views = require(&apos;koa-views&apos;)const path = require(&apos;path&apos;)const app = new Koa()// 加载模板引擎app.use(views(path.join(__dirname, &apos;./view&apos;), &#123; extension: &apos;ejs&apos;&#125;))app.use(async(ctx) =&gt; &#123; let title = &apos;hello koa2&apos; await ctx.render(&apos;index&apos;, &#123; title, &#125;)&#125;)app.listen(3000) index.ejs模板内容：12345678910&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;%= title %&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&lt;%= title %&gt;&lt;/h1&gt; &lt;p&gt;EJS Welcome to &lt;%= title %&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; jsonp实现在项目复杂的业务场景，有时候需要在前端跨域获取数据，这时候提供数据的服务就需要提供跨域请求的接口，通常是使用JSONP的方式提供跨域接口。123456789101112131415161718192021222324252627var Koa = require(&apos;koa&apos;)var app = new Koa()app.use(async(ctx) =&gt; &#123; // 判断是否为JSONP的请求 if (ctx.method === &apos;GET&apos; &amp;&amp; ctx.url.split(&apos;?&apos;)[0] === &apos;/getData.jsonp&apos;) &#123; // 获取jsonp的callback let callbackName = ctx.query.callback || &apos;callback&apos; let returnData = &#123; success: true, data: &#123; text: &apos;this is a jsonp api&apos;, time: new Date().getTime(), &#125; &#125; // jsonp的script字符串 let jsonpStr = `;$&#123;callbackName&#125;($&#123;JSON.stringify(returnData)&#125;)` // 用text/javascript，让请求支持跨域获取 ctx.type = &apos;text/javascript&apos; // 输出jsonp字符串 ctx.body = jsonpStr &#125;&#125;)app.listen(3000, () =&gt; &#123; console.log(&apos;server is running on port 3000&apos;)&#125;) 下面使用koa-jsonp中间件实现一版。123456789101112131415161718192021222324const Koa = require(&apos;koa&apos;)const jsonp = require(&apos;koa-jsonp&apos;)const app = new Koa()// 使用中间件app.use(jsonp())app.use(async(ctx) =&gt; &#123; let returnData = &#123; success: true, data: &#123; text: &apos;this is a jsonp api&apos;, time: new Date().getTime(), &#125; &#125; // 直接输出JSON ctx.body = returnData&#125;)app.listen(3000, () =&gt; &#123; console.log(&apos;[demo] jsonp is starting at port 3000&apos;)&#125;) jsonp解析原理： JSONP跨域输出的数据是可执行的JavaScript代码 ctx输出的类型应该是’text/javascript’ ctx输出的内容为可执行的返回数据JavaScript代码字符串 需要有回调函数名callbackName，前端获取后会通过动态执行JavaScript代码字符，获取里面的数据 单元测试测试是一个项目周期里必不可少的环节，开发者在开发过程中也是无时无刻进行“人工测试”，如果每次修改一点代码，都要牵一发动全身都要手动测试关联接口，这样子是禁锢了生产力。为了解放大部分测试生产力，相关的测试框架应运而生，比较出名的有mocha，karma，jasmine等。虽然框架繁多，但是使用起来都是大同小异。 安装单元测试库npm install --save-dev mocha chai supertest mocha 模块是测试框架 chai 模块是用来进行测试结果断言库，比如一个判断 1 + 1 是否等于 2 supertest 模块是http请求测试库，用来请求API接口 创建demo程序12345678910111213141516171819202122232425262728293031323334353637383940const Koa = require(&apos;koa&apos;)const app = new Koa()const server = async(ctx, next) =&gt; &#123; let result = &#123; success: true, data: null &#125; if (ctx.method === &apos;GET&apos;) &#123; if (ctx.url === &apos;/getString.json&apos;) &#123; result.data = &apos;this is string data&apos; &#125; else if (ctx.url === &apos;/getNumber.json&apos;) &#123; result.data = 123456 &#125; else &#123; result.success = false &#125; ctx.body = result next &amp;&amp; next() &#125; else if (ctx.method === &apos;POST&apos;) &#123; if (ctx.url === &apos;/postData.json&apos;) &#123; result.data = &apos;ok&apos; &#125; else &#123; result.success = false &#125; ctx.body = result next &amp;&amp; next() &#125; else &#123; ctx.body = &apos;hello world&apos; next &amp;&amp; next() &#125;&#125;app.use(server)module.exports = appapp.listen(3000, () =&gt; &#123; console.log(&apos;[demo] test-unit is starting at port 3000&apos;)&#125;) 注意上述代码中有一句app.use(server)这是为了供测试框架调用，访问http://127.0.0.1:3000/getString.json，输出结果为：1234&#123;success: true,data: &quot;this is string data&quot;&#125; 测试框架代码123456789101112131415161718const supertest = require(&apos;supertest&apos;)const chai = require(&apos;chai&apos;)const app = require(&apos;../index&apos;)const expect = chai.expectconst request = supertest(app.listen())//测试组套件describe(&apos;开始测试demo的getString请求&apos;, //测试用例 it(&apos;测试/getString.json请求&apos;, (done) =&gt; &#123; request.get(&apos;/getString.json&apos;).expect(200).end((err, res) =&gt; &#123; expect(res.body).to.be.an(&apos;object&apos;) expect(res.body.success).to.be.an(&apos;boolean&apos;) expect(res.body.data).to.be.an(&apos;string&apos;) done() &#125;) &#125;)) 全部安装mocha后，直接在上述代码文件目录内执行mocha --harmony，执行结果如下：1234[demo] test-unit is starting at port 3000 √ 测试/getString.json请求 1 passing (46ms) 更新package.json中的包 安装：npm install -g npm-check-updates 检查package.json中dependencies的最新版本：ncu 更新dependencies到新版本：ncu -u 参考感谢让我站在肩膀上的大神chenshenhai。koa2实践5分钟讲透jsonp","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"koa2学习之路-01","slug":"2018-05-11-koa2学习之路-01","date":"2019-06-26T03:40:38.977Z","updated":"2019-06-26T03:40:38.977Z","comments":true,"path":"2019/06/26/2018-05-11-koa2学习之路-01/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-11-koa2学习之路-01/","excerpt":"","text":"koa2学习之路-01继续尝试学习koa2，首先从helloworld开始，直接上代码。 helloworld123456789var koa = require(&apos;koa&apos;)var app = new koa()app.use(async(ctx) =&gt; &#123; ctx.body = &apos;hello koa2&apos;&#125;)app.listen(3000)console.log(&apos;server is running on port 3000&apos;) 运行上述js文件，通过浏览器访问127.0.0.1:3000。 async/await特点 可以让异步逻辑用同步写法实现 最底层的await返回需要是Promise对象 可以通过多层 async function的同步写法代替传统的callback嵌套 async 中间件在koa2中使用中间件的代码如下：12345678910function log(ctx) &#123; console.log(ctx.method, ctx.header.host + ctx.url)&#125;module.exports = function() &#123; return async function(ctx, next) &#123; log(ctx); await next(); &#125;;&#125; 调用中间的代码如下：123456789101112var koa = require(&apos;koa&apos;)var async_log = require(&apos;./middleware/async-log&apos;)var app = new koa()app.use(async_log())app.use(async(ctx) =&gt; &#123; ctx.body = &apos;hello koa2&apos;&#125;)app.listen(3000)console.log(&apos;server is running on port 3000&apos;) 综上可以看出所谓的middleware实际上就是一个async函数，函数最后需要用await 调用下一个middleware，上述代码中为await next()。koa2代码对middleware的编写顺序即为中间件的执行顺序。 java web的filter 可以使用 middleware去实现，包括校验用户是否登陆，均可以通过middleware实现。 GET请求数据获取使用方法在koa中，获取GET请求数据源头是koa中request对象中的query方法或querystring方法，query返回是格式化好的参数对象，querystring返回的是请求字符串，由于ctx对request的API有直接引用的方式，所以获取GET请求数据有两个途径。 是从上下文中直接获取 请求对象ctx.query，返回如 { a:1, b:2 } 请求字符串 ctx.querystring，返回如 a=1&amp;b=2 是从上下文的request对象中获取 请求对象ctx.request.query，返回如 { a:1, b:2 } 请求字符串 ctx.request.querystring，返回如 a=1&amp;b=2 代码实例如下：12345678910111213141516171819202122var Koa = require(&apos;koa&apos;)var app = new Koa()app.use(async(ctx) =&gt; &#123; let url = ctx.url let ctx_query = ctx.query let ctx_query_string = ctx.querystring let request = ctx.request let request_query = request.query let request_query_string = request.querystring ctx.body = &#123; url, ctx_query, ctx_query_string, request_query, request_query_string &#125;&#125;)app.listen(3000, () =&gt; &#123; console.log(&apos;[demo] request get is starting at port 3000&apos;)&#125;) POST请求参数post请求需要使用koa-body包获取表单提交的参数。12345678910111213141516171819202122232425262728293031323334var Koa = require(&apos;koa&apos;)var koaBody = require(&apos;koa-body&apos;)var app = new Koa()app.use(koaBody())app.use(async(ctx) =&gt; &#123; console.log(ctx.url) if (ctx.url === &apos;/&apos; &amp;&amp; ctx.method === &apos;GET&apos;) &#123; // 当GET请求时候返回表单页面 let html = ` &lt;h1&gt;koa2 request post demo&lt;/h1&gt; &lt;form method=&quot;POST&quot; action=&quot;/&quot;&gt; &lt;p&gt;userName&lt;/p&gt; &lt;input name=&quot;userName&quot; /&gt;&lt;br/&gt; &lt;p&gt;nickName&lt;/p&gt; &lt;input name=&quot;nickName&quot; /&gt;&lt;br/&gt; &lt;p&gt;email&lt;/p&gt; &lt;input name=&quot;email&quot; /&gt;&lt;br/&gt; &lt;button type=&quot;submit&quot;&gt;submit&lt;/button&gt; &lt;/form&gt; ` ctx.body = html &#125; else if (ctx.url === &apos;/&apos; &amp;&amp; ctx.method === &apos;POST&apos;) &#123; // 当POST请求的时候，解析POST表单里的数据，并显示出来 ctx.body = `Request Body: $&#123;JSON.stringify(ctx.request.body)&#125;`; &#125; else &#123; // 其他请求显示404 ctx.body = &apos;&lt;h1&gt;404！！！ o(╯□╰)o&lt;/h1&gt;&apos; &#125;&#125;)app.listen(3000, () =&gt; &#123; console.log(&apos;[demo] request post is starting at port 3000&apos;)&#125;) 代码github地址 参考感谢让我站在肩膀上的大神chenshenhai。koa2学习进阶笔记","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"ES6学习之路07_Promise与async深入","slug":"2018-05-09-ES6学习之路07_Promise与async深入","date":"2019-06-26T03:40:38.974Z","updated":"2019-06-26T03:40:38.974Z","comments":true,"path":"2019/06/26/2018-05-09-ES6学习之路07_Promise与async深入/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-09-ES6学习之路07_Promise与async深入/","excerpt":"","text":"ES6-学习之路-07 async Promise深入继续深入对js异步操作的学习，主要针对async和await。 Promise方式首先继续回顾Promise，Promise针对异步回调的处理方式，是通过链式，每一步resole data，并将数据传递给下一个then函数。123456789101112new Promise(function(resolve, reject) &#123; // if OK resolve(data); // if ERROR reject(error);&#125;).then(function(data) &#123; return turtle;&#125;).then(function(turtle) &#123; return turtleToRat&#125;).catch(function(error) &#123; // handle an error&#125;); fetch函数针对大多数浏览器都适用并且这个函数会返回一个Promise，下面是一个具体示例，我们会根据用户名访问github的用户信息和用户名下的仓库。123456789101112131415161718192021222324252627282930313233343536var fetch = require(&apos;node-fetch&apos;)function fetchGitProfile(username) &#123; return fetch(`https://api.github.com/users/$&#123;username&#125;`) .then((data) =&gt; data.json()) .then((&#123; bio, company, followers, following, repos_url &#125;) =&gt; (&#123; bio, company, followers, following, repos_url &#125;));&#125;function includeGitRepos(user) &#123; return fetch(user.repos_url) .then((data) =&gt; data.json()) .then((data) =&gt; data.map((&#123; name, stargazers_count &#125;) =&gt; (&#123; name, stargazers_count &#125;))) .then((repoList) =&gt; &#123; return &#123; ...user, repoList &#125;; &#125;);&#125;function log(data) &#123; console.log(data);&#125;fetchGitProfile(&apos;dumingcode&apos;) .then(includeGitRepos) .then(log); 以上代码也许仅仅比回调函数写的好理解一些，而且还需要深入理解Promise代码。 使用async/awaitasync/await 由ES7引入，这两个关键词需要共同使用，await必须在async函数中使用，绝对不能单独使用。一个有趣的特性是，这两个关键字适配Promise。如果一个函数返回结果是Promise你可以用await去resole，或者针对async函数的返回使用then去解析。 基本用法1234async function resolveMyData() &#123; const data = await fetchData(&apos;/a&apos;); return await fetchMoreData(&apos;/b/&apos; + data.id);&#125; 改写上述github代码：12345678910111213141516171819202122232425262728var fetch = require(&apos;node-fetch&apos;)async function fetchGitProfile(username) &#123; const userinfo = await fetch(`https://api.github.com/users/$&#123;username&#125;`) let &#123; bio, company, followers, following, repos_url &#125; = await userinfo.json() return &#123; bio, company, followers, following, repos_url &#125;&#125;async function includeGitRepos(repoUrl) &#123; const repo = await fetch(repoUrl) .then((data) =&gt; data.json()); return repo.map((&#123; name, stargazers_count &#125;) =&gt; (&#123; name, stargazers_count &#125;));&#125;async function resolveGithubProfile() &#123; const profile = await fetchGitProfile(&apos;duming&apos;); const repoList = await includeGitRepos(profile.repos_url); console.log(&#123; ...profile, repoList &#125;);&#125;;resolveGithubProfile(); 上述代码中includeGitRepos中的fetch API后跟着一个then函数，fetch函数仍然会返回一个Promise。 并行异步调用没必要针对每一个函数都使用一个await函数，可以并行地调用，并逐个处理每个请求的返回值:1234567891011async function resolveGithubProfileParallel() &#123; let dumingPromise = fetchGitProfile(&apos;dumingcode&apos;) let octocatPromise = fetchGitProfile(&apos;octocat&apos;) const duming = await dumingPromise; const octocat = await octocatPromise; // this will complete in the same time as rkotzePromise. return [duming, octocat, &quot;done!&quot;];&#125;resolveGithubProfileParallel().then((data) =&gt; &#123; console.log(data)&#125;) 使用promise all12345678910111213141516Promise.all([fetchGitProfile(&apos;rkotze&apos;), fetchGitProfile(&apos;octocat&apos;)]).then(function(values) &#123; console.log(values);&#125;);输出（注意输出的是一个数组）：[ &#123; bio: &apos;Software engineer. Currently into ES6, ReactJS, NodeJS and Elixir.&apos;, company: &apos;@findmypast&apos;, followers: 24, following: 52, repos_url: &apos;https://api.github.com/users/rkotze/repos&apos; &#125;, &#123; bio: null, company: &apos;GitHub&apos;, followers: 2238, following: 5, repos_url: &apos;https://api.github.com/users/octocat/repos&apos; &#125; ] 异步测试框架优先研究一下MochaMochaJestJasmine codewar挑战如果能通过这个挑战，说明你掌握了js异步，点击链接code war异步挑战 参考感谢大神们，让我站在你们的肩膀上。promises-async-await-testing","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"},{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"ES6-学习之路-06promise-async","slug":"2018-05-08-ES6学习之路06_Promise深入","date":"2019-06-26T03:40:38.971Z","updated":"2019-06-26T03:40:38.972Z","comments":true,"path":"2019/06/26/2018-05-08-ES6学习之路06_Promise深入/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-08-ES6学习之路06_Promise深入/","excerpt":"","text":"ES6-学习之路-06promise-async刚接触ES6学了好久，对promise、async、await还是一知半解，so 我能怎么办呢，网上找资料抄代码，研究一番。 回调函数地域当使用回调函数来进行事件处理的时候，如果嵌套多层回调函数的时候，就会出现回调地狱，例如：123456789101112131415161718192021method1(function(err, result) &#123; if (err) &#123; throw err; &#125; method2(function(err, result) &#123; if (err) &#123; throw err; &#125; method3(function(err, result) &#123; if (err) &#123; throw err; &#125; method4(function(err, result) &#123; if (err) &#123; throw err; &#125; method5(result); &#125;); &#125;); &#125;);&#125;); 本例一样嵌套多个方法调用会创建错综复杂的代码，会难以理解与调试。当想要实现更复杂的功能时，回调函数也会存在问题。要是你想让两个异步操作并行运行，并且在它们都结束后提醒你，那该怎么做？要是你想同时启动两个异步操作，但只采用首个结束的结果，那又该怎么做？而使用Promise就能避免回调地狱的情况。 Promise可以当做是一个占位符，表示异步操作的执行结果。函数可以返回一个Promise，而不必订阅一个事件或者向函数传递一个回调函数。 Promise生命周期每个 Promise 都会经历一个短暂的生命周期，初始为挂起状态（pending state） ，这表示异步操作尚未结束。一个挂起的 Promise 也被认为是未决的（unsettled )。一旦异步操作结束， Promise就会被认为是已决的（settled ） ，并进入两种可能状态之一： 已完成（fulfilled ） ： Promise 的异步操作已成功结束； 已拒绝（rejected ） ： Promise 的异步操作未成功结束，可能是一个错误，或由其他原因导致。内部的[[PromiseState]] 属性会被设置为”pending” 、 “fulfilled” 或 “rejected”，以反映Promise的状态。该属性并未在 Promise 对象上被暴露出来，因此你无法以编程方式判断 Promise 到底处于哪种状态。不过你可以使用then()方法在 Promise 的状态改变时执行一些特定操作。 then方法then()方法在所有的 Promise 上都存在，并且接受两个参数。第一个参数是 Promise 被完成时要调用的函数，异步操作的结果数据都会被传入这个完成函数。第二个参数则是 Promise 被拒绝时要调用的函数，与完成函数相似，拒绝函数会被传入与拒绝相关联的任何附加数据。then()方法的两个参数是可选的，因此可以自由组合监听完成和失败的处理函数； catch方法Promise有catch()方法，等同于只传递拒绝处理函数给then()方法：123456789promise.catch(function(err) &#123; // 拒绝 console.error(err.message);&#125;);// 等同于：promise.then(null, function(err) &#123; // 拒绝 console.error(err.message);&#125;); 创建未决的Promise使用Promise构造器可以创建一个Promise实例，此构造器接收一个参数：一个被称之为执行器（excutor）的函数，该函数包含了resolve()函数和reject()函数这两个参数。resolve()函数在异步任务执行成功时调用，而reject()函数在异步任务执行失败时调用。例如：1234567891011let pro = new Promise((resolve, reject) =&gt; &#123; console.log(&apos;hi promise&apos;)&#125;)pro.then(() =&gt; &#123; console.log(&apos;hi then&apos;)&#125;)输出：hi promisei am resole parameterhi then 从输出结果可以看出，Promise构造器中的代码是最先执行的，而then()代码是最后执行的，这是因为只有在Promise中的处理器函数执行结束之后，then()方法中的完成处理函数或者拒绝处理函数才会添加到作业队列的尾部。 创建已决的Promise使用Promise.resolve()Promise.resolve()方法接收一个参数，并会返回一个处于已完成状态的 Promise ，在then()方法中使用完成处理函数才能提取该完成态的Promise传递的值，例如：1234let promise = Promise.resolve(&apos;hi&apos;);promise.then((value) =&gt; &#123; console.log(value); //hi&#125;); 使用Promise.reject()可以使用Promise.reject()方法来创建一个已拒绝状态的Promise，同样只有在拒绝处理函数中或者catch()方法中才能接受reject()方法传递的值：1234let promise_rec = Promise.reject(&apos;reject&apos;)promise_rec.catch((val) =&gt; &#123; console.log(val)&#125;) 执行器错误当执行器内部抛出错误，那么Promise的拒绝处理函数就会被调用，例如：123456let promise_catch = new Promise((resolve, reject) =&gt; &#123; throw new Error(&apos;Error!&apos;)&#125;)promise_catch.catch((msg) =&gt; &#123; console.log(msg)&#125;) Promise链除了使用单个Promise外，多个Promise可以进行级联使用，实际上then()方法或者catch()方法会返回一个新的Promise，仅当前一个Promise被决议之后，后一个Promise才会进行处理。 串联调用12345678910/**串联Promise链 */let p1 = new Promise((resolve, reject) =&gt; &#123; resolve(&apos;p1串联链式调用&apos;)&#125;)p1.then((val) =&gt; &#123; console.log(val) throw new Error(&apos;串联调用抛出异常Err&apos;)&#125;).catch((msg) =&gt; &#123; console.log(msg)&#125;) 可以看出当p1的then()方法执行结束后会返回一个Promise，因此，在此基础上可以继续执行catch()方法。同时，Promise链允许捕获前一个Promise的错误。 Promise链中传值Promise链的另一个重要方面是能从一个Promise传递数据给另一个Promise的能力。**前一个Promise的完成处理函数的返回值，传递到下一个Promise中。12345678910/**Promise链中传值 */let p2 = new Promise((resolve, reject) =&gt; &#123; resolve(1)&#125;)p2.then((val =&gt; val + 1)).then((val) =&gt; &#123; console.log(val)&#125;)输出：2 Promise链中传递Promise在完成或者拒绝处理函数中可以返回基本类型值，从而可以在Promise链中传递。另外，在Promise链中也可以传递对象，如果传递的是Promise对象，就需要额外的处理。12345678910111213/**Promise链中传Promise */let p31 = new Promise((resolve, reject) =&gt; &#123; resolve(100)&#125;)let p41 = new Promise((resolve, reject) =&gt; &#123; resolve(200)&#125;)p31.then((val) =&gt; &#123; console.log(val) return p41&#125;).then((value) =&gt; &#123; console.log(value)&#125;) 如果传递的是reject，则代码如下1234567891011121314151617let p32 = new Promise((resolve, reject) =&gt; &#123; resolve(100)&#125;)let p42 = new Promise((resolve, reject) =&gt; &#123; reject(200)&#125;)p32.then((val) =&gt; &#123; console.log(val) return p42&#125;).then(() =&gt; &#123; console.log(&apos;检验是否执行&apos;)&#125;).catch((value) =&gt; &#123; console.log(value)&#125;)输出：100200 注：上面代码P42的then函数没有执行。 响应多个Promise如果想监视多个Promise的状态，从而决定下一步动作，可以使用ES6提供的两个方法：Promise.all()和Promise.race()； Promise.allPromise.all()方法能接受单个可迭代对象（如数组）作为参数，可迭代对象的元素都是Promise。该方法会返回一个Promise，只有传入所有的Promise都已完成，所返回的Promise才会完成，例如：12345678910111213let p1 = new Promise((resolve, reject) =&gt; &#123; console.log(&apos;p1&apos;)&#125;)let p2 = new Promise((resolve, reject) =&gt; &#123; console.log(&apos;p2&apos;)&#125;)let p3 = new Promise((resolve, reject) =&gt; &#123; console.log(&apos;p3&apos;)&#125;)let p4 = Promise.all([p1, p2, p3]).then((arr) =&gt; &#123; console.log(arr)&#125;) 对 Promise.all() 的调用创建了新的Promise p4，在 p1 、 p2 与 p3 都被完成后， p4 最终会也被完成。传递给 p4 的完成处理函数的结果是一个包含每个决议值（1 、 2 与 3 ） 的数组，这些值的存储顺序保持了待决议的 Promise 的顺序（与完成的先后顺序无关） ，因此你可以将结果匹配到每个Promise。1234567891011121314151617let p1 = new Promise((resolve, reject) =&gt; &#123; resolve(&apos;p1&apos;)&#125;)let p2 = new Promise((resolve, reject) =&gt; &#123; reject(&apos;p2&apos;)&#125;)let p3 = new Promise((resolve, reject) =&gt; &#123; resolve(&apos;p3&apos;)&#125;)let p4 = Promise.all([p1, p2, p3]).then((arr) =&gt; &#123; console.log(arr)&#125;).catch((msg) =&gt; &#123; console.log(msg)&#125;)输出:p2 在此例中， p2 被使用数值 2 进行了拒绝，则 p4 的拒绝处理函数就立刻被调用，而不会 等待 p1 或 p3 结束执行（它们仍然会各自结束执行，只是 p4 不等它们） 。拒绝处理函数总会接受到单个值，而不是一个数组。该值是被拒绝的Promise所返回的拒绝值。 Promise.race()Promise.race()方法接收一个元素是Promise的可迭代对象，并返回一个新的Promise。一旦传入Promise.race()的可迭代对象中有一个Promise是已决状态，那么返回的Promise对象就会立刻成为已决状态。而Promise.all()方法得必须等到所有传入的Promise全部变为已决状态，所返回的Promise才会已决。123456789101112131415161718192021let p1 = new Promise(function(resolve, reject) &#123; resolve(1);&#125;)let p2 = new Promise(function(resolve, reject) &#123; resolve(2);&#125;)let p3 = new Promise(function(resolve, reject) &#123; resolve(3);&#125;)let p4 = Promise.race([p1, p2, p3]);p4.then(value =&gt; &#123; console.log(Array.isArray(value)); //false console.log(value); //1&#125;)输出：false1 romise.race() 方法传入的Promise中哪一个Promise先变成已完成状态，就会将值传递给所返回的Promise对象的完成处理函数中。若哪一个Promise最先变成已拒绝状态，同样的，会将值传递给p4的拒绝处理函数中。 Promise总结 参考感谢以下各位大神，让我站在你们的肩膀上。掘金-你听___","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"},{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"nodejs学习之路03-express webscoket","slug":"2018-05-07-nodejs学习之路03","date":"2019-06-26T03:40:38.969Z","updated":"2019-06-26T03:40:38.969Z","comments":true,"path":"2019/06/26/2018-05-07-nodejs学习之路03/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-07-nodejs学习之路03/","excerpt":"","text":"expressConnect基于http提供了web开发常用的基础功能，express又在Connect基础上提供了构建整个网站和web更为方便的API。通过一个小demo项目熟悉express的用法。demo程序实现的内容： 提供一个表单，接受用户的输入。 输入作为检索关键字，调用tweetAPI，并将返回结果返回给用户。ejs模板文件创建index.ejs和result.ejs12345678index.ejs&lt;h1&gt; Twitter app &lt;/h1&gt;&lt;p&gt;Please enter your search name&lt;/p&gt;&lt;form action=&quot;/search&quot; method=&quot;GET&quot;&gt; name: &lt;input type=&quot;text&quot; name=&quot;q&quot; /&gt; &lt;button&gt;search&lt;/button&gt;&lt;/form&gt; result.ejs实现比较简单，仅仅是回调一下函数12&lt;h1&gt;查询结果：&lt;/h1&gt;&lt;%= results %&gt; nodejs webserver代码注意下面的express代码使用的是最新的express库，《了不起的nodejs》书中的API已经严重老化了。123456789101112131415161718192021222324var express = require(&apos;express&apos;)var app = express();app.set(&apos;view engine&apos;, &apos;ejs&apos;)app.set(&apos;views&apos;, __dirname + &apos;/views&apos;)app.set(&apos;view options&apos;, &#123; layout: false &#125;)app.get(&apos;/&apos;, function(req, res) &#123; res.render(&apos;index&apos;)&#125;);app.get(&apos;/search&apos;, function(req, res, next) &#123; console.log(&apos;Accessing the secret section ...&apos;); res.render(&apos;result&apos;, &#123; results: req.param(&apos;q&apos;) &#125;)&#125;);var server = app.listen(3000, function() &#123; var host = server.address().address; var port = server.address().port; console.log(&apos;Example app listening at http://%s:%s&apos;, host, port);&#125;); package.json文件123456789101112131415&#123; &quot;name&quot;: &quot;express-demo&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;express demo&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;scripts&quot;: &#123; &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot; &#125;, &quot;author&quot;: &quot;duming&quot;, &quot;license&quot;: &quot;ISC&quot;, &quot;dependencies&quot;: &#123; &quot;ejs&quot;: &quot;^2.6.1&quot;, &quot;express&quot;: &quot;^4.16.3&quot; &#125;&#125; 代码组织策略一个项目可能会有非常多的路由，这时良好的代码组织策略非常必要。比如一个应用包含三个模块，\\blog、\\tags、\\pages，每个版块都包含各自的路由，例如：/blog/search,/tags/new,/pages/del。 好的代码组织方式，就是维护一个server.js，该文件包含了路由表。同时将每一个部分的路由处理器通过模块化的方式引入，如blog.js，tags.js，pages.js，首先定义依赖的模块，并初始化app定义中间件。1234567891011121314151617var express = require(&apos;express&apos;), blog = require(&apos;/blog&apos;), tags = require(&apos;/tags&apos;), pages = require(&apos;/pages&apos;)var app = express();//blog routesapp.get(&apos;/blog&apos;,blog.home)app.get(&apos;/blog/search&apos;,blog.search)//tags routesapp.get(&apos;/tags&apos;,tags.home)app.get(&apos;/tags/search&apos;,tags.search)//pages routesapp.get(&apos;/pages&apos;,pages.home)app.get(&apos;/pages/search&apos;,pages.search) 以blog.js为例，针对每个路由函数使用exports123exports.home = function(res,rep,next)&#123; &#125; websocketwebsocket是web下的TCP， 一个底层的双向socket， 允许用户对消息传递进行控制。websocket包含两个部分：一个是前端浏览器实现的WebSocket API，另一个是服务器端实现的WebSocket协议，websocket还是建立在http之上。websocket是双向的，这样server可以向客户端主动推送数据，而之前的http协议交互是单向的，只能是客户端请求server。 （1）建立在 TCP 协议之上，服务器端的实现比较容易。 （2）与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。 （3）数据格式比较轻量，性能开销小，通信高效。 （4）可以发送文本，也可以发送二进制数据。 （5）没有同源限制，客户端可以与任意服务器通信。 （6）协议标识符是ws（如果加密，则为wss），服务器网址就是 URL。 代码实例本文采用ws包实现server端的web service协议。server端代码如下：12345678910const WebSocket = require(&apos;ws&apos;);const wss = new WebSocket.Server(&#123; port: 8181 &#125;);wss.on(&apos;connection&apos;, function connection(ws) &#123; ws.on(&apos;message&apos;, function incoming(message) &#123; console.log(&apos;received: %s&apos;, message); &#125;); ws.send(&apos;something&apos;);&#125;); frontend代码如下：123456789101112131415&lt;h1&gt; Twitter app &lt;/h1&gt;&lt;p&gt;Please enter your search name&lt;/p&gt;&lt;input type=&quot;text&quot; name=&quot;message&quot; id=&quot;message&quot; placeholder=&quot;Type text to echo in here&quot; value=&quot;&quot; /&gt;&lt;button onclick=&quot;sendMessage();&quot;&gt;提交&lt;/button&gt;&lt;script&gt; var ws = new WebSocket(&quot;ws://localhost:8181&quot;); ws.onopen = function(e) &#123; console.log(&apos;Connection to server opened&apos;); &#125; function sendMessage() &#123; ws.send(&quot;test&quot;); &#125;&lt;/script&gt; 参考《了不起的nodejs》express官方网站阮一峰老师blog","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"nodejs学习之路02","slug":"2018-05-06-nodejs学习之路02","date":"2019-06-26T03:40:38.966Z","updated":"2019-06-26T03:40:38.966Z","comments":true,"path":"2019/06/26/2018-05-06-nodejs学习之路02/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-06-nodejs学习之路02/","excerpt":"","text":"nodejs学习之路02简单http代码实例1234567var http = require(&apos;http&apos;)http.createServer((req, res) =&gt; &#123; res.writeHead(200, &#123; &apos;Content-type&apos;: &apos;text/html&apos; &#125;) res.end(&apos;Hello &lt;b&gt;World&lt;/b&gt;&apos;)&#125;).listen(3000) 下面尝试另一个案例，通过http协议传输png文件，体会png文件传输中对chunk的使用。12345678910111213141516171819var http = require(&apos;http&apos;)var fs = require(&apos;fs&apos;)http.createServer((req, res) =&gt; &#123; res.writeHead(200, &#123; &apos;Content-type&apos;: &apos;iamge/png&apos; &#125;) //创建png视图 let stream = fs.createReadStream(&apos;D:/Workspaces/blog/source/images/docker_vm.png&apos;) stream.on(&apos;data&apos;, (data) =&gt; &#123; res.write(data) &#125;) stream.on(&apos;end&apos;, () =&gt; &#123; res.end() &#125;) stream.on(&apos;error&apos;, (err) =&gt; &#123; console.log(err) &#125;)&#125;).listen(3000) http实战demo通过nodejs http API完成一个实际的任务，接收浏览器传来的form数据。首先构建一个发送表单的http服务，代码如下所示：123456var http = require(&apos;http&apos;)http.createServer((req, res) =&gt; &#123; res.writeHead(200, &#123; &apos;Content-type&apos;: &apos;text/html&apos; &#125;) res.end([&apos;&lt;form method=&quot;post&quot; action=&quot;/url&quot;&gt;&apos;, &apos;&lt;h1&gt;My Form&lt;/h1&gt;&apos;, &apos;&lt;fieldset&gt;&apos;, &apos;&lt;label&gt;Personal information&lt;/label&gt;&apos;, &apos;&lt;p&gt;what is your name?&lt;/p&gt;&apos;, &apos;&lt;input type=&quot;text&quot; name=&quot;name&quot;&gt;&lt;/input&gt;&apos;, &apos;&lt;p&gt;&lt;button&gt;Submit&lt;/button&gt;&lt;/p&gt;&apos;, &apos;&lt;/form&gt;&apos;].join(&apos;&apos;))&#125;).listen(3000) 点击表单中的按钮，因为server端没有/url的具体方法，所以无响应。下面增加提交按钮响应逻辑(接受并将输入的name返回到新的页面中,若访问的url不存在返回404)：12345678910111213141516171819var http = require(&apos;http&apos;)http.createServer((req, res) =&gt; &#123; res.writeHead(200, &#123; &apos;Content-type&apos;: &apos;text/html&apos; &#125;) let body = &apos;&apos; if (&apos;/&apos; == req.url) res.end([&apos;&lt;form method=&quot;post&quot; action=&quot;/url&quot;&gt;&apos;, &apos;&lt;h1&gt;My Form&lt;/h1&gt;&apos;, &apos;&lt;fieldset&gt;&apos;, &apos;&lt;label&gt;Personal information&lt;/label&gt;&apos;, &apos;&lt;p&gt;what is your name?&lt;/p&gt;&apos;, &apos;&lt;input type=&quot;text&quot; name=&quot;name&quot;&gt;&lt;/input&gt;&apos;, &apos;&lt;p&gt;&lt;button&gt;Submit&lt;/button&gt;&lt;/p&gt;&apos;, &apos;&lt;/form&gt;&apos;].join(&apos;&apos;)) else if (&apos;/url&apos; == req.url) &#123; req.on(&apos;data&apos;, (chunk) =&gt; &#123; body += chunk &#125;) req.on(&apos;end&apos;, () =&gt; &#123; res.end(`your name is:$&#123;body&#125;`) &#125;) &#125; else &#123; res.writeHead(404) res.end(&apos;not found&apos;) &#125;&#125;).listen(3000) http客户端http包提供了request API，但是本人倾向于使用axios第三方包。axios get请求是一个promise函数，封装了抽象的函数。12345678var axios = require(&apos;axios&apos;)axios.get(&apos;http://www.sse.com.cn/assortment/stock/list/info/company/index.shtml?COMPANY_CODE=600033&apos;) .then(function (response) &#123; console.log(response); &#125;) .catch(function (error) &#123; console.log(error); &#125;); connect 中间件尝试使用一个demo来熟悉connet中间件，demo功能如下： 托管静态文件 处理错误以及损坏或者不存在的文件 处理不同类型的请求新建package.json文件12345678&#123; &quot;name&quot;: &quot;connect&quot;, &quot;version&quot;: &quot;0.01&quot;, &quot;description&quot;: &quot;a simple connect demo&quot;, &quot;dependencies&quot;: &#123; &quot;connect&quot;: &quot;^3.6.6&quot; &#125;&#125; 了不起的nodejs这本书中提供的例子实在是太老了，所以我按照现在的API重写了下面的代码。12345678910var finalHandler = require(&apos;finalHandler&apos;)var http = require(&apos;http&apos;)var serveStatic = require(&apos;serve-static&apos;)console.log(__dirname)var serve = serveStatic(__dirname, &#123; &apos;index&apos;: [&apos;1.html&apos;] &#125;)var server = http.createServer((req, res) =&gt; &#123; serve(req, res, finalHandler(req, res))&#125;)server.listen(3000) 除了http包，finalHandler和serve-static。在书写大型应用时，每个请求可能会触发多个功能，如果把这些功能代码都放在一个回调函数中，函数逻辑会很复杂。这时候可以考虑使用中间件机制，中间件本质上是函数。中间件函数除了接受req和res，还可以接受一个next函数来做流控制。这样一个大的业务逻辑，可以划分成多个中间件函数，业务逻辑划分就比较清晰了,next函数会按代码编写顺序由上到下执行，直到最后一个没有调用next函数为止。12345678app.use(function middleware1(req, res, next) &#123; // middleware 1 next();&#125;);app.use(function middleware2(req, res, next) &#123; // middleware 2 next();&#125;); 实践代码：12345678910111213141516171819202122232425262728293031var connect = require(&apos;connect&apos;)var http = require(&apos;http&apos;)var app = connect()var serveStatic = require(&apos;serve-static&apos;)var cookieSession = require(&apos;cookie-session&apos;)/**静态服务器 */app.use((req, res, next) =&gt; &#123; serveStatic(__dirname, &#123; &apos;index&apos;: [&apos;1.html&apos;] &#125;) next()&#125;)/**打印访问url */app.use((req, res, next) =&gt; &#123; console.log(req.url) next()&#125;)/**打印访问url */app.use((req, res, next) =&gt; &#123; console.log(req.method)&#125;)http.createServer(app).listen(3000) npm install命令总结一直都是直接输入npm install 直接输入，今天整理下这条命令的相关用法。 npm install 会把X包安装到node_modules目录中 不会修改package.json 之后运行npm install命令时，不会自动安装Xnpm install X –save: 会把X包安装到node_modules目录中 会在package.json的dependencies属性下添加X 之后运行npm install命令时，会自动安装X到node_modules目录中 之后运行npm install –production或者注明NODE_ENV变量值为production时，会自动安装msbuild到node_modules目录中 npm install X –save-dev: 会把X包安装到node_modules目录中 会在package.json的devDependencies属性下添加X 之后运行npm install命令时，会自动安装X到node_modules目录中 之后运行npm install –production或者注明NODE_ENV变量值为production时，不会自动安装X到node_modules目录中 使用原则:运行时需要用到的包使用–save，否则使用–save-dev。 参考npm install","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"ES6学习之路05","slug":"2018-05-03-ES6学习之路05","date":"2019-06-26T03:40:38.963Z","updated":"2019-06-26T03:40:38.964Z","comments":true,"path":"2019/06/26/2018-05-03-ES6学习之路05/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-03-ES6学习之路05/","excerpt":"","text":"ES6-学习之路-05用Proxy进行预处理在运行函数前初始化一些数据，在改变对象值后做一些善后处理。这些都算钩子函数，Proxy的存在就可以让我们给函数加上这样的钩子函数，你也可以理解为在执行方法前预处理一些代码。你可以简单的理解为他是函数或者对象的生命周期。 首先定义对象如下：12345678let obj = &#123; add: (val) =&gt; &#123; return val + 100 &#125;, name: &apos;I am du&apos;&#125;console.log(obj.add(10))console.log(obj.name) 声明proxy用new的方法对Proxy进行声明。可以看一下声明Proxy的基本形式。new Proxy（{},{}） 以下是定义proxy的完整代码：12345678910111213/** 定义proxy*/let pro = new Proxy(&#123; add: (val) =&gt; &#123; return val + 100 &#125;, name: &apos;I am du&apos;&#125;, &#123; get: (target, key, property) =&gt; &#123; console.log(&apos;before get&apos;) return target[key] &#125;&#125;)console.log(pro.name) get属性 target：得到的目标值 key：目标的key值，相当于对象的属性 property：这个不太常用，用法还在研究中，还请大神指教。 set属性set属性是值你要改变Proxy属性值时，进行的预先处理。它接收四个参数： target:目标值。 key：目标的Key值。 value：要改变的值。 receiver：改变前的原始值。 123456789101112131415161718192021222324252627let pro = new Proxy(&#123; add: (val) =&gt; &#123; return val + 100 &#125;, name: &apos;I am du&apos;&#125;, &#123; get: (target, key, property) =&gt; &#123; console.log(&apos;before get&apos;) return target[key] &#125;, set: (target, key, value, receiver) =&gt; &#123; console.log(`$&#123;receiver[key]&#125;=&gt;$&#123;value&#125; `) return target[key] = value &#125;&#125;)console.log(pro.name)pro.name = &apos;ming&apos;console.log(pro.name)输出如下：before getproxy.js:28 I am duproxy.js:20 before getproxy.js:24 I am du=&gt;ming proxy.js:20 before getproxy.js:30 ming applyapply的作用是调用内部的方法，它使用在方法体是一个匿名函数。12345678910111213let target = function() &#123; return &apos;I am test&apos;;&#125;;var handler = &#123; apply(target, ctx, args) &#123; console.log(&apos;do apply&apos;); return Reflect.apply(...arguments); &#125;&#125;var pro2 = new Proxy(target, handler);console.log(pro2()); promise使用ES6中的promise的出现给我们很好的解决了回调地狱的问题，在使用ES5的时候，在多层嵌套回调时，写完的代码层次过多，很难进行维护和二次开发，ES6认识到了这点问题，现在promise的使用，完美解决了这个问题。那我们如何理解promise这个单词在ES5中的作用那，你可以想象他是一种承诺，当它成功时执行一些代码，当它失败时执行一些代码,更符合人类的行为思考习惯。 promise的基本用法promise执行多步操作非常好用，那我们就来模仿一个多步操作的过程，那就以吃饭为例吧。要想在家吃顿饭，是要经过三个步骤： 洗菜做饭。 坐下来吃饭。 收拾桌子洗碗。这个过程是有一定的顺序的，你必须保证上一步完成，才能顺利进行下一步，现在用promise来实现。 123456789101112131415161718192021222324252627282930313233343536373839404142/**promise的使用 */let state = 1;function step1(resolve, reject) &#123; console.log(&apos;1.开始-洗菜做饭&apos;); if (state == 1) &#123; resolve(&apos;洗菜做饭--完成&apos;); &#125; else &#123; reject(&apos;洗菜做饭--出错&apos;); &#125;&#125;function step2(resolve, reject) &#123; console.log(&apos;2.开始-坐下来吃饭&apos;); if (state == 1) &#123; resolve(&apos;坐下来吃饭--完成&apos;); &#125; else &#123; reject(&apos;坐下来吃饭--出错&apos;); &#125;&#125;function step3(resolve, reject) &#123; console.log(&apos;3.开始-收拾桌子洗完&apos;); if (state == 1) &#123; resolve(&apos;收拾桌子洗完--完成&apos;); &#125; else &#123; reject(&apos;收拾桌子洗完--出错&apos;); &#125;&#125;new Promise(step1).then(function(val) &#123; console.log(val); return new Promise(step2);&#125;).then(function(val) &#123; console.log(val); return new Promise(step3);&#125;).then(function(val) &#123; console.log(val); return val;&#125;); 上面示例代码中，resolve 和 reject是两个函数变量，resole函数在Promise对象执行成功的时候触发，reject函数则在Promise对象执行失败的时候触发，Promise实际上就是前面说过的Proxy。 class类的使用class的定义见如下代码：12345678class Code &#123; name(val) &#123; console.log(val) &#125;&#125;let code = new Codecode.name(&apos;duduming&apos;)console.log(code.name) class类内多函数互相调用：123456789101112class Code &#123; name(val) &#123; console.log(val) return val &#125; anthorM(val) &#123; console.log(this.name(&apos;test&apos;) + &apos; another method&apos; + val) &#125;&#125;let code = new Code //code.name(&apos;duduming&apos;)code.anthorM(&apos;ano&apos;) 类的构造函数：通过constructor构造函数，向class内传参，然后在class内的方法中使用参数。12345678910111213141516171819class Code &#123; name(val) &#123; console.log(val) return val &#125; anthorM(val) &#123; console.log(this.name(&apos;test&apos;) + &apos; another method&apos; + val) &#125; add() &#123; return this.a + this.b &#125; constructor(a, b) &#123; this.a = a this.b = b &#125;&#125;let code = new Code(3, 4) //code.name(&apos;duduming&apos;)console.log(code.add()) class的继承,通过extends关键字。123456789101112131415161718192021222324class Code &#123; name(val) &#123; console.log(val) return val &#125; anthorM(val) &#123; console.log(this.name(&apos;test&apos;) + &apos; another method&apos; + val) &#125; add() &#123; return this.a + this.b &#125; constructor(a, b) &#123; this.a = a this.b = b &#125;&#125;class SonCode extends Code &#123;&#125;let sonCode = new SonCode(3, 4) //code.name(&apos;duduming&apos;)console.log(sonCode.add(4, 5)) 模块化exportexport可以让我们把变量，函数，对象进行模块话，提供外部调用接口，让外部进行引用。先来看个最简单的例子，把一个变量模块化。我们新建一个temp.js文件，然后在文件中输出一个模块变量。 export default的使用加上default相当是一个默认的入口。在一个文件里export default只能有一个。我们来对比一下export和export default的区别 export12345export var a =&apos;jspang&apos;; export function add(a,b)&#123; return a+b;&#125; 对应的引入方法：12345export var a =&apos;jspang&apos;; export function add(a,b)&#123; return a+b;&#125; export default1export default var a=&apos;jspang&apos; 对应引入方式1import str from &apos;./temp&apos;; 参考链接技术胖blog","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"ES6学习之路04","slug":"2018-05-02-ES6学习之路04","date":"2019-06-26T03:40:38.961Z","updated":"2019-06-26T03:40:38.961Z","comments":true,"path":"2019/06/26/2018-05-02-ES6学习之路04/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-02-ES6学习之路04/","excerpt":"","text":"ES6-学习之路-04对象key值构建有时候我们会在后台定义key值，而不是我们前台定义好的，这时候我们如何构建我们的key值。比如我们在后台取了一个key值，然后可以用[ ] 的形式，进行对象的构建。123456/**key值构建 */let key = &apos;name&apos;let obj = &#123; [key]: &apos;duming&apos;&#125;console.log(obj[&apos;name&apos;]) 自定义对象方法对象方法就是把兑现中的属性，用匿名函数的形式编程方法。1234567/**自定义对象方法 */let objFun = &#123; add: (a, b) =&gt; &#123; return a + b &#125;&#125;console.log(objFun.add(3, 4)) Object.is( ) 对象比较对象的比较方法,以前进行对象值的比较，经常使用===来判断，比如下面的代码：123456789 /**对象比较 */let obj1 = &#123; name: &apos;duming&apos; &#125;let obj2 = &#123; name: &apos;duming&apos; &#125;console.log(obj1.name === obj2.name)console.log(Object.is(obj1.name, obj2.name))console.log(Object.is(obj1, obj2)) /** * output: true true false */ ====表示值相等，Object.is()表示严格相等。 Object.assign( )合并对象123456789 /**合并对象 */let obja1 = &#123; tel: &apos;187&apos; &#125;let obja2 = &#123; address: &apos;js&apos; &#125;let obja3 = &#123; age: &apos;20&apos; &#125;let objA = Object.assign(obja1, obja2, obja3)console.log(objA) /** * &#123;tel: &quot;187&quot;, address: &quot;js&quot;, age: &quot;20&quot;&#125; */ Set和WeakSet数据结构首先看set的声明12let setdemo1 = new Set([&apos;apple&apos;, &apos;pear&apos;])console.log(setdemo1) Set和Array 的区别是Set不允许内部有重复的值，如果有只显示一个，相当于去重。虽然Set很像数组，但是他不是数组。增删改查示例代码：1234567891011121314151617181920212223/**set的声明 */let setdemo = new Set([&apos;apple&apos;, &apos;pear&apos;])console.log(setdemo) /** * Set(2) &#123;&quot;apple&quot;, &quot;pear&quot;&#125; */ /**set的增删改查 */setdemo.add(&apos;orange&apos;)setdemo.delete(&apos;apple&apos;)console.log(setdemo.has(&apos;apple&apos;))console.log(setdemo.has(&apos;pear&apos;))/**循环 foreach*/setdemo.forEach(item =&gt; &#123; console.log(item)&#125;)/** * 输出 * falsepearorang */ WeakSet跟Set相比，WeakSet的特点如下： 作为Object的容器，而不是简单类型 WeakSet中的对象如果没有其他地方引用，会被垃圾回收吸收掉。12345678910111213/**WeakSet */var ws = new WeakSet();var windows = &#123;&#125;;var foo = &#123;&#125;;ws.add(windows);ws.has(windows); // truews.has(foo); // false, foo has not been added to the setws.delete(windows); // removes window from the setws.has(windows); // false, window has been removed Map数据结构在一些构建工具中是非常喜欢使用map这种数据结构来进行配置的，因为map是一种灵活，简单的适合一对一查找的数据结构。我们知道的数据结构，已经有了json和set。那map有什么特点。123456789101112131415161718let json = &#123; name: &apos;du&apos;, age: 20&#125;console.log(json)/**map数据结构 */let map1 = new Mapmap1.set(&apos;name&apos;, &apos;du2&apos;)map1.set(&apos;age&apos;, 20)map1.set(&apos;json&apos;, json)console.log(map1)/** * 输出 * &#123;name: &quot;du&quot;, age: 20&#125;Map(3) &#123;&quot;name&quot; =&gt; &quot;du2&quot;, &quot;age&quot; =&gt; 20, &quot;json&quot; =&gt; &#123;…&#125;&#125; */ Map增删改查map的增删改查语句如下：12345map1.delete(&apos;json&apos;)map1.forEach(item =&gt; &#123; console.log(item)&#125;)console.log(map1.get(&apos;json&apos;)) 参考链接技术胖老师blog","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"nodejs学习之路01-异步调用与net fs核心类库","slug":"2018-05-01-nodejs学习之路01","date":"2019-06-26T03:40:38.958Z","updated":"2019-06-26T03:40:38.959Z","comments":true,"path":"2019/06/26/2018-05-01-nodejs学习之路01/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-05-01-nodejs学习之路01/","excerpt":"","text":"nodejs学习之路01本文github地址 https://github.com/dumingcode/nodejs-demo.git 异步调用实例123456789console.log(&quot;Hello&quot;);setTimeout(() =&gt; &#123; console.log(&quot;World&quot;) &#125;, 5000)console.log(&quot;非阻塞&quot;);output:node src\\demo01.js Hello非阻塞World 如上例所示setTimeOut是非阻塞的，程序没有等待而是直接执行了打印非阻塞信息，nodejs一切调用都是异步的。 单线程1234567891011121314151617/**单线程示例 */let start = new Date()console.log(start)setTimeout(() =&gt; &#123; console.log(Date.now() - start.getTime()) for (let i = 0; i &lt; 100000000000; i++) &#123;&#125;&#125;, 1000)setTimeout(() =&gt; &#123; console.log(Date.now() - start.getTime())&#125;, 2000)output:node src\\singleThread.js 2018-04-30T07:03:54.402Z1009160788 可以看到第二个setTimeOut的执行时间为160788秒，这是一个非常大的数字。 Node.js 在主线程里维护了一个事件队列，当接到请求后，就将该请求作为一个事件放入这个队列中，然后继续接收其他请求。当主线程空闲时(没有请求接入时)，就开始循环事件队列，检查队列中是否有要处理的事件，这时要分两种情况： 如果是非 I/O 任务，就亲自处理，并通过回调函数返回到上层调用； 如果是 I/O 任务，就从 线程池 中拿出一个线程来处理这个事件，并指定回调函数，然后继续循环队列中的其他事件上述代码就是因为是非IO任务，所以耗费了主进程计算时间，使主进程无法执行事件循环，所以nodejs不适合执行CPU计算密集型任务。单线程总结 nodejs单线程是针对图中的Event Loop，事件循环运行在主线程中。换言之nodejs接收任务请求是单线程的。 nodejs在具体执行IO任务的时候是多线程方式，在Async I/O队列中执行。 错误处理12345678910111213141516171819var http = require(&quot;http&quot;)http.createServer(() =&gt; &#123; throw new Error(&quot;错误不会被捕获&quot;)&#125;).listen(3000)输出如下：node src\\err.js d:\\Workspaces\\nodejs-demo\\src\\err.js:4 throw new Error(&quot;错误不会被捕获&quot;) ^Error: 错误不会被捕获 at Server.http.createServer (d:\\Workspaces\\nodejs-demo\\src\\err.js:4:11) at emitTwo (events.js:126:13) at Server.emit (events.js:214:7) at parserOnIncoming (_http_server.js:602:12) at HTTPParser.parserOnHeadersComplete (_http_common.js:116:23) nodejs在发生未被捕获的进程后，进程的状态就不确定了，无法正常工作。下一步添加uncaughtException函数1234567891011var http = require(&quot;http&quot;)http.createServer(() =&gt; &#123; throw new Error(&quot;错误不会被捕获&quot;)&#125;).listen(3000)process.on(&apos;uncaughtException&apos;, (err) =&gt; &#123; console.log(err) process.exit(1)&#125;) 绝大多数异步API回调函数的第一个参数都是err对象或者null。 Node中的js下面主要介绍一些nodejs的核心类库。 global任何global上的属性都可以被全局访问到。 process所有全局执行的上下文都在process中。 nodejs模块模块系统有三个全局的变量，require、module、exports。 绝对模块和相对模块绝对模块指node在node_modules内部查找的模块，或者node内置的比如fs这样的模块。相对模块是require指向工作目录中的js文件。相对模块引用示例require &#39;./moduleA&#39;。 暴露API要让模块暴露一个API成为require调用的返回值，需要依靠module和require的返回值。moduleA.js:123456exports.name = &apos;John&apos;exports.data = &apos;This is module demo&apos;let privateData = 5exports.getPriData = () =&gt; &#123; return privateData&#125; moduleDemo.js代码如下：1234var moduleA = require(&apos;./module_a&apos;)console.log(moduleA.data)console.log(moduleA.name)console.log(moduleA.getPriData()) Nodejs重要API本部分通过定义一个需求来实现： 程序启动后，需要显示当前目录下的文件列表。 选择某个文件后，程序需要显示文件内容。 选择一个目录时，程序需要显示当前目录下的内容。 运行结束后程序退出。程序设计： 创建模块 决定使用同步fs还是异步fs 理解什么是流 实现输入输出 重构 使用fs进行文件交互 完成创建模块创建名为 file-explorer的项目目录，创建一个简单的package.json文件。12345&#123; &quot;name&quot;: &quot;file-explorer&quot;, &quot;version&quot;: &quot;0.01&quot;, &quot;description&quot;: &quot;a command file-explorer&quot;&#125; 同步还是异步需要首先引入fs，fs模块是nodejs中唯一既提供同步又提供异步方法的API，为了体现nodejs异步的特性，决定使用异步方式。1234var fs = require(&apos;fs&apos;)fs.readdir(__dirname, (err, files) =&gt; &#123; console.log(files)&#125;) 示例代码V1版本如下：1234567891011121314151617181920212223242526272829var fs = require(&apos;fs&apos;)fs.readdir(__dirname, (err, files) =&gt; &#123; console.log(&apos;&apos;) if (!files.length) &#123; return console.log(&quot;no file!&quot;) &#125; console.log(&quot;select which dir or file you want to see\\n&quot;) let filefunc = (i) =&gt; &#123; let filename = files[i] //if (filename.startsWith(&quot;.&quot;)) return fs.stat(__dirname + &apos;/&apos; + filename, (err, stats) =&gt; &#123; if (stats.isDirectory()) &#123; console.log(`dir is ` + filename) &#125; else &#123; console.log(`file is ` + filename) &#125; &#125;); i++ if (i == files.length) &#123; console.log(&apos;&apos;) process.stdout.write(&apos;Enter your choice\\n&apos;) process.stdin.resume() &#125; else &#123; filefunc(i) &#125; &#125; filefunc(0)&#125;) TCP本章示例，基于TCP的聊天程序： 成功连接到server后会返回欢迎消息，server要求client输入用户名，并告知当前有多少用户方位 输入用户名，按下回车键人为链接成功 链接成功后，可以向其他用户输入消息V1版本如下:123456789101112131415161718var net = require(&apos;net&apos;)var server = net.createServer((c =&gt; &#123; console.log(`new client $&#123;c.localAddress&#125;`) c.on(&apos;end&apos;, () =&gt; &#123; console.log(&apos;client disconnected&apos;); &#125;); c.write(&apos;hello\\r\\n&apos;); c.pipe(c);&#125;))server.listen(3000, () =&gt; &#123; console.log(&apos;server bound&apos;);&#125;)server.on(&apos;error&apos;, (err) =&gt; &#123; throw err;&#125;) 聊天程序优化 增加当前已连接客户计数器 当有新客户输入昵称时，判断当前是否有重名用户，若无重名用户，则将新客户的信息通知到其余客户。 当有客户退出时，清除存储结构，并通知其他客户。 123456789101112131415161718192021222324252627282930313233343536373839404142434445var net = require(&apos;net&apos;)var conCount = 0, users = &#123;&#125;var server = net.createServer((c =&gt; &#123; var nickname console.log(`new client $&#123;c.localAddress&#125;`) c.setDefaultEncoding(&apos;utf8&apos;) c.write(&apos;hello\\r\\n&apos;); c.write(`$&#123;++conCount&#125; guests\\r\\n please write your name\\r\\n`) c.pipe(c); c.on(&apos;close&apos;, () =&gt; &#123; console.log(&apos;client disconnected&apos;); conCount-- delete users[nickname] for (key in users) &#123; users[key].write(`$&#123;nickname&#125; left out game!\\r\\n当前用户数目$&#123;conCount&#125;\\r\\n`) &#125; &#125;) c.on(&apos;data&apos;, (data) =&gt; &#123; let buf = data.toString(&apos;utf8&apos;).replace(&apos;\\r\\n&apos;, &apos;&apos;) if (buf == &apos;&apos;) return if (!nickname) &#123; if (users[buf]) &#123; c.write(`$&#123;nickname&#125; is used,please input again \\r\\n`) &#125; else &#123; nickname = buf users[nickname] = c for (key in users) &#123; users[key].write(`$&#123;nickname&#125; has joined our game\\r\\n`) &#125; &#125; &#125; &#125;)&#125;))server.listen(3000, () =&gt; &#123; console.log(&apos;server bound&apos;);&#125;)server.on(&apos;error&apos;, (err) =&gt; &#123; throw err;&#125;) users 对象存储每一个用户的链接，在每一个链接内增加nickname属性。上述代码，所有的链接共享当前链接数和user对象，任何一个链接改了这两个变量，都会对其他的链接产生影响。 参考链接i5ting大神《了不起的nodejs》","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"}]},{"title":"docker学习笔记03-Dockerfile指令详解","slug":"2018-04-30-docker学习笔记03","date":"2019-06-26T03:40:38.955Z","updated":"2019-06-26T03:40:38.955Z","comments":true,"path":"2019/06/26/2018-04-30-docker学习笔记03/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-30-docker学习笔记03/","excerpt":"","text":"docker学习笔记03-Dockerfile指令详解COPY复制文件命令格式如下： COPY &lt;源路径&gt;… &lt;目标路径&gt; COPY [“&lt;源路径1&gt;”,… “&lt;目标路径&gt;”]COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。比如： 1COPY package.json /usr/src/app/ &lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如：12COPY hom* /mydir/COPY hom?.txt /mydir/ &lt;目标路径&gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 CMD容器启动命令CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式： CMD &lt;命令&gt; exec 格式： CMD [“可执行文件”, “参数1”, “参数2”…]之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。 CMD 指令就是用于指定默认的容器主进程的启动命令的。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如， ubuntu 镜像默认的CMD 是 /bin/bash ，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash 。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release 。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 “ ，而不要使用单引号。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。一些初学者将 CMD 写为：1CMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ “sh”, “-c”, “service nginxstart”] ，因此主进程实际上是 sh 。那么当 service nginx start 命令结束后， sh 也就结束了， sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如：1CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] ENTRYPOINT 入口点ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。 ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。 当指定了 ENTRYPOINT 后， CMD 的含义就发生了改变，不再是直接的运行其命令，而是将CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为：1&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 场景一：让镜像变成像命令一样使用假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现：12345FROM ubuntu:16.04RUN apt-get update \\&amp;&amp; apt-get install -y curl \\&amp;&amp; rm -rf /var/lib/apt/lists/*CMD [ &quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot; ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行：12[docker@iz2ze1fd7d8ota0f9ysaazz myubuntu]$ docker run myip当前 IP：39.107.119.46 来自：广东省深圳市 阿里云 这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl ，那么如果我们希望显示 HTTP头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？123[docker@iz2ze1fd7d8ota0f9ysaazz myubuntu]$ docker run myip -idocker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused &quot;exec: \\&quot;-i\\&quot;: executable file not found in $PATH&quot;: unknown.[docker@iz2ze1fd7d8ota0f9ysaazz myubuntu]$ 我们可以看到可执行文件找不到的报错， executable file not found 。之前我们说过，跟在镜像名后面的是 command ，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的CMD ，而不是添加在原来的 curl -s http://ip.cn 后面。而 -i 根本不是命令，所以自然找不到。那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令：1docker run myip curl -s http://ip.cn -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用ENTRYPOINT 来实现这个镜像：12345FROM ubuntu:16.04RUN apt-get update \\&amp;&amp; apt-get install -y curl \\&amp;&amp; rm -rf /var/lib/apt/lists/*ENTRYPOINT [ &quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot; ] 重新build然后再次运行docker run myip -t可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后， CMD 的内容将会作为参数传给ENTRYPOINT ，而这里 -i 就是新的 CMD ，因此会作为参数传给 curl ，从而达到了我们预期的效果。 场景二：应用运行前的准备工作启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 ）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的：1234567FROM alpine:3.4...RUN addgroup -S redis &amp;&amp; adduser -S -G redis redis...ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]EXPOSE 6379CMD [ &quot;redis-server&quot; ] ENV 设置环境变量格式有两种： ENV ENV = =…这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN ，还是运行时的应用，都可以直接使用这里定义的环境变量。 12ENV VERSION=1.0 DEBUG=on \\NAME=&quot;Happy Feet&quot; ARG 构建参数构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是， ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令docker build 中用 –build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。 VOLUME 定义匿名卷格式为： VOLUME [“&lt;路径1&gt;”, “&lt;路径2&gt;”…] VOLUME &lt;路径&gt;之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 1VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：1docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了Dockerfile 中定义的匿名卷的挂载配置。 EXPOSE 声明端口格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;…]EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。要将 EXPOSE 和在运格式为 WORKDIR &lt;工作目录路径&gt;行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。 -p ，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 WORKDIR 指定工作目录格式为 WORKDIR &lt;工作目录路径&gt;使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在， WORKDIR 会帮你建立目录。 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：12RUN cd /appRUN echo &quot;hello&quot; &gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello 。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUNcd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 USER 指定当前用户格式： USER &lt;用户名&gt;USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。 WORKDIR 是改变工作目录， USER 则是改变之后层的执行 RUN , CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样， USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 123RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ &quot;redis-server&quot; ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo ，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu 。123456789# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64&quot; \\&amp;&amp; chmod +x /usr/local/bin/gosu \\&amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ &quot;exec&quot;, &quot;gosu&quot;, &quot;redis&quot;, &quot;redis-server&quot; ] HEALTHCHECK 健康检查格式： HEALTHCHECK [选项] CMD &lt;命令&gt; ：设置检查容器健康状况的命令 HEALTHCHECK NONE ：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12引入的新指令。 HEALTHCHECK 支持下列选项： –interval=&lt;间隔&gt; ：两次健康检查的间隔，默认为 30 秒； –timeout=&lt;时长&gt; ：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； –retries=&lt;次数&gt; ：当连续失败指定次数后，则将容器状态视为 unhealthy ，默认 3次。 和 CMD , ENTRYPOINT 一样， HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写：1234FROM nginxRUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \\CMD curl -fs http://localhost/ || exit 1 这里我们设置了每 5 秒检查一次（这里为了试验所以间隔非常短，实际应该相对较长），如果健康检查命令超过 3 秒没响应就视为失败，并且使用 curl -fs http://localhost/ || exit1 作为健康检查命令。使用 docker build 来构建这个镜像：1docker build -t myweb:v1 . 构建好了后，我们启动一个容器：1docker run -d --name web -p 82:80 myweb:v1 执行docker container ls:123[docker@iz2ze1fd7d8ota0f9ysaazz mynginx]$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6519b8beac6f myweb:v1 &quot;nginx -g &apos;daemon of…&quot; 23 seconds ago Up 22 seconds (healthy) 0.0.0.0:82-&gt;80/tcp web ONBUILD 为他人做嫁衣裳格式： ONBUILD &lt;其它指令&gt;ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN , COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile ：1234567FROM node:slimRUN mkdir /appWORKDIR /appCOPY ./package.json /appRUN [ &quot;npm&quot;, &quot;install&quot; ]COPY . /app/CMD [ &quot;npm&quot;, &quot;start&quot; ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。 如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile ，再次构建，问题解决。第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile ，而第二个项目的 Dockerfile 就会被自动修复。 那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为：1234567FROM node:slimRUN mkdir /appWORKDIR /appONBUILD COPY ./package.json /appONBUILD RUN [ &quot;npm&quot;, &quot;install&quot; ]ONBUILD COPY . /app/CMD [ &quot;npm&quot;, &quot;start&quot; ] 这次我们回到原始的 Dockerfile ，但是这次将项目相关的指令加上 ONBUILD ，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地：1FROM my-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install ，生成应用镜像。 参考dockerFile最佳实践github yeasy/docker_practice","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"指数基金投资指南-银行螺丝钉","slug":"2018-04-29-指数基金投资指南","date":"2019-06-26T03:40:38.952Z","updated":"2019-06-26T03:40:38.952Z","comments":true,"path":"2019/06/26/2018-04-29-指数基金投资指南/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-29-指数基金投资指南/","excerpt":"","text":"指数基金投资指南-银行螺丝钉利用一天时间翻了下螺丝钉写的《指数基金投资指南》，还是挺有收获的，下面重点写下这本书提到的指数估值方法。 指数估值-盈利收益率法该方法是格林厄姆提出的，在盈利收益率高的时候开始定投，在盈利收益率低的时候结束甚至止盈。盈利收益率多高算高？格林厄姆认为满足如下两个条件即为高。 盈利收益率要大于10 盈利收益率要大幅高于同期无风险利率 回到中国市场，当盈利收益率大于10坚持定投，盈利收益率小于6.4%时，结束定投（或者止盈分批卖出）。为何是6.4%，因为6.4%是中国债券基金的长期收益率，低于6.4%了不如投资债券了。 盈利收益率法使用条件 成分股流通性好 成分股盈利稳定盈利收益率法适用的指数 上证红利、中证红利、 上证50、基本面50、上证50AH优选、央视50 恒生指数、国企指数 指数估值-博格公式法影响指数基金收益的三大因素 初始投资时刻的股息率 投资期内的市盈率变化 投资期内的盈利增长率 指数基金未来的年复合收益率等于投资初期股息率+指数基金每年的市盈率变化率（可正可负）+指数基金每年的盈利变化率。注：平均到每年的变化率和收益率。 博格公式法如何实践三个因素中，可以确定的因素有两个：（1）股息率（2）当前市盈率所处历史波动中的位置。无法确定的只有一个：未来盈利的增速。根据已知因素，可以做到如下三点： 股息率高的时候买入 市盈率处于较低位置处买入 买入之后，耐心等待均值回归，等待市盈率从低到高。 博格公式法适用指数 沪深300、中证500、创业板 红利机会、消费行业、医药、养老产业指数 指数估值-博格公式变种针对周期行业，盈利E的变化很大已经失去了比较意义，此时我们引入PB对指数进行分析。指数基金未来的年复合收益率 = 指数基金每年市净率的变化率 + 指数基金每年净资产的变化率 博格公式变种适合指数 证券行业、金融行业、非银金融行业、地产行业 定期不定额定投作为一个老韭菜，自认为心理抗波动能力还可以，所以重点记录下定期不定额定投。简单地说，就是跌的多买的多，跌的少买的少。 每期定投资金 = 1000元 【n （当期盈利收益率） / （基准盈利收益率 10%）】。n相当于一个放大器，根据自身随意调节。举例子，比如当前盈利收益率是11.2% ， n=1，则本月投入资金为：1000 1（11.2/10）=1120元。 上面这个公司对博格公式法也适用，博格公式变种法用PB代替PE。 思维导图总结 点击查看大图](https://blog.gunxueqiu.site/images/fund.svg","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"docker学习笔记02-镜像","slug":"2018-04-28-docker学习笔记02","date":"2019-06-26T03:40:38.949Z","updated":"2019-06-26T03:40:38.949Z","comments":true,"path":"2019/06/26/2018-04-28-docker学习笔记02/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-28-docker学习笔记02/","excerpt":"","text":"docker学习笔记02-镜像Docker 运行容器前需要本地存在对应的镜像，如果本地不存在该镜像，Docker 会从镜像仓库下载该镜像。 获取镜像从 Docker 镜像仓库获取镜像的命令是 docker pull 。其命令格式为：docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] Docker 镜像仓库地址：地址的格式一般是 &lt;域名/IP&gt;[:端口号] 。默认地址是 DockerHub。仓库名：如之前所说，这里的仓库名是两段式名称，即 &lt;用户名&gt;/&lt;软件名&gt; 。对于 DockerHub，如果不给出用户名，则默认为 library ，也就是官方镜像。123456789docker pull ubuntu:16.0416.04: Pulling from library/ubuntu297061f60c36: Pull complete e9ccef17b516: Pull complete dbc33716854d: Pull complete 8fe36b178d25: Pull complete 686596545a94: Pull complete Digest: sha256:15f721c027e007887ba6cb071a65628c81122cb2b406e341d07cf2c180f7d759Status: Downloaded newer image for ubuntu:16.04 从下载过程中可以看到我们之前提及的分层存储的概念，镜像是由多层存储所构成。下载也是一层层的去下载，并非单一文件。下载过程中给出了每一层的 ID 的前 12 位。并且下载结束后，给出该镜像完整的sha256的摘要，以确保下载一致性。 运行image有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的ubuntu:16.04为例，如果我们打算启动里面的 bash 并且进行交互式操作的话，可以执行下面的命令。123456789101112131415161718docker run -it --rm \\ubuntu:16.04 \\bash[docker@iz2ze1fd7d8ota0f9ysaazz ~]$ docker run -it --rm \\&gt; ubuntu:16.04 \\&gt; bashroot@aee0271f7ebc:/# cat /etc/os-releaseNAME=&quot;Ubuntu&quot;VERSION=&quot;16.04.4 LTS (Xenial Xerus)&quot;ID=ubuntuID_LIKE=debianPRETTY_NAME=&quot;Ubuntu 16.04.4 LTS&quot;VERSION_ID=&quot;16.04&quot;HOME_URL=&quot;http://www.ubuntu.com/&quot;SUPPORT_URL=&quot;http://help.ubuntu.com/&quot;BUG_REPORT_URL=&quot;http://bugs.launchpad.net/ubuntu/&quot;VERSION_CODENAME=xenialUBUNTU_CODENAME=xenial docker run 就是运行容器的命令，具体格式我们会在 容器 一节进行详细讲解，我们这里简要的说明一下上面用到的参数。 -it ：这是两个参数，一个是 -i ：交互式操作，一个是 -t 终端。我们这里打算进入bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 –rm ：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm 。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 –rm 可以避免浪费空间。 ubuntu:16.04 ：这是指用 ubuntu:16.04 镜像为基础来启动容器。 bash ：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash 。 进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release ，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 Ubuntu 16.04.4 LTS 系统。最后我们通过 exit 退出了这个容器。 列出镜像要想列出已经下载下来的镜像，可以使用 docker image ls 命令。1234[docker@iz2ze1fd7d8ota0f9ysaazz ~]$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 0b1edfbffd27 4 hours ago 113MBhello-world latest e38bc07ac18e 2 weeks ago 1.85kB 可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间123456[docker@iz2ze1fd7d8ota0f9ysaazz ~]$ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 2 1 113MB 113MB (99%)Containers 1 0 0B 0BLocal Volumes 0 0 0B 0BBuild Cache 0B 0B 中间层镜像为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。 删除本地镜像如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为：12345678910[docker@iz2ze1fd7d8ota0f9ysaazz ~]$ docker image rm 0b1edfbffd27Untagged: ubuntu:16.04Untagged: ubuntu@sha256:15f721c027e007887ba6cb071a65628c81122cb2b406e341d07cf2c180f7d759Deleted: sha256:0b1edfbffd27c935a666e233a0042ed634205f6f754dbe20769a60369c614f85Deleted: sha256:a606d2db36205a11036adff8d9556e7180c26639eede2466a128d0de9b3e1b2dDeleted: sha256:fc07fc8491e0f830ebdd30de1c1b683cb2456051d17401351948ed34fe64d4afDeleted: sha256:9ddf57d8cd7fe42a9c547584549ede3dac5990b4c78ddec319ffae2cde20a496Deleted: sha256:349f8ed4d525976a9549088aa1979bc36a79f8209f89024c3cd127399914f46bDeleted: sha256:c8aa3ff3c3d351787cc5f84d960870fad16c9615aab7aa47ab343906fc8cfc24[docker@iz2ze1fd7d8ota0f9ysaazz ~]$ 用 docker image ls 命令来配合删除所有仓库名为 redis 的镜像1docker image rm $(docker image ls -q redis) 使用docker file定制镜像镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像。这个脚本就叫做Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。还以之前定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。在一个空白目录中，建立一个文本文件，并命名为 Dockerfile ：123$ mkdir mynginx$ cd mynginx$ touch Dockerfile 其内容为:12FROM nginxRUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令， FROM 和 RUN 。 FROM 指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Store 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如nginx 、 redis 、 mongo 、 mysql 、 httpd 、 php 、 tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node 、 openjdk 、 python 、 ruby 、 golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如ubuntu 、 debian 、 centos 、 fedora 、 alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch 。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。使用 Dockerfile 定制镜像。 RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力， RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式： RUN &lt;命令&gt; ，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 exec 格式： RUN [“可执行文件”, “参数1”, “参数2”] ，这更像是函数调用中的格式。既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样：12345678FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。正确的写法应该是这样：12345678910111213FROM debian:jessieRUN buildDeps=&apos;gcc libc6-dev make&apos; \\&amp;&amp; apt-get update \\&amp;&amp; apt-get install -y $buildDeps \\&amp;&amp; wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot; \\&amp;&amp; mkdir -p /usr/src/redis \\&amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\&amp;&amp; make -C /usr/src/redis \\&amp;&amp; make -C /usr/src/redis install \\&amp;&amp; rm -rf /var/lib/apt/lists/* \\&amp;&amp; rm redis.tar.gz \\&amp;&amp; rm -r /usr/src/redis \\&amp;&amp; apt-get purge -y --auto-remove $buildDeps 构建镜像进入之前创建的 Dockerfile所在目录 12345678910111213141516[docker@iz2ze1fd7d8ota0f9ysaazz mynginx]$ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048kBStep 1/2 : FROM nginxlatest: Pulling from library/nginx2a72cbf407d6: Pull complete 04b2d3302d48: Pull complete e7f619103861: Pull complete Digest: sha256:18156dcd747677b03968621b2729d46021ce83a5bc15118e5bcced925fb4ebb9Status: Downloaded newer image for nginx:latest ---&gt; b175e7467d66Step 2/2 : RUN echo Hello, Docker! &gt; /usr/share/nginx/html/index.html ---&gt; Running in 332935fd6a32Removing intermediate container 332935fd6a32 ---&gt; c08e23705638Successfully built c08e23705638Successfully tagged nginx:v3 运行nginx:v3镜像 12345678910111213141516171819202122232425262728docker run --name web2 -d -p 81:80 nginx:v3[docker@iz2ze1fd7d8ota0f9ysaazz mynginx]$ docker run --name web2 -d -p 81:80 nginx:v372a659ee4374df18579dbe2a662ddc1927cfce5b507fe79a158409934e851aad 在shell中运行 links 127.0.0.1:81``` #### 镜像构建上下文（Context） 如果注意，会看到 docker build 命令最后有一个 . 。 . 表示当前目录，而 Dockerfile就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 DockerRemote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当构建的时候，用户会指定构建镜像上下文的路径， dockerbuild 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中这么写： COPY ./package.json /app/`这并不是要复制执行 docker build 命令所在的目录下的 package.json ，也不是复制Dockerfile 所在目录下的 package.json ，而是复制 上下文（context） 目录下的package.json 。 因此， COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore ，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 参考链接github yeasy/docker_practice","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker学习笔记01-docker介绍与安装","slug":"2018-04-28-docker学习笔记01","date":"2019-06-26T03:40:38.946Z","updated":"2019-06-26T03:40:38.947Z","comments":true,"path":"2019/06/26/2018-04-28-docker学习笔记01/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-28-docker学习笔记01/","excerpt":"","text":"docker学习笔记01-docker介绍与安装docker与VM区别首先上两张图，第一张是传统vm，第二张是docker。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 对比传统虚拟机总结 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为MB 一般为GB 性能 接近原生 弱于 系统支持量 单机上千容器 一般几十个 基本概念镜像ImageDocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 容器Container镜像（ Image ）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户ID空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学Docker时常常会混淆容器和虚拟机。前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 仓库Repository镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，DockerRegistry就是这样的服务。一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过&lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。 以 Ubuntu 镜像为例，ubuntu是仓库的名字，其内包含有不同的版本标签，如，14.04,16.04。我们可以通过ubuntu:14.04 ，或者 ubuntu:16.04来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu ，那将视为 ubuntu:latest 。仓库名经常以 两段式路径形式出现，比如jwilder/nginx-proxy ，前者往往意味着DockerRegistry多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。 centos安装docker engine卸载旧版本12345678910yum remove docker \\docker-client \\docker-client-latest \\docker-common \\docker-latest \\docker-latest-logrotate \\docker-logrotate \\docker-selinux \\docker-engine-selinux \\docker-engine 使用脚本自动安装12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。 启动 Docker CE12345678910111213141516171819202122$ sudo systemctl enable docker$ sudo systemctl start docker[root@iz2ze1fd7d8ota0f9ysaazz ~]# docker versionClient: Version: 18.04.0-ce API version: 1.37 Go version: go1.9.4 Git commit: 3d479c0 Built: Tue Apr 10 18:21:36 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarmServer: Engine: Version: 18.04.0-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.4 Git commit: 3d479c0 Built: Tue Apr 10 18:25:25 2018 OS/Arch: linux/amd64 Experimental: false 建立 docker 用户组默认情况下， docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用root用户。因此，更好地做法是将需要使用 docker 的用户加入 docker用户组。 建立 docker 组用户groupadd dockeruseradd docker -g docker 将docker用户加入 docker 组usermod -aG docker docker 测试 Docker 是否安装正确1234567891011121314151617181920212223242526272829docker run hello-world[docker@iz2ze1fd7d8ota0f9ysaazz ~]$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-world9bb5a5d4561a: Pull complete Digest: sha256:f5233545e43561214ca4891fd1157e1c3c563316ed8e237750d59bde73361e77Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 添加内核参数默认配置下，如果在CentOS使用DockerCE看到下面的这些警告信息：12WARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled 请添加内核配置参数以启用这些功能。1234$ sudo tee -a /etc/sysctl.conf &lt;&lt;-EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF 然后重新加载 sysctl.conf 即可sysctl -p重新执行docker info命令， 没有warning信息即可。 镜像加速鉴于国内网络问题，后续拉取Docker镜像十分缓慢，强烈建议安装 Docker 之后配置 国内镜像加速。对于centos7，请在/etc/docker/daemon.json中写入如下内容（如果文件不存在请新建该文件）。12345&#123;&quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125; 修改完毕后重启docker服务12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 检查加速器是否生效配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行 docker info ，如果从结果中看到了如下内容，说明配置成功。12Registry Mirrors: https://registry.docker-cn.com/ 参考链接github yeasy/docker_practice","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"我的常用git命令","slug":"2018-04-26-我的常用git命令","date":"2019-06-26T03:40:38.944Z","updated":"2019-06-26T03:40:38.944Z","comments":true,"path":"2019/06/26/2018-04-26-我的常用git命令/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-26-我的常用git命令/","excerpt":"","text":"常用git命令回退到某次提交git reset –hard xxxxxxx好吧，这个如此rude的操作，放在第一条有点不协调。 push/pull 远程git clone xxx.git (本地目录)git push [远程名] [本地分支]:[远程分支]git pull [远程名] [远程分支]:[本地分支] checkout 远端新分支从remote拉取一个新分支 git checkout -b [本地分支] [远程名]/[远程分支] checkout的本地分支和远程分支名可以不一样 git loggit log –oneline –graph –decorate使用上面命令打印出来的日志，比较美观直观。 git rebase 需要注意的第一点，如果是执行git rebase master，此处的master实际上是本地的master 。 所以在执行git rebase 。 master之前，最好保证本地的master是最新的。 rebase适用的黄金法则：最好是本地的feature，不需要同步到remote的，这样安全性是最高的，不会对他人的commit造成混乱。因为rebase实际上是将master分支的commit，压到最前面。 rebase fix conflictrebase提示有冲突，可以按以下顺序解决： 修改程序代码，解决冲突。 git add fix后的代码文件 git rebase –continue 如果仍然有冲突继续解决，即重复1-3步骤，直至结束。 rebase –skip –abortgit rebase –skip 高风险操作，是将本次提交的代码删除掉（慎用）。git rebase –abort 将代码回退到 执行rebase之前的操作 查看远端分支git branch -a 冷冻当前dev分支修复bug软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交 12345678910111213$ git stash //冷冻现在在dev分支上的工作状态 冻结吧！ $ git checkout master //这个bug发生在master主分支上,我们切回master分支$ git checkout -b issue-101 //创建代号101的修复bug分支修改你的bug$ git add readme.txt //提交到暂存区$ git commit -m &quot;fix bug 101&quot; //注意填写信息，以免日后查证$ git checkout master //切换回master分支$ git merge --no-ff -m &quot;merged bug fix 101&quot; issue-101 //合并分支，注意不使用fast forward模式$ git branch -d issue-101 //删除issue-101分支$ git checkout dev //bug 改完了，是时候回到dev继续写bug了$ git stash list //查看刚刚的冻结现场$ git stash pop //git stash pop，恢复的同时把stash内容也删了：//一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除 github上定期从目标仓库更新个人仓库1234git remote add upstream https://github.com/目标仓库/docker_practice$ git fetch upstream$ git rebase upstream/master$ git push -f origin master git diff 分支之间对比 1git diff branch1 branch2 对比暂存区和当前的HEAD 1git diff --cached 恢复暂存区完全删除文件1git rm -file -f git reset 恢复一些已经提交的修改 1git rest 去除所有修改，包括索引中的内容和工作目录中的修改，那么使用 1git reset --hard 恢复至某一版本 1git reset --hard COMMIT_ID 查看谁对文件做了改动1git blame path/to/file 挑拣提交1git cherry-pick COMMIT_ID 参考链接掘金—bibi94git技巧","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"ES6学习之路-03","slug":"2018-04-25-ES6学习之路03","date":"2019-06-26T03:40:38.941Z","updated":"2019-06-26T03:40:38.941Z","comments":true,"path":"2019/06/26/2018-04-25-ES6学习之路03/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-25-ES6学习之路03/","excerpt":"","text":"ES6学习之路-03数组JSON数组格式转换JSON的数组格式就是为了前端快速的把JSON转换成数组的一种格式。1234567891011let json = &#123; &apos;0&apos;: &apos;test1&apos;, &apos;1&apos;: &apos;test2&apos;, &apos;2&apos;: &apos;test3&apos;, length: 3&#125;let arr = Array.from(json)console.log(arr)output:[&quot;test1&quot;, &quot;test2&quot;, &quot;test3&quot;] 这就是一个标准的JSON数组格式，跟普通的JSON对比是在最后多了一个length属性。只要是这种特殊的json格式都可以轻松使用ES6的语法转变成数组。在ES6中绝大部分的Array操作都存在于Array对象里。我们就用Array.from(xxx)来进行转换。我们把上边的JSON代码转换成数组，并打印在控制台。 Array.of()方法它负责把一堆文本或者变量转换成数组。在开发中我们经常拿到了一个类似数组的字符串，需要使用eval来进行转换，d但是eval的效率是很低的，它会拖慢我们的程序。这时候我们就可以使用Array.of方法。我们看下边的代码把一堆数字转换成数组并打印在控制台上：12345678let arr2 = Array.of(3, 4, 5, 6)console.log(arr2)let arr3 = Array.of(&quot;1test&quot;, &quot;2test&quot;, &quot;3test&quot;)console.log(arr3)[3, 4, 5, 6][&quot;1test&quot;, &quot;2test&quot;, &quot;3test&quot;] find( )实例方法这里的find方法是从数组中查找。在find方法中我们需要传入一个匿名函数，函数需要传入三个参数： value：表示当前查找的值。 index：表示当前查找的数组索引。 arr：表示当前数组。在函数中如果找到符合条件的数组元素就进行return，并停止查找。你可以拷贝下边的代码进行测试，就会知道find作用。1234let arr4 = [1, 2, 3, 4, 5, 6, 7, 8, 9];console.log(arr4.find(function(value, index, arr) &#123; return value &gt; 6;&#125;)) fill( )实例方法fill()也是一个实例方法，它的作用是把数组进行填充，它接收三个参数，第一个参数是填充的变量，第二个是开始填充的位置，第三个是填充到的位置。12345678let arr4 = [1, 2, 3, 4, 5, 6, 7, 8, 9];console.log(arr4.find(function(value, index, arr) &#123; return value &gt; 6;&#125;))arr4.fill(&quot;es6&quot;, 3, 5)[1, 2, 3, &quot;es6&quot;, &quot;es6&quot;, 6, 7, 8, 9] 数组的遍历for…of循环先来一个最简单的for of循环1234let arr5 = [1, 2, 3, 4, 5, 6, 7, 8, 9];for (let item of arr5) &#123; console.log(item)&#125; for…of数组索引1234let arr5 = [1, 2, 3, 4, 5, 6, 7, 8, 9]for (let index of arr5.keys()) &#123; console.log(index)&#125; for…of 值和索引同时访问用entries()这个实例方法，配合我们的for…of循环就可以同时输出内容和索引了。123for (let [index, value] of arr4.entries()) &#123; console.log(index + &apos;:&apos; + value)&#125; entries()实例方式生成的是Iterator形式的数组，那这种形式的好处就是可以让我们在需要时用next()手动跳转到下一个值。 箭头函数和扩展默认值1234function add(a, b = 1) &#123; return a + b;&#125;console.log(add(1)); 主动抛出错误ES6中我们直接用throw new Error( xxxx ),就可以抛出错误。123456789/**函数抛出异常 */function addExp(a, b = 1) &#123; if (a == 0) &#123; throw new Error(&apos;This is error&apos;) &#125; return a + b;&#125;console.log(addExp(0));console.log(addExp.length); 注意上面代码最后一行，可以打印出函数的参数（必传参数）个数。 箭头函数感觉有点像java的lambda表达式，语言果然像融合的方向发展。12let add1 = (a, b = 1) =&gt; a + bconsole.log(add1(3, 4)) {}的使用括号右侧如果是两句话，就需要使用{}。12345let add2 = (a, b = 1) =&gt; &#123; console.log(&quot;function&quot;) return a + b&#125;console.log(add2(6, 7)) 箭头函数中不可加new，也就是说箭头函数不能当构造函数进行使用。 对象的函数解构在前后端分离时，后端经常返回来JSON格式的数据，前端的美好愿望是直接把这个JSON格式数据当作参数，传递到函数内部进行处理。ES6就为我们提供了这样的解构赋值。123456let json = &#123; a: &apos;es6&apos;, b: &apos;study&apos;&#125;let foo = (&#123; a, b = &apos;study&apos; &#125;) =&gt; console.log(a, b)foo(json) in的用法in是用来判断对象或者数组中是否存在某个值的。我们先来看一下用in如何判断对象里是否有某个值。 对象判断12345let ajson = &#123; a: &apos;es6&apos;, b: &apos;study&apos;&#125;console.log(&apos;a&apos; in ajson); //true 数组判断先来看一下ES5判断的弊端，以前会使用length属性进行判断，为0表示没有数组元素。但是这并不准确，或者说真实开发中有弊端。1234/**in的用法 判断数组是否为空 */let arr11 = [, , , , , ];console.log(arr11.length); //5console.log(0 in arr11); //false 数组的遍历方法下面的代码包含了forEach，filter,some，map，这四个方法的参数本质上也是个函数，函数参数使用了匿名箭头函数。1234567891011/**数组循环 */let arrs = [&quot;w1&quot;, &quot;w2&quot;, &quot;w3&quot;, &quot;w4&quot;, &quot;w5&quot;]arrs.forEach((value, index) =&gt; console.log(index + &quot;:&quot; + value))arrs.filter(x =&gt; console.log(x))arrs.some(x =&gt; console.log(x))let arrs1 = [&quot;w11&quot;, &quot;w21&quot;, &quot;w31&quot;, &quot;w41&quot;, &quot;w51&quot;]arrs1.map(x =&gt; &#123; &apos;web&apos;; console.log(x)&#125;) 数组转换字符串 join()方法 12let arrs22 = [&quot;w11&quot;, &quot;w21&quot;, &quot;w31&quot;, &quot;w41&quot;, &quot;w51&quot;]console.log(arrs22.join(&apos;|&apos;)); toString方法 123let arrs22 = [&quot;w11&quot;, &quot;w21&quot;, &quot;w31&quot;, &quot;w41&quot;, &quot;w51&quot;]console.log(arrs22.join(&apos;|&apos;));console.log(arrs22.toString()); 参考链接技术胖老师es6学习blog","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"ES6学习之路-02","slug":"2018-04-24-ES6学习之路02","date":"2019-06-26T03:40:38.929Z","updated":"2019-06-26T03:40:38.931Z","comments":true,"path":"2019/06/26/2018-04-24-ES6学习之路02/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-24-ES6学习之路02/","excerpt":"","text":"ES6学习之路-02ES6声明变量的方式var方式var在ES6里是用来升级全局变量的，我们可以先作一个最简单的实例，用var声明一个变量a,然后用console.log进行输出。如何理解它的作用是声明全局变量那？我们用匿名函数给他进行一个包裹，然后在匿名函数中调用这个a变量，看看能不能调用到。12345var a=2;&#123; var a=3;&#125;console.log(a); 上述代码打印出3，可见var 声明变量的范围是全局的。 let方式通过上面简单的例子，我们对var的全局声明有了一定了解。那跟var向对应的是let，它是局部变量声明。还是上面的例子，我们试着在区块里用let声明。1234let a = 2; &#123; let a = 3;&#125;console.log(a); babel转换成es5的文件内容为：1234var a = 2;&#123; var _a = 3;&#125;console.log(a); let的作用范围et声明只在区块内起作用，外部是不可以调用的。下面的代码执行时会报错。1234&#123; let a=3;&#125;console.log(a); let的作用是防止数据污染，在一个大型的项目中，使用let安全性会非常高。 const方式在程序开发中，有些变量是希望声明后在业务层就不再发生变化了，简单来说就是从声明开始，这个变量始终不变，就需要用const进行声明。123const a = &quot;JSPang&quot;;var a = &apos;技术胖&apos;;console.log(a); 上面的代码编译阶段就报错了，原因就是我们const声明的变量是不可以改变的。 变量的解构赋值ES6允许按照一定模式，从数组和对象中提取值，对变量进行赋值，这被称为解构。解构赋值在实际开发中可以大量减少我们的代码量，并且让我们的程序结构更清晰。 数组的解构赋值简单的数组解构以前，为变量赋值，我们只能直接指定值。比如下面的代码：123let a = 0;let b = 1;let c = 2; 而现在我们可以用数组解构的方式来进行赋值。1let [a,b,c]=[1,2,3]; 上面的代码表示，可以从数组中提取值，按照位置的对象关系对变量赋值。 数组模式和赋值模式统一可以简单的理解为等号左边和等号右边的形式要统一，如果不统一解构将失败。1let [a,[b,c],d]=[1,[2,3],4]; 解构的默认值解构赋值是允许你使用默认值的，先看一个最简单的默认是的例子。12let [foo = true] = [];console.log(foo); //控制台打印出true 上边的例子数组中只有一个值，可能你会多少有些疑惑，我们就来个多个值的数组，并给他一些默认值。12let [a,b=&quot;JSPang&quot;]=[&apos;技术胖&apos;]console.log(a+b); //控制台显示“技术胖JSPang” 现在我们对默认值有所了解，需要注意的是undefined和null的区别。12let [a,b=&quot;JSPang&quot;]=[&apos;技术胖&apos;,undefined];console.log(a+b); //控制台显示“技术胖JSPang” undefined相当于什么都没有，b是默认值。12let [a,b=&quot;JSPang&quot;]=[&apos;技术胖&apos;,null];console.log(a+b); //控制台显示“技术胖null” null相当于有值，但值为null。所以b并没有取默认值，而是解构成了null。 对象的解构赋值解构不仅可以用于数组，还可以用于对象。123let &#123; foo, bar &#125; = &#123; foo: &quot;foo&quot;, bar: &quot;bar&quot; &#125;console.log(foo)console.log(bar) 对象的解构与数组有一个重要的不同。数组的元素是按次序排列的，变量的取值由它的位置决定；而对象的属性没有次序，变量必须与属性同名，才能取到正确的值。 扩展运算符和rest运算符对象扩展运算符（…）当编写一个方法时，我们允许它传入的参数是不确定的。这时候可以使用对象扩展运算符来作参数，看一个简单的列子：12345678function jspang(...arg)&#123; console.log(arg[0]); console.log(arg[1]); console.log(arg[2]); console.log(arg[3]); &#125;jspang(1,2,3); 扩展运算符的用处 先用一个例子说明，我们声明两个数组arr1和arr2，然后我们把arr1赋值给arr2，然后我们改变arr2的值，你会发现arr1的值也改变了，因为我们这是对内存堆栈的引用，而不是真正的赋值。123456789 /**扩展运算符的用处-数组赋值 */let arr1 = [&quot;du&quot;, &quot;ming&quot;, &quot;es6&quot;]let arr2 = arr1console.log(arr2)arr2.push(&quot;study&quot;)console.log(arr1)输出实例如下：[&quot;du&quot;, &quot;ming&quot;, &quot;es6&quot;][&quot;du&quot;, &quot;ming&quot;, &quot;es6&quot;, &quot;study&quot;] 利用扩展运算符可以解决数组深度构造的问题。12345678let arr3 = [&quot;du&quot;, &quot;ming&quot;, &quot;es6&quot;]let arr4 = [...arr3]console.log(arr3)arr4.push(&quot;study&quot;)console.log(arr3)(3) [&quot;du&quot;, &quot;ming&quot;, &quot;es6&quot;](3) [&quot;du&quot;, &quot;ming&quot;, &quot;es6&quot;] 可见arr3并没有被改变，与arr4是各自独立占一块内存。 rest运算符 如果你已经很好的掌握了对象扩展运算符，那么理解rest运算符并不困难，它们有很多相似之处，甚至很多时候你不用特意去区分。它也用…（三个点）来表示，我们先来看一个例子。1234567 /**rest运算符 */function restOper(first, ...args) &#123; console.log(args.length)&#125;restOper(0, 1, 2, 3, 4, 5, 6, 7)打印出7 ，说明args中有7个元素。 for…of循环12345function restOperForOf(first, ...args) &#123; for (let val of args) console.log(val)&#125;restOperForOf(0, 1, 2, 3, 4, 5, 6, 7) for…of的循环可以避免我们开拓内存空间，增加代码运行效率，所以建议大家在以后的工作中使用for…of循环。有的小伙伴会说了，反正最后要转换成ES5，没有什么差别，但是至少从代码量上我们少打了一些单词，这就是开发效率的提高。 字符串模板这节主要学习ES6对字符串新增的操作，最重要的就是字符串模版，字符串模版的出现让我们再也不用拼接变量了，而且支持在模板里有简单计算操作。 先看ES5下的字符串拼接实例123let username = &apos;jake1036&apos;let blog = &apos;欢迎大家来到&apos; + username + &apos;博客&apos;document.write(blog) ES5下必须用+username+这样的形式进行拼接，这样很麻烦而且很容易出错。ES6新增了字符串模版，可以很好的解决这个问题。字符串模版不再使用’xxx’这样的单引号，而是换成了xxx这种形式(可以看下面的代码)，也叫连接号。这时我们再引用jspang变量就需要用${username}这种形式了，我们对上边的代码进行改造。12345/**es6改造 */let username_es6 = &apos;jake1036&apos;let blog_es6 = `es6欢迎大家来到 $&#123;username&#125;博客`document.write(blog_es6) 还可以在字符串中增加html样式和代码12345 /**es6字符串添加html代码 */let username_es6_html = &apos;jake1036&apos;let blog_es6_html = `&lt;br/&gt;es6欢迎大家来到&lt;b&gt; $&#123;username&#125;&lt;/b&gt;博客`document.write(blog_es6_html) 对运算的支持 1234 /**对运算符的支持 */let a_es6 = 1let b_es6 = 2document.write(`&lt;br/&gt;a+b:$&#123;a_es6+b_es6&#125;`) 字符串函数 1234 /**字符串函数的应用 */let str_es6 = `测试es6字符串函数查找，支持汉字`console.log(str_es6.includes(&apos;es6&apos;))document.write(&apos;es6|&apos;.repeat(3)); //字符串重复 ES6数字操作二进制和八进制二进制和八进制数字的声明并不是ES6的特性，我们只是做一个常识性的回顾。 二进制声明二进制的英文单词是Binary,二进制的开始是0（零），然后第二个位置是b（注意这里大小写都可以实现），然后跟上二进制的值就可以了。12let binaryVal = 0B0110console.log(binaryVal) 控制台打印出6. 八进制声明八进制的英文单词是Octal，也是以0（零）开始的，然后第二个位置是O（欧），然后跟上八进制的值就可以了。12let b=0o666;console.log(b); 控制台打印出438。 数字判断和转换 数字验证Number.isFinite( xx )可以使用Number.isFinite( )来进行数字验证，只要是数字，不论是浮点型还是整形都会返回true，其他时候会返回false。 12345/**判断是否是数字 */let numberVal = 11 / 4console.log(Number.isFinite(numberVal)) //trueconsole.log(Number.isFinite(0.3434)) //trueconsole.log(Number.isFinite(&apos;test&apos;)) //false NaN验证NaN是特殊的非数字，可以使用Number.isNaN()来进行验证。下边的代码控制台返回了true。 1console.log(Number.isNaN(NaN)); 判断是否为整数Number.isInteger(xx) 1console.log(Number.isInteger(0.3434)) 参考jishupang老师博客","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"ES6学习之路-01搭建环境","slug":"2018-04-23-ES6学习之路01","date":"2019-06-26T03:40:38.925Z","updated":"2019-06-26T03:40:38.925Z","comments":true,"path":"2019/06/26/2018-04-23-ES6学习之路01/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-23-ES6学习之路01/","excerpt":"","text":"ES6学习之路-01搭建环境继续跟着技术胖老师学习前端，接下来学习ES6，首先是搭建开发环境。本实例教程github地址：https://github.com/dumingcode/es6.git 搭建开发环境有些低版本的浏览器还是不支持ES6语法，需要我们把ES6的语法自动的转变成ES5的语法。webpack能实现此功能，Babel也可以，本文使用Babel将ES6编译成ES5。 建立工程目录es6，新建两个目录 src : 书写ES6代码的文件夹，写的js程序都放在这里 dist : 利用Babel编译成的ES5代码的文件夹，在HTML页面需要引入的时这里的js文件。 编写index.html页面（在工程根目录下）,html代码我就直接复制了，注意下面js的位置在dist中，我们的js文件写在src下，需要借助babel编译到dist目录下。 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; &lt;script src=&quot;./dist/index.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; Hello ECMA Script 6 &lt;/body&gt;&lt;/html&gt; 在src目录下编写index.js功能简单只作一个a变量的声明，并用console.log()打印出来。 12let a=1;console.log(a); let是ES6的一种声明方式，接下来我们需要把这个ES6的语法文件自动编程成ES5的语法文件。 初始化项目在安装Babel之前，需要用npm init先初始化我们的项目。npm init -y-y代表全部默认同意，就不用一次次按回车了。命令执行完成后，会在项目根目录下生产package.json文件。 123456789101112&#123; &quot;name&quot;: &quot;es6&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;scripts&quot;: &#123; &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot; &#125;, &quot;keywords&quot;: [], &quot;author&quot;: &quot;&quot;, &quot;license&quot;: &quot;ISC&quot;&#125; 全局安装Babel-clicnpm install -g babel-cli虽然已经安装了babel-cli，只是这样还不能成功进行转换，尝试输入命令babel src/index.js -o dist/index.js ， 显示结果如下 12let a = 1;console.log(a); 说明编译没有成功。 本地安装babel-preset-es2015 和 babel-clicnpm install --save-dev babel-preset-es2015 babel-cli 提示babel-preset-es2015过期，采用最新包继续安装cnpm install --save-dev babel-preset-env babel-cli。安装完成后打开package.json文件，发现新增配置如下： 12345&quot;devDependencies&quot;: &#123; &quot;babel-cli&quot;: &quot;^6.26.0&quot;, &quot;babel-preset-env&quot;: &quot;^1.6.1&quot;, &quot;babel-preset-es2015&quot;: &quot;^6.24.1&quot;&#125; 新建.babelrc在根目录下新建.babelrc文件，并打开录入下面的代码 123456&#123; &quot;presets&quot;:[ &quot;env&quot; ], &quot;plugins&quot;:[]&#125; 然后重新执行编译命令babel src/index.js -o dist/index.js。然后打开编译后的dist/index.js文件，源代码如下：1234&quot;use strict&quot;;var a = 1;console.log(a); 已经成功将ES6的语法编译成ES5。 编写转换script使用npm run build 直接利用webpack进行打包，在这里也希望利用这种方式完成转换。打开package.json文件，把文件修改成下面的样子。1234567891011121314151617&#123; &quot;name&quot;: &quot;es6&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;scripts&quot;: &#123; &quot;build&quot;: &quot;babel src/index.js -o dist/index.js&quot; &#125;, &quot;keywords&quot;: [], &quot;author&quot;: &quot;&quot;, &quot;license&quot;: &quot;ISC&quot;, &quot;devDependencies&quot;: &#123; &quot;babel-cli&quot;: &quot;^6.26.0&quot;, &quot;babel-preset-env&quot;: &quot;^1.6.1&quot;, &quot;babel-preset-es2015&quot;: &quot;^6.24.1&quot; &#125;&#125; 然后就可以使用npm run build打包了。 参考技术胖老师ES6","categories":[],"tags":[{"name":"ES6","slug":"ES6","permalink":"http://yoursite.com/tags/ES6/"}]},{"title":"软技能-代码之外的生存指南读书心得","slug":"2018-04-22-软技能 代码之外的生存指南读书心得","date":"2019-06-26T03:40:38.922Z","updated":"2019-06-26T03:40:38.922Z","comments":true,"path":"2019/06/26/2018-04-22-软技能 代码之外的生存指南读书心得/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-22-软技能 代码之外的生存指南读书心得/","excerpt":"","text":"软技能-代码之外的生存指南读后感花费半个月读完了John Z. Sonmez写的《软技能 代码之外的生存指南》一书。非常推荐广大程序员看一下此书，越早看受益越早。作者针对程序员职业生涯重点写到了如下几个方面： 如何学习技术 对待工作的态度，一周工作60小时以上是必要前提 程序员如何自我营销 节省时间提供工作效率的方法 建议程序员培养健身+理财的意识用思维导图把作者的主要观点表述如下 读后个人实践读完这本书，我具体实践了以下几个方面（已经在做，不是口号）： 建立了个人blog，养成每周至少发一篇高质量（尽量做到高质量）文章。 每周跑三次步（从14年就开始跑了） 理财意识（17年初开始关注股权投资） 每周工作60小时以上（公司工作） 日常工作使用番茄工作法，主要借助此方法评估每天的工作，每天10个番茄。 微信朋友圈消息提醒关闭。 每天7点前到公司，学习前端技术，vue、es6、nodejs，学习到9点。 作者推荐的书籍看一下，然后发表读书心得","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"http://yoursite.com/tags/读书/"}]},{"title":"hexo使用jenkins自动部署到阿里云","slug":"2018-04-21-hexo使用jenkins自动部署到阿里云","date":"2019-06-26T03:40:38.920Z","updated":"2019-06-26T03:40:38.920Z","comments":true,"path":"2019/06/26/2018-04-21-hexo使用jenkins自动部署到阿里云/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-21-hexo使用jenkins自动部署到阿里云/","excerpt":"","text":"hexo使用jenkins自动部署到阿里云本地安装hexo12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server 使用github pages服务部署hexo我们用来托管博客的服务叫做 Github Pages，它是 Github 用来提供给个人/组织或者项目的网页服务，只需要部署到你的 Github Repository，推送代码，便可以实时呈现。 首先，你需要有一个 Github 的账号。然后创建一个名称为 .github.io 的仓库来托管网页即可。 以我的 Github 为例，我的用户名是 dumingcode，所以创建一个名为 dumingcode.github.io 的仓库，创建的仓库地址便是：https://github.com/dumingcode/dumingcode.github.io.git 创建完后，我们可以暂时不用管它，不需要往仓库里面 push 任何的东西。 hexo部署配置接着，我们来配置一下本地的 Hexo。 在博客的根目录下有一个名为 _config.yml 的文件，这是博客的主配置文件。前面的其他部分我们先不理会，后文再谈，我们先看最后的 Deployment 配置项：1234# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: 根据官方的文档显示，现在 Hexo 支持 Git、Heroku、Rsync、OpenShift、FTPSync 等部署方式，我们选择 Git 来部署的话，需要首先安装 hexo-deployer-git 插件：cnpm install hexo-deployer-git --save然后编辑上面的配置文件：12345deploy: type: git repo: &lt;repository url&gt; branch: [branch] message: [message] 我们需要把刚才创建的仓库地址添加进来，branch 和 message 项可以不填，默认情况下推送到 master 分支，这里我建议使用 SSH 加密的仓库地址（参看 Github 官方文档配置 SSH 免密操作）。 保存配置文件之后，我们在博客的跟目录键入：hexo g -d便可以把博客部署到 Github 了。现在，所有人都可以通过 http://.github.io 来访问自己的博客。 hexo使用第三方模板找了半天发现hexo-theme-BlueLake主题很简洁，于是使用下面的命令安装（进入blog根目录执行）。 123git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/BlueLakecnpm install hexo-renderer-jade@0.3.0 --savecnpm install hexo-renderer-stylus --save 本人搭建好的github个人主页:https://dumingcode.github.io/，欢迎访问。 hexo部署到阿里云虽说利用github pages服务能够对外发布博客，但是作为一个码农还是希望有自己的域名博客，但是我比较懒，不想手动发布博客。我想自动化地既发布到github也能同时发布到个人网站。所以决定采用CICD的方法，CICD工具使用开源的jenkins，jenkins也搭建在阿里云个人服务器上。 下载并运行jenkins注意端口使用的是8081123mkdir /usr/local/jenkinswget http://mirrors.jenkins.io/war-stable/latest/jenkins.warnohup java -jar jenkins.war --ajp13Port=-1 --httpPort=8081 &amp; 安装nginx 安装nginx依赖 1234567891011yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-develwget http://nginx.org/download/nginx-1.13.10.tar.gztar xvf nginx-1.13.10.tar.gz./configure --prefix=/usr/local/nginxmakemake installcd /usr/local/nginx/sbin./nginx -s reloadnginx: [error] open() &quot;/usr/local/nginx/logs/nginx.pid&quot; failed (2: No such file or direc需要设置nginx.conf./nginx -c /usr/local/nginx/conf/nginx.conf nginx.conf配置 12345678910111213141516171819upstream jenkins &#123; server 127.0.0.1:8081; keepalive 64;&#125;server &#123; listen 80; server_name jenkins.buyasset.com; client_max_body_size 60M; client_body_buffer_size 512k; location / &#123; port_in_redirect on; proxy_pass http://jenkins/; proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; &#125; &#125; 以上通过nginx 反向代理jenkins，在浏览器输入http://jenkins.buyasset.club就能够进入jenkins管理后台。 配置jenkins在jenkins页面提示目录中找到默认密码，输入jenkis域名，登陆jenkins。 安装jenkins社区推荐的插件 配置github获取sercret text登陆github网站，进入 github-&gt;Settings-&gt;Developer settings-&gt; Generate new token，点击生成完毕一定记录下下面的secret text。secret text一定要记住，忘记的话只能重新生成。 GitHub webhooks 设置进入GitHub上指定的项目（hexo 仓库） –&gt; setting –&gt; WebHooks&amp;Services –&gt; add webhook –&gt; 输入刚刚部署jenkins的服务器的IP图片中标红区域是变化的，后缀都是一样的为github-webhook。 jenkins中的github配置配置GitHub Plugin系统管理 –&gt; 系统设置 –&gt; GitHub –&gt; Add GitHub SeverAPI URL 输入 https://api.github.com，Credentials点击Add添加，Kind选择Secret Text,具体如下图所示。设置完成后，点击TestConnection,提示Credentials verified for user UUserName, rate limit: xxx,则表明有效。 创建一个freestyle任务 General 设置填写GitHub project URL, 也就是你的项目主页eg. https://github.com/your_name/your_repo_name 配置源码管理 构建触发器，构建环境 构建 构建脚本将上图的构建脚本替换如下： 1234cd /var/www/blog（hexo目录）git pullhexo cleanhexo g -d 构建后操作 构建前clone hexo将hexo初始代码拉取到/var/www/blog目录中，以后jenkins会监控github的push操作，一旦发现push会自动更新。cd /var/wwwgit clone https://github.com/dumingcode/dumingcode.github.io.git blog nginx反向代理hexohexo为静态网站，所以直接用nginx反向代理即可,nginx脚本如下：注意root指向的是hexo部署目录。12345678910111213141516171819202122232425server&#123; listen 80; server_name blog.buyasset.club; index index.html index.htm index.php default.html default.htm default.php; root /var/www/blog; #error_page 404 /404.html; location ~ .*\\.(ico|gif|jpg|jpeg|png|bmp|swf)$ &#123; access_log off; expires 1d; &#125; location ~ .*\\.(js|css|txt|xml)?$ &#123; access_log off; expires 12h; &#125; location / &#123; try_files $uri $uri/ =404; &#125;&#125; 测试CICD效果进入本地hexo目录，修改发布的博客，然后执行hexo g -d，登陆jenkins发现jenkins已经获取到了push操作，并且执行了自动构建任务。以下为jenkins的变更记录1234Site updated: 2018-04-21 13:35:51 (commit: 76f3c53) (details)Commit 76f3c530d077782fd66a8ca375afaa17cd188286 by dumingSite updated: 2018-04-21 13:35:51 (commit: 76f3c53) 参考链接手把手教你搭建Jenkins+Github持续集成环境Jenkins+Github持续集成Jenkins最佳实践hexo自动部署基于 Hexo 的全自动博客构建部署系统","categories":[],"tags":[{"name":"CICD","slug":"CICD","permalink":"http://yoursite.com/tags/CICD/"}]},{"title":"2018我的小目标","slug":"2018-04-15-我的小目标","date":"2019-06-26T03:40:38.918Z","updated":"2019-06-26T03:40:38.918Z","comments":true,"path":"2019/06/26/2018-04-15-我的小目标/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-15-我的小目标/","excerpt":"","text":"技术方面学习知识 掌握vue框架，能使用vue实现功能。 学习使用ELK日志分析系统。 CICD-学习使用jenkins、Travis。 nginx open resty开发。 mongodb java8新特性比如lambda docker 阿里云部署实践docker java设计模式 nodejs学习并实践 按照前端开发栈学习es6 webpack等基础技术 实践知识学习-实践-教授-再学习，只学习效率会很低，学习技术必须学以致用，实践会加深理解。 大数整体仓位算下加权pb pe roe 股息率。 按申万一级，统计每个各行业符合大数的个股情况。 他山石现金流模型。 建立自己的专用域名blog，github和个人域名都写博客,坚持每周至少一篇blog。 每学习一种技术，写blog，然后录音频视频提供大家培训。 微信、app、都进行尝试 学会造轮子。","categories":[],"tags":[{"name":"规划","slug":"规划","permalink":"http://yoursite.com/tags/规划/"}]},{"title":"2018北京长跑节半程马拉松","slug":"2018-04-15-北京长跑节半程马拉松","date":"2019-06-26T03:40:38.915Z","updated":"2019-06-26T03:40:38.915Z","comments":true,"path":"2019/06/26/2018-04-15-北京长跑节半程马拉松/","link":"","permalink":"http://yoursite.com/2019/06/26/2018-04-15-北京长跑节半程马拉松/","excerpt":"","text":"2018北京半程马拉松20180415忙里偷闲参加了北京长跑节半程马拉松，2:07跑完全程，创造个人最好成绩了。下面晒一些图，留下纪念。","categories":[],"tags":[{"name":"运动","slug":"运动","permalink":"http://yoursite.com/tags/运动/"}]}]}